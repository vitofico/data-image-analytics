{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code for Pre-processing and Exploratory Data Analysis (EDA) with pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Creating a sample DataFrame\n",
    "data = {\n",
    "    'City': ['Sevilla', 'NY', 'LA', 'New York', np.nan, 'Seville', 'Los Angeles'],\n",
    "    'Date': ['2023-06-28', '28-06-2023', np.nan, '28/06/2023', '2023-06-28', '2023-06-28', '28/06/2023'],\n",
    "    'Feature1': [5, np.nan, 7, 8, 9, 10, np.nan],\n",
    "    'Feature2': [15, np.nan, 17, 18, np.nan, 20, 21],\n",
    "    'Feature3': ['A', 'B', np.nan, 'A', 'B', 'A', np.nan],\n",
    "    'Feature4': ['X', 'Y', np.nan, 'X', 'Y', 'X', np.nan],\n",
    "    'Surname': ['Smith', 'Johnson', 'Williams', 'Jones', 'Brown', 'Davis', 'Miller']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame\")\n",
    "print(df)\n",
    "\n",
    "# Data Cleaning\n",
    "\n",
    "## 1. Incorrect Data\n",
    "name_correction = {\"Sevilla\": \"Seville\", \"NY\": \"New York\", \"LA\": \"Los Angeles\"}\n",
    "df['City'] = df['City'].replace(name_correction)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Improperly Formatted Data\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Duplicated Data\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "## 4. Irrelevant Features or Data\n",
    "df.drop('Surname', axis=1, inplace=True)\n",
    "\n",
    "# Missing Data\n",
    "\n",
    "# Finding the number of missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping a column if more than 90 percent of its instances are missing\n",
    "df_dropped = df.dropna(thresh=0.9*len(df), axis=1)\n",
    "display(df_dropped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_imputed = df.copy()\n",
    "\n",
    "# Filling missing values in quantitative features with mean value\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed[['Feature1', 'Feature2']] = imputer.fit_transform(df_imputed[['Feature1', 'Feature2']])\n",
    "display(df_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replacing missing values in categorical features with the most frequent value\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df_imputed[['City', 'Feature3', 'Feature4']] = imputer.fit_transform(df_imputed[['City', 'Feature3', 'Feature4']])\n",
    "\n",
    "# Filling missing values in quantitative features with mean value\n",
    "imputer = IterativeImputer()\n",
    "df_imputed[['Feature1', 'Feature2']] = imputer.fit_transform(df_imputed[['Feature1', 'Feature2']])\n",
    "\n",
    "print(\"\\nDataFrame after imputation:\")\n",
    "print(df_imputed)\n",
    "\n",
    "\n",
    "# Creating a new binary feature to indicate the imputation\n",
    "df_imputed['Feature1_imputed'] = 0\n",
    "df_imputed.loc[df['Feature1'].isnull(), 'Feature1_imputed'] = 1\n",
    "print(\"\\nDataFrame after adding binary feature:\")\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "The Python library `pandas` provides functions for calculating mean, median, and quantiles. You can also use `matplotlib` to create boxplots for visualizing outliers.\n",
    "\n",
    "Here is the Python code for each of the strategies mentioned:\n",
    "\n",
    "**Mean and Median:**\n",
    "\n",
    "You can use the `mean()` and `median()` functions of a pandas DataFrame to calculate the mean and median of each column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a numeric DataFrame\n",
    "np.random.seed(0)  # for reproducibility\n",
    "data = {\n",
    "    'Feature1': np.random.normal(0, 1, 1000),\n",
    "    'Feature2': np.random.normal(10, 2, 1000),\n",
    "    'Feature3': np.random.normal(-5, 5, 1000),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add some extreme values (outliers)\n",
    "df.loc[1000] = [20, 20, -10]\n",
    "df.loc[1001] = [-10, 20, 20]\n",
    "\n",
    "print(df)\n",
    "\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "print(\"Mean of each column:\")\n",
    "print(df.mean())\n",
    "print(\"\\nMedian of each column:\")\n",
    "print(df.median())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Interquartile Range (IQR):**\n",
    "\n",
    "You can use the `quantile()` function of a pandas DataFrame to calculate the 1st and 3rd quartiles (25th and 75th percentiles) and then compute the IQR. Values that are 1.5 times the IQR less than the first quartile or 1.5 times the IQR more than the third quartile are considered as outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame and 'Feature1' is one of its columns\n",
    "Q1 = df['Feature1'].quantile(0.25)\n",
    "Q3 = df['Feature1'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Identifying outliers\n",
    "outliers = df[(df['Feature1'] < (Q1 - 1.5 * IQR)) | (df['Feature1'] > (Q3 + 1.5 * IQR))]\n",
    "print(\"Outliers in 'Feature1':\")\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Boxplot**:\n",
    "\n",
    "You can use the `boxplot()` function of `matplotlib.pyplot` to create boxplots. Boxplots are useful for visually identifying outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame and 'Feature1' is one of its columns\n",
    "plt.boxplot(df['Feature1'].dropna())\n",
    "plt.title(\"Boxplot of 'Feature1'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This boxplot visually represents the distribution of 'Feature1'. The line in the middle of the box is the median, the box represents the IQR, the whiskers represent the range within 1.5 times the IQR from the box, and points outside the whiskers are potential outliers.\n",
    "\n",
    "Please remember to replace `'Feature1'` with the actual column name you're interested in. Note that these examples assume your data is numeric. If it's not, you'll have to convert or handle the data appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing outliers:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mstats\n",
    "from scipy.special import boxcox, inv_boxcox\n",
    "\n",
    "# Calculate the IQR for each column\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define upper and lower bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify the outliers\n",
    "outliers = (df < lower_bound) | (df > upper_bound)\n",
    "\n",
    "# Print the identified outliers\n",
    "print(\"\\nOutliers detected:\")\n",
    "print(outliers)\n",
    "\n",
    "# Removing outliers\n",
    "df_no_outliers = df[~outliers.any(axis=1)]\n",
    "print(\"\\nDataFrame after removing outliers:\")\n",
    "print(df_no_outliers)\n",
    "\n",
    "# Transforming outliers: Logarithmic transformation\n",
    "df_log_transformed = df.copy()\n",
    "# Add a small constant to avoid division by zero error when applying log\n",
    "df_log_transformed = np.log(df_log_transformed + 0.1)\n",
    "print(\"\\nDataFrame after logarithmic transformation:\")\n",
    "print(df_log_transformed)\n",
    "\n",
    "# Power transformation\n",
    "df_power_transformed = df.copy()\n",
    "df_power_transformed = np.sqrt(df_power_transformed)\n",
    "print(\"\\nDataFrame after power transformation:\")\n",
    "print(df_power_transformed)\n",
    "\n",
    "# Winsorizing outliers: Capping\n",
    "df_capped = df.copy()\n",
    "for column in df_capped.columns:\n",
    "    df_capped[column] = np.where(df_capped[column] < lower_bound[column], lower_bound[column], df_capped[column])\n",
    "    df_capped[column] = np.where(df_capped[column] > upper_bound[column], upper_bound[column], df_capped[column])\n",
    "print(\"\\nDataFrame after capping outliers:\")\n",
    "print(df_capped)\n",
    "\n",
    "# Winsorizing outliers: Truncation\n",
    "df_truncated = df.copy()\n",
    "df_truncated[outliers] = np.nan\n",
    "print(\"\\nDataFrame after truncating outliers:\")\n",
    "print(df_truncated)\n",
    "\n",
    "# Winsorizing outliers using scipy's mstats module\n",
    "df_winsorized = df.copy()\n",
    "for column in df_winsorized.columns:\n",
    "    df_winsorized[column] = mstats.winsorize(df_winsorized[column], limits=[0.05, 0.05])\n",
    "print(\"\\nDataFrame after Winsorizing:\")\n",
    "print(df_winsorized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Create a dataframe\n",
    "data = {'City': ['Amsterdam', 'Rotterdam', 'The Hague', 'Utrecht'],\n",
    "        'Province': ['North Holland', 'South Holland', 'South Holland', 'Utrecht'],\n",
    "        'Population': [821752, 623652, 514861, 334176]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"\\nOriginal DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# (a) Substituting categorical variables with numeric values using LabelEncoder:\n",
    "\n",
    "# Instantiate LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply LabelEncoder to the 'City' column\n",
    "df['City_encoded'] = le.fit_transform(df['City'])\n",
    "\n",
    "print(\"\\nDataFrame after Label Encoding 'City':\")\n",
    "print(df)\n",
    "\n",
    "# (b) Using one-hot encoding for categorical variables:\n",
    "\n",
    "# Instantiate OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "# Apply OneHotEncoder to the 'Province' column\n",
    "ohe_results = ohe.fit_transform(df[['Province']])\n",
    "\n",
    "# Manually create feature names\n",
    "feature_names = ohe.categories_[0]\n",
    "\n",
    "# Convert the results to a DataFrame and append to original DataFrame\n",
    "ohe_df = pd.DataFrame(ohe_results.toarray(), columns=feature_names)\n",
    "df = pd.concat([df, ohe_df], axis=1)\n",
    "\n",
    "print(\"\\nDataFrame after One-Hot Encoding 'Province':\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Binning\n",
    "\n",
    "Feature binning or discretization is the process of converting continuous features into discrete bins. It can be useful for algorithms that can't handle continuous values and to reduce the effect of small observation errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Suppose we have a DataFrame with age and salary columns\n",
    "data = {'age': [22, 25, 47, 52, 46, 56, 55, 60, 62, 61, 18, 28, 27, 29, 49],\n",
    "        'salary': [20691, 223500, 68730, 158731, 124150, 65123, 75400, 61398, 74127, 81293, 59419, 59639, 99524, 74816, 43429]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# We can bin the continuous age data into categories (bins)\n",
    "bins = [18, 30, 40, 50, 60, 70]\n",
    "labels = ['18-29', '30-39', '40-49', '50-59', '60-69']\n",
    "df['age_bin'] = pd.cut(df['age'], bins=bins, labels=labels)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Feature Scaling (Standardization and Normalization)\n",
    "\n",
    "Feature scaling is a step in pre-processing input data for a machine learning model. Standardization (zero mean and unit variance) and normalization (scaling features to lie between a given minimum and maximum value) are common methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Standardization\n",
    "scaler_standard = StandardScaler()\n",
    "df['salary_standardized'] = scaler_standard.fit_transform(df[['salary']])\n",
    "\n",
    "# Normalization\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df['salary_normalized'] = scaler_minmax.fit_transform(df[['salary']])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Class Label Imbalance (SMOTE)\n",
    "\n",
    "In imbalanced classification problems, one class can heavily outnumber the other class(es). Synthetic Minority Over-sampling Technique (SMOTE) is one method to address this issue by generating new samples in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.datasets import make_classification\n",
    "from collections import Counter\n",
    "\n",
    "# Create a imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2,\n",
    "                           n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n",
    "\n",
    "print(f'Original Dataset Shape: {Counter(y)}')\n",
    "\n",
    "# Apply SMOTE\n",
    "sm = SMOTE(random_state=2)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "print(f'Resampled Dataset Shape: {Counter(y_res)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effects of Scaling on Supervised Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing methods like the scalers are usually applied before applying a supervised machine learning algorithm. As an example, say we want to apply the kernel SVM (SVC) to the cancer dataset, and use MinMaxScaler for preprocessing the data. We start by loading our dataset and splitting it into a training set and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define the scaling methods\n",
    "scalers = [StandardScaler(), MinMaxScaler(), RobustScaler()]\n",
    "\n",
    "# Initialize lists to store scaler names and accuracies\n",
    "scaler_names = ['Unscaled']\n",
    "accuracies = []\n",
    "\n",
    "# Calculate and store the accuracy of the unscaled dataset\n",
    "lr = LogisticRegression(max_iter=5000)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "unscaled_accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracies.append(unscaled_accuracy)\n",
    "\n",
    "for scaler in scalers:\n",
    "    # Scale the training and test sets\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Instantiate the model\n",
    "    lr = LogisticRegression(max_iter=5000)\n",
    "\n",
    "    # Fit the model to the scaled training set\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predict on the scaled test set\n",
    "    y_pred = lr.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate the accuracy score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Store scaler name and accuracy\n",
    "    scaler_name = type(scaler).__name__\n",
    "    scaler_names.append(scaler_name)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Plot the accuracies\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(scaler_names))\n",
    "colors = ['darkblue', 'mediumseagreen', 'darkorange', 'red']\n",
    "bar_width = 0.4\n",
    "\n",
    "plt.bar(x, accuracies, width=bar_width, color=colors, edgecolor='black')\n",
    "\n",
    "# Add text labels for accuracy values\n",
    "for i, acc in enumerate(accuracies):\n",
    "    plt.text(i, acc + 0.01, f'{acc:.2f}', ha='center', color='black', fontsize=10)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Scaling Method')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Comparison - Breast Cancer Classification')\n",
    "plt.xticks(x, scaler_names, rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scale a dataset and use it in a machine learning model, follow these steps:\n",
    "\n",
    "1. Load the dataset: Start by loading the dataset you want to work with. This could be a dataset you have collected or a pre-existing dataset from a library like scikit-learn.\n",
    "2. Split the dataset: Divide the dataset into training and test sets using the `train_test_split` function. This step is essential to evaluate the model's performance on unseen data.\n",
    "3. Choose a scaling method: Decide on a scaling method based on your data and the requirements of your machine learning algorithm. Some commonly used scaling methods are StandardScaler, MinMaxScaler, and RobustScaler.\n",
    "4. Initialize the scaler: Create an instance of the chosen scaler. For example, if you want to use StandardScaler, initialize it using `scaler = StandardScaler()`.\n",
    "5. Fit and transform the training data: Fit the scaler to the training data using the `fit` method: `scaler.fit(X_train)`. This step calculates the necessary statistics from the training data (e.g., mean and standard deviation for StandardScaler). Then, transform the training data using the `transform` method: `X_train_scaled = scaler.transform(X_train)`. This step scales the training data based on the calculated statistics.\n",
    "6. Transform the test data: Use the same scaler instance to transform the test data. This ensures that the test data is scaled in the same way as the training data: `X_test_scaled = scaler.transform(X_test)`.\n",
    "7. Build and train the model: Create an instance of the machine learning model you want to use (e.g., Logistic Regression, Random Forest, etc.). Fit the model to the scaled training data: `model.fit(X_train_scaled, y_train)`.\n",
    "8. Evaluate the model: Use the trained model to make predictions on the scaled test data: `y_pred = model.predict(X_test_scaled)`. Evaluate the model's performance using appropriate evaluation metrics (e.g., accuracy, precision, recall, etc.)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-and-image-analytics-ut4sJMav-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
