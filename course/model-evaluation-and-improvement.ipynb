{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python code uses the scikit-learn library to perform five-fold cross-validation on the Iris dataset using a Random Forest Classifier. It then prints out the cross-validation scores for each fold and their mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Define the model\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "\n",
    "print(\"Cross-validation scores for each fold: \", scores)\n",
    "\n",
    "# A common way to summarize the cross-validation accuracy is to compute the mean\n",
    "print(\"Mean cross-validation score: \", scores.mean())\n",
    "\n",
    "# Change the number of folds to 3\n",
    "scores = cross_val_score(clf, X, y, cv=3)\n",
    "\n",
    "print(\"Cross-validation scores for each fold: \", scores)\n",
    "print(\"Mean cross-validation score: \", scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Grid Search Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Grid Search Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Random Search\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=3, cv=3)\n",
    "random_search.fit(X, y)\n",
    "\n",
    "print(\"Random Search Best Parameters:\", random_search.best_params_)\n",
    "print(\"Random Search Best Score:\", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Grid Search, we define a parameter grid param_grid that specifies the different values we want to try for the n_estimators and max_depth hyperparameters. We then create a GridSearchCV object, passing the model and parameter grid, and specifying the number of cross-validation folds with cv=3. We fit the Grid Search object to the data using fit(X, y).\n",
    "\n",
    "After the Grid Search is performed, we print the best parameters found (grid_search.best_params_) and the best score (grid_search.best_score_).\n",
    "\n",
    "For Random Search, we define a parameter distribution param_dist similar to the parameter grid in Grid Search, but without specifying all possible combinations. We create a RandomizedSearchCV object, passing the model, parameter distribution, and the number of iterations n_iter=3 (which specifies the number of random combinations to try). We fit the Random Search object to the data using fit(X, y).\n",
    "\n",
    "After the Random Search is performed, we print the best parameters found (random_search.best_params_) and the best score (random_search.best_score_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing the result of grid search\n",
    "\n",
    "Analyzing the result of grid search is an important step in hyperparameter tuning. Grid search exhaustively explores a defined grid of hyperparameter combinations and provides the best parameter values that maximize the performance metric. Once the grid search is complete, it is crucial to analyze the results to gain insights into the performance of different hyperparameter settings and make informed decisions.\n",
    "\n",
    "To analyze the result of grid search, you can follow these steps:\n",
    "\n",
    "1. Access the grid search results: After performing grid search, you can access the results using the cv_results_ attribute of the grid search object. This attribute provides a dictionary-like object containing various information about the search process, including the hyperparameter settings, mean scores, and standard deviations.\n",
    "2. Extract relevant information: Extract the relevant information from the cv_results_ attribute based on your analysis requirements. Commonly used information includes the hyperparameter values, mean test scores, standard deviations, and any other relevant metrics.\n",
    "3. Visualize the results: Create visualizations to better understand the impact of different hyperparameters on the model's performance. You can use plots such as bar plots, line plots, or heatmaps to visualize the mean scores or other relevant metrics across different hyperparameter settings.\n",
    "4. Interpret the results: Analyze the visualizations to identify the hyperparameter settings that result in the best performance. Look for any trends, patterns, or trade-offs among the different hyperparameters. Consider the impact of each hyperparameter on the model's performance and assess whether the chosen parameter values make sense based on the data and domain knowledge.\n",
    "5. Make decisions and refine the hyperparameter search: Based on the analysis, make decisions about which hyperparameter settings to choose. You may select the combination with the highest mean test score or consider a trade-off between performance and model complexity. If needed, refine the hyperparameter search by adjusting the grid space or adding new hyperparameters to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Access the grid search results\n",
    "results = grid_search.cv_results_\n",
    "\n",
    "# Convert results to a Pandas DataFrame\n",
    "results_pd = pd.DataFrame(results)\n",
    "\n",
    "# Print the first five rows of the DataFrame\n",
    "display(results_pd.head())\n",
    "\n",
    "# Extract relevant information\n",
    "param_combinations = results['params']\n",
    "mean_test_scores = results['mean_test_score']\n",
    "std_test_scores = results['std_test_score']\n",
    "\n",
    "# Find the index of the highest mean test score\n",
    "best_index = np.argmax(mean_test_scores)\n",
    "\n",
    "# Get the highest mean test score and its corresponding parameters\n",
    "best_score = mean_test_scores[best_index]\n",
    "best_params = param_combinations[best_index]\n",
    "\n",
    "\n",
    "# Interpret the results and make decisions based on the analysis\n",
    "print(\"Best Mean Test Score:\", best_score)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "\n",
    "# Extract relevant information\n",
    "param_combinations = results['params']\n",
    "mean_test_scores = results['mean_test_score']\n",
    "\n",
    "# Reshape the mean test scores into a 2D array\n",
    "scores = np.array(mean_test_scores).reshape(len(param_grid['n_estimators']), len(param_grid['max_depth']))\n",
    "\n",
    "# Plot the mean cross-validation scores as a heatmap\n",
    "plt.figure()\n",
    "sns.heatmap(scores, annot=True, fmt=\".3f\", cmap=\"viridis\",\n",
    "            xticklabels=param_grid['max_depth'], yticklabels=param_grid['n_estimators'])\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('n_estimators')\n",
    "plt.title('Grid Search Results (Mean Test Scores)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs grid search to find the best hyperparameters for a RandomForestClassifier model using the Iris dataset. Let's go through the code and understand it step by step:\n",
    "\n",
    "1. Loading the dataset:\n",
    "    The Iris dataset is loaded using load_iris() function from sklearn.datasets.\n",
    "    The input features are assigned to X, and the target variable is assigned to y.\n",
    "\n",
    "2. Defining the model:\n",
    "    The RandomForestClassifier model is initialized.\n",
    "    Defining the parameter grid:\n",
    "        The parameter grid is defined using param_grid dictionary. It specifies the values to be searched for n_estimators and max_depth.\n",
    "\n",
    "3. Performing grid search:\n",
    "    GridSearchCV is used to perform grid search. The model, parameter grid, and the number of cross-validation folds (cv) are passed as arguments.\n",
    "    The grid search is executed by calling fit() on the grid search object, passing the input features (X) and target variable (y).\n",
    "\n",
    "4. Accessing the grid search results:\n",
    "    The cv_results_ attribute of the grid search object is accessed to obtain the results of the grid search.\n",
    "    The results are stored in the results variable.\n",
    "\n",
    "5. Converting results to a DataFrame:\n",
    "    The results are converted to a Pandas DataFrame for easier analysis and visualization.\n",
    "    results_pd is a DataFrame created from results.\n",
    "\n",
    "6. Printing the first five rows of the DataFrame:\n",
    "    The head() method is used to display the first five rows of the DataFrame.\n",
    "\n",
    "7. Extracting relevant information:\n",
    "    The relevant information from the results is extracted, including the parameter combinations, mean test scores, and standard deviations.\n",
    "    These values are stored in param_combinations, mean_test_scores, and std_test_scores respectively.\n",
    "\n",
    "8. Finding the best mean test score:\n",
    "    The index of the highest mean test score is found using np.argmax().\n",
    "    The best mean test score and its corresponding parameters are extracted using the index.\n",
    "\n",
    "9. Printing the best mean test score and parameters:\n",
    "\n",
    "10. Reshaping mean test scores:\n",
    "    The mean test scores are reshaped into a 2D array, scores, using np.array.reshape().\n",
    "    The dimensions of the array are based on the number of values for n_estimators and max_depth in the parameter grid.\n",
    "\n",
    "11. Plotting the mean cross-validation scores as a heatmap:\n",
    "    A heatmap is created using sns.heatmap(), which takes scores as the data to be plotted.\n",
    "    The annot=True argument adds annotations to each cell with the mean test score.\n",
    "    The fmt=\".3f\" argument specifies the formatting of the annotations.\n",
    "    Colormap cmap=\"viridis\" is used to visualize the scores.\n",
    "    The x-axis and y-axis labels are set using xticklabels and yticklabels respectively.\n",
    "    The heatmap is displayed using plt.show().\n",
    "\n",
    "The code allows you to analyze the grid search results by printing the DataFrame, identifying the best mean test score and parameters, and visualizing the mean test scores using a heatmap. This information helps you make decisions about which hyperparameters to choose for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning and Validation Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import learning_curve, validation_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Learning Curves\n",
    "train_sizes, train_scores, val_scores = learning_curve(model, X, y, train_sizes=np.linspace(0.1, 1.0, 5), cv=3)\n",
    "\n",
    "# Calculate mean and standard deviation of training scores and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure()\n",
    "plt.title(\"Learning Curves\")\n",
    "plt.xlabel(\"Training Examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(train_sizes, val_scores_mean - val_scores_std, val_scores_mean + val_scores_std, alpha=0.1, color=\"g\")\n",
    "\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training Score\")\n",
    "plt.plot(train_sizes, val_scores_mean, 'o-', color=\"g\", label=\"Validation Score\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "# Validation Curves\n",
    "param_range = np.arange(1, 11)\n",
    "train_scores, val_scores = validation_curve(model, X, y, param_name=\"max_depth\", param_range=param_range, cv=3)\n",
    "\n",
    "# Calculate mean and standard deviation of training scores and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Plot validation curves\n",
    "plt.figure()\n",
    "plt.title(\"Validation Curves\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(param_range, val_scores_mean - val_scores_std, val_scores_mean + val_scores_std, alpha=0.1, color=\"g\")\n",
    "\n",
    "plt.plot(param_range, train_scores_mean, 'o-', color=\"r\", label=\"Training Score\")\n",
    "plt.plot(param_range, val_scores_mean, 'o-', color=\"g\", label=\"Validation Score\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For learning curves, we use the learning_curve function from scikit-learn to calculate the training and validation scores for different training set sizes. We specify the model, input data X, target data y, the sizes of the training sets to use with train_sizes, and the number of cross-validation folds with cv=3.\n",
    "\n",
    "After obtaining the scores, we calculate the mean and standard deviation for the training and validation scores. Then, we plot the learning curves, showing the mean scores as well as the shaded regions indicating the standard deviation.\n",
    "\n",
    "For validation curves, we use the validation_curve function from scikit-learn to calculate the training and validation scores for different values of a hyperparameter (max_depth in this case). We specify the model, input data X, target data y, the name of the hyperparameter with param_name, the range of values to test with param_range, and the number of cross-validation folds with cv=3.\n",
    "\n",
    "After obtaining the scores, we calculate the mean and standard deviation for the training and validation scores. Then, we plot the validation curves, showing the mean scores as well as the shaded regions indicating the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To produce learning curves and validation curves, you can follow these general steps:**\n",
    "\n",
    "1. Load the dataset: Begin by loading your dataset that you will use for training and evaluation.\n",
    "2. Define the model: Choose the machine learning model you want to analyze and fine-tune.\n",
    "\n",
    "3. Learning Curves:\n",
    "    - Use the learning_curve function to generate learning curves. Specify the model, input features (X), target variable (y), the sizes of the training sets to use (train_sizes), and the number of cross-validation folds (cv).\n",
    "    - Calculate the mean and standard deviation of the training and validation scores across the different training set sizes.\n",
    "    - Plot the learning curves using the mean scores. You can also visualize the standard deviation using shaded regions.\n",
    "\n",
    "3. Validation Curves:\n",
    "    - Use the validation_curve function to generate validation curves. Specify the model, input features (X), target variable (y), the name of the hyperparameter to tune (param_name), the range of values to test for the hyperparameter (param_range), and the number of cross-validation folds (cv).\n",
    "    - Calculate the mean and standard deviation of the training and validation scores across the different hyperparameter values.\n",
    "    - Plot the validation curves using the mean scores. You can also visualize the standard deviation using shaded regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics and Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics for Binary Classification\n",
    "\n",
    "When working with binary classification problems, various metrics are used to evaluate the performance of machine learning models. These metrics provide insights into the model's ability to distinguish between the two classes and make accurate predictions. Let's explore some commonly used metrics for binary classification:\n",
    "\n",
    "1. Accuracy: Accuracy is the most straightforward metric, representing the proportion of correct predictions out of the total number of predictions. It is calculated by dividing the number of correct predictions by the total number of samples.\n",
    "2. Precision: Precision measures the proportion of true positive predictions (correctly predicted positive class) out of all positive predictions. It focuses on the accuracy of positive predictions and helps evaluate the model's ability to avoid false positives.\n",
    "3. Recall (Sensitivity/True Positive Rate): Recall calculates the proportion of true positive predictions out of all actual positive samples. It quantifies the model's ability to find all positive instances and avoid false negatives.\n",
    "5. F1 Score: The F1 score is the harmonic mean of precision and recall. It provides a balanced measure of the model's performance by considering both precision and recall. It is useful when there is an imbalance between the classes.\n",
    "6. Receiver Operating Characteristic (ROC) Curve: The ROC curve is a graphical representation of the trade-off between true positive rate (sensitivity) and false positive rate (1-specificity) at different classification thresholds. It helps visualize the model's performance across various threshold values.\n",
    "7. Area Under the ROC Curve (AUC-ROC): The AUC-ROC is a scalar value that quantifies the overall performance of a binary classification model. It represents the area under the ROC curve and provides a measure of the model's ability to distinguish between positive and negative classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "# Load dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build and fit model\n",
    "model = LogisticRegression(max_iter=5000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "precision = metrics.precision_score(y_test, y_pred)\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "f1 = metrics.f1_score(y_test, y_pred)\n",
    "auc_roc = metrics.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"AUC-ROC: {auc_roc}\")\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.imshow(conf_matrix, cmap='Blues', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.xticks([0, 1], ['Negative', 'Positive'])\n",
    "plt.yticks([0, 1], ['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Add labels to the plot\n",
    "class_names = ['TN', 'FP', 'FN', 'TP']\n",
    "thresh = conf_matrix.max() / 2.\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        plt.text(j, i, f\"{class_names[i*2 + j]} = {conf_matrix[i, j]}\",\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Generate a synthetic binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build and fit a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities of positive class\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the false positive rate (fpr) and true positive rate (tpr)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Calculate the area under the ROC curve (AUC-ROC)\n",
    "auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % auc_roc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Random classifier line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Print the AUC-ROC score\n",
    "print(\"AUC-ROC:\", auc_roc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Receiver Operating Characteristic (ROC) curve is a graphical representation of the performance of a binary classifier as the discrimination threshold is varied. It plots the true positive rate (TPR) against the false positive rate (FPR) at different threshold settings. The TPR is also known as sensitivity or recall, while the FPR is the complement of the specificity.\n",
    "\n",
    "The ROC curve is useful for evaluating the trade-off between the true positive rate and the false positive rate of a classifier. It provides a visual representation of how well the classifier is able to distinguish between the positive and negative classes at different decision thresholds.\n",
    "\n",
    "Interpreting the ROC curve:\n",
    "\n",
    "The closer the ROC curve is to the top-left corner of the plot, the better the classifier's performance. This indicates high TPR and low FPR across various threshold settings.\n",
    "A random classifier would have a ROC curve that is a diagonal line from the bottom-left to the top-right, indicating an equal chance of correctly classifying positive and negative instances.\n",
    "A classifier that performs worse than random would have an ROC curve that falls below the diagonal line.\n",
    "The point on the ROC curve where the TPR and FPR intersect depends on the decision threshold. By adjusting the threshold, you can achieve different trade-offs between TPR and FPR.\n",
    "The Area Under the ROC Curve (AUC-ROC) is a numerical measure that quantifies the performance of a classifier across all possible threshold settings. It represents the entire two-dimensional area underneath the ROC curve. The AUC-ROC value ranges from 0 to 1, where:\n",
    "\n",
    "A perfect classifier has an AUC-ROC score of 1, indicating that it achieves a TPR of 1 and an FPR of 0 across all thresholds.\n",
    "A random classifier has an AUC-ROC score of 0.5, indicating no discriminatory power, as it performs as well as flipping a coin.\n",
    "A classifier with an AUC-ROC score above 0.5 has some level of discriminatory power, with higher values indicating better performance.\n",
    "In summary, the ROC curve visually displays the trade-off between true positive rate and false positive rate at various decision thresholds, while the AUC-ROC provides a single metric to assess the overall performance of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics for Multiclass Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "# Generate a synthetic dataset with four classes\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_classes=4, random_state=42)\n",
    "\n",
    "# Split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build and fit model\n",
    "model = LogisticRegression(max_iter=5000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "precision = metrics.precision_score(y_test, y_pred, average='macro')\n",
    "recall = metrics.recall_score(y_test, y_pred, average='macro')\n",
    "f1 = metrics.f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.imshow(conf_matrix, cmap='Blues', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(4))\n",
    "plt.yticks(range(4))\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Add labels to the plot\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        plt.text(j, i, str(conf_matrix[i, j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if conf_matrix[i, j] > conf_matrix.max() / 2. else \"black\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Metrics\n",
    "\n",
    "When working with regression problems, various metrics are used to evaluate the performance of machine learning models. These metrics provide insights into the model's ability to make accurate predictions. Let's explore some commonly used metrics for regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "california = datasets.fetch_california_housing()\n",
    "X = california.data\n",
    "y = california.target\n",
    "\n",
    "# Split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build and fit model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = metrics.r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"R2 Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_blobs(n_samples=(400, 50), cluster_std=[7.0, 2], random_state=22)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Initialize and fit the SVC model\n",
    "svc = SVC(gamma=.05)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Generate grid of points to evaluate the model\n",
    "xx, yy = np.meshgrid(np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100),\n",
    "                     np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 100))\n",
    "Z = svc.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundaries and data points\n",
    "plt.contourf(xx, yy, Z, levels=[-1, 0, 1], alpha=0.5, colors='blue')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='rainbow')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Boundaries')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph displays the decision boundaries generated by the Support Vector Classifier (SVC) model.\n",
    "\n",
    "- Decision Boundaries: The decision boundaries are the lines or curves that separate the different classes in the dataset. They represent the regions where the model assigns a particular class label based on the input features. In the graph, the decision boundaries are depicted as a filled contour plot with the color blue. Each contour level (-1, 0, 1) corresponds to a different class or region.\n",
    "\n",
    "- Data Points: The data points from the synthetic dataset are scattered across the plot. Each data point represents a sample with two feature values. The color of each data point represents its true class label. Different classes are indicated by different colors, creating a rainbow-like effect in the scatter plot.\n",
    "\n",
    "By analyzing the graph, you can gain insights into the performance and behavior of the SVC model:\n",
    "\n",
    "- Separation of Classes: The decision boundaries should ideally separate the different classes effectively. In other words, each class should be enclosed within its respective region on the plot. The graph allows you to visually assess how well the SVC model distinguishes between the classes.\n",
    "\n",
    "- Overlapping or Misclassified Points: If some data points of different classes overlap or are misclassified, it indicates that the model may have difficulty separating those particular instances. The decision boundaries may not perfectly capture the underlying patterns in the data, leading to classification errors.\n",
    "\n",
    "- Decision Thresholds: The contour levels (-1, 0, 1) on the decision boundaries represent the decision thresholds of the model. These thresholds determine how the model assigns class labels based on the predicted values or scores generated by the SVC. By observing the decision boundaries, you can visualize the regions where the model is more confident in assigning a specific class label."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
