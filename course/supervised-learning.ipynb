{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning Algorithms\n",
    "## Sample Datasets\n",
    "In the realm of supervised machine learning, various algorithms are utilized to make predictions based on labeled training data. These algorithms encompass a range of approaches, each with its own strengths and weaknesses. To better understand these algorithms, we will explore a variety of datasets that serve as illustrative examples. Some datasets are artificially generated, providing controlled scenarios to emphasize specific aspects of the algorithms. Other datasets are sourced from real-world applications, offering a glimpse into how these algorithms perform in practical contexts.\n",
    "\n",
    "### Classification Datasets \n",
    "To begin, let's consider a synthetic two-class classification dataset known as the \"forge\" dataset. This dataset comprises two features and is designed to showcase the capabilities of classification algorithms. We can visualize the data points in this dataset using a scatter plot. In the plot, the first feature is represented on the x-axis, while the second feature is represented on the y-axis. Each data point is depicted as a dot, with the color and shape of the dot indicating its corresponding class.\n",
    "\n",
    "Here's an example Python code snippet that generates a scatter plot for the forge dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "caption": "Forge dataset",
    "label": "forge_scatter"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate the forge dataset\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "# Create a scatter plot of the forge dataset\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Forge Dataset - Scatter Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we utilize the make_classification function from the sklearn.datasets module to generate the forge dataset. The n_samples parameter determines the number of data points in the dataset, while n_features specifies the number of features for each data point. The n_informative parameter controls the number of informative features that contribute to the classification, and n_redundant specifies the number of redundant features.\n",
    "\n",
    "After generating the dataset, we use plt.scatter from the matplotlib.pyplot module to create the scatter plot. The X[:, 0] and X[:, 1] indexing allows us to extract the values of the first and second features, respectively, from the dataset. The c=y parameter assigns colors to the data points based on their corresponding class labels, using the 'bwr' colormap. Finally, we add labels to the x-axis, y-axis, and provide a title for the plot before displaying it using plt.show().\n",
    "\n",
    "This code snippet will produce a scatter plot visualizing the forge dataset, enabling us to observe the distribution of data points and the relationship between the two features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Datasets\n",
    "\n",
    "\n",
    "In the realm of machine learning, regression algorithms play a vital role in modeling and predicting continuous target variables. To delve into the intricacies of regression algorithms, we will examine the synthetic wave dataset, which serves as an illustrative example. This dataset consists of a single input feature and a continuous target variable that we aim to model accurately.\n",
    "\n",
    "Let's visualize the wave dataset using a plot, specifically a scatter plot. In this plot, the single input feature will be represented on the x-axis, while the regression target (the output) will be depicted on the y-axis. By visualizing the relationship between the input feature and the target variable, we can gain insights into the underlying patterns and understand how regression algorithms can capture and model these patterns.\n",
    "\n",
    "Here's an example Python code snippet that generates a scatter plot for the wave dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate the wave dataset\n",
    "def generate_wave_dataset():\n",
    "    X = np.linspace(-0.5, 0.5, 100)\n",
    "    y = np.sin(4 * X) + np.random.randn(100) * 0.3\n",
    "    return X.reshape(-1, 1), y.reshape(-1, 1)\n",
    "\n",
    "X, y = generate_wave_dataset()\n",
    "\n",
    "# Create a scatter plot of the wave dataset\n",
    "plt.scatter(X, y, marker='o', color='blue', alpha=0.7)\n",
    "plt.xlabel('Input Feature')\n",
    "plt.ylabel('Regression Target')\n",
    "plt.title('Wave Dataset - Scatter Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we first utilize np.linspace from the NumPy library to create an array X of 100 evenly spaced values ranging from -0.5 to 0.5. These values represent our input feature. Next, we generate the corresponding target variable y by applying the sine function (np.sin) to the input feature, and then adding random Gaussian noise (np.random.randn(100) * 0.3) to create some variability in the target variable.\n",
    "\n",
    "After generating the dataset, we use plt.scatter from the matplotlib.pyplot module to create the scatter plot. The X array represents the input feature values, while the y array represents the regression target values. The marker='o' parameter sets the marker shape to circles, and the color='blue' parameter determines the color of the markers. The alpha=0.7 parameter controls the transparency of the markers.\n",
    "\n",
    "We add labels to the x-axis and y-axis using plt.xlabel and plt.ylabel, respectively. Additionally, we provide a title for the plot using plt.title. Finally, we display the plot using plt.show().\n",
    "\n",
    "Executing this code snippet will generate a scatter plot visualizing the wave dataset, enabling us to observe the relationship between the input feature and the regression target. This visualization aids in comprehending the data distribution and assists in understanding how regression algorithms can model and predict the continuous target variable.\n",
    "\n",
    "We are using these very simple, low-dimensional datasets because we can easily visualize them—a printed page has two dimensions, so data with more than two features is hard to show. Any intuition derived from datasets with few features (also called low-dimensional datasets) might not hold in datasets with many features (highdimensional datasets). As long as you keep that in mind, inspecting algorithms on low-dimensional datasets can be very instructive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the Wisconsin Breast Cancer Dataset\n",
    "In addition to the small synthetic datasets used to illustrate machine learning algorithms, we will also examine two real-world datasets provided by scikit-learn. One of these datasets is the Wisconsin Breast Cancer dataset, commonly referred to as the \"cancer\" dataset. This dataset contains clinical measurements of breast cancer tumors. Each tumor in the dataset is categorized as either \"benign\" (representing harmless tumors) or \"malignant\" (representing cancerous tumors). The goal of the task is to develop a predictive model that can accurately classify tumors as benign or malignant based on the tissue measurements.\n",
    "\n",
    "To load the Wisconsin Breast Cancer dataset, we can utilize the load_breast_cancer function from the scikit-learn library. This function allows us to easily access the dataset and retrieve the necessary features and labels for training our machine learning models.\n",
    "\n",
    "Here's an example Python code snippet that demonstrates how to load the breast cancer dataset using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Retrieve the features (input) and labels (output)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Print the shape of the dataset\n",
    "print(\"Dataset shape:\")\n",
    "print(\"Features (X):\", X.shape)\n",
    "print(\"Labels (y):\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we import the load_breast_cancer function from the sklearn.datasets module. By calling this function, we obtain a Bunch object containing the breast cancer dataset. We assign this object to the variable data.\n",
    "\n",
    "Next, we extract the features (input) and labels (output) from the dataset. The input features are stored in the data.data attribute, while the corresponding labels are available in the data.target attribute. We assign these values to the variables X and y, respectively.\n",
    "\n",
    "Finally, we print the shape of the dataset using the shape attribute of the NumPy arrays X and y. This provides information about the number of samples and the number of features in the dataset.\n",
    "\n",
    "Executing this code will load the Wisconsin Breast Cancer dataset and print the shape of the dataset, indicating the number of samples and features. This dataset can then be further processed and used for training classification models to predict the nature of breast cancer tumors accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of these 569 data points, 212 are labeled as malignant and 357 as benign:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample counts per class:\\n\",\n",
    "      {n: v for n, v in zip(data.target_names, np.bincount(data.target))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a description of the semantic meaning of each feature, we can have a look at the feature_names attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature names:\\n\", data.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find out more about the data by reading cancer.DESCR if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the California Housing Dataset\n",
    "\n",
    "As we dive into the realm of regression, we will explore a real-world dataset known as the California Housing dataset. This dataset provides valuable insights into the housing market in various regions of California. It encompasses a vast array of information that can be utilized to build regression models for predicting housing prices.\n",
    "\n",
    "The California Housing dataset comprises a substantial number of data points, totaling 20,640 instances. Each instance is characterized by eight features, which play a crucial role in determining the price of a house. These features encompass various aspects, including location, population density, median income, and more. By understanding and analyzing these features, we can gain a deeper understanding of the factors that influence housing prices in different areas of California.\n",
    "\n",
    "Here's an example Python code snippet that demonstrates how to load and explore the California Housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "\n",
    "# Load the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "\n",
    "# Create a pandas DataFrame to analyze the dataset\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display the statistical summary of the dataset\n",
    "print(\"Statistical summary of the dataset:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Display the shape of the dataset\n",
    "print(\"Dataset shape:\")\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we import the fetch_california_housing function from the sklearn.datasets module to load the California Housing dataset. The dataset is stored in the data object.\n",
    "\n",
    "We then create a pandas DataFrame, df, using the dataset's data attribute (data.data) and specifying the column names as the feature names (data.feature_names).\n",
    "\n",
    "To explore the dataset, we display the first few rows of the DataFrame using the head() method. This allows us to examine the structure and content of the dataset.\n",
    "\n",
    "We also print the statistical summary of the dataset using the describe() method. This summary provides key statistical measures such as count, mean, standard deviation, minimum, 25th percentile, median (50th percentile), 75th percentile, and maximum for each feature. It offers insights into the distribution and variability of the dataset's features.\n",
    "\n",
    "Additionally, we display the shape of the dataset using the shape attribute of the DataFrame. This provides information about the number of instances (rows) and features (columns) in the dataset.\n",
    "\n",
    "Executing this code will load the California Housing dataset, create a pandas DataFrame for analysis, and provide the first few rows of the dataset, the statistical summary, and the shape of the dataset. These exploratory analyses pave the way for a deeper understanding of the dataset's characteristics, assisting in further data preprocessing, feature engineering, and modeling tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kn-neighbors regression\n",
    "One of the fundamental regression algorithms is k-nearest neighbors regression. This algorithm leverages the concept of nearest neighbors to make predictions for continuous target variables. It is a non-parametric algorithm that operates based on the idea that instances with similar feature values tend to have similar target values.\n",
    "\n",
    "In k-nearest neighbors regression, the prediction for a given data point is determined by considering the average or weighted average of the target values of its k nearest neighbors. The value of k represents the number of neighbors taken into account for the prediction, and it is a hyperparameter that needs to be specified.\n",
    "\n",
    "Here's an example Python code snippet that demonstrates how to implement k-nearest neighbors regression using the scikit-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a k-nearest neighbors regressor\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# Fit the regressor to the training data\n",
    "knn_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = knn_regressor.predict(X_test)\n",
    "\n",
    "# Calculate the R-squared score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the R-squared score\n",
    "print(\"R-squared score:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we use the fetch_california_housing function from the sklearn.datasets module to load the California Housing dataset. This dataset provides information about housing prices in different regions of California.\n",
    "\n",
    "We split the dataset into training and testing sets using the train_test_split function from the sklearn.model_selection module. This allows us to evaluate the performance of the k-nearest neighbors regressor on unseen data.\n",
    "\n",
    "We create a k-nearest neighbors regressor by instantiating the KNeighborsRegressor class from the sklearn.neighbors module. The n_neighbors parameter is set to 5, indicating that the algorithm will consider the 5 nearest neighbors for prediction.\n",
    "\n",
    "After creating the regressor, we fit it to the training data using the fit method. This trains the model on the provided features (X_train) and target values (y_train).\n",
    "\n",
    "We then make predictions on the test data using the predict method of the regressor, and store the predicted target values in the y_pred variable.\n",
    "\n",
    "After making predictions on the test data using the k-nearest neighbors regressor, we calculate the R-squared score by providing the true target values (y_test) and the predicted target values (y_pred) to the r2_score function.\n",
    "\n",
    "By printing the R-squared score, you can assess the goodness of fit of the k-nearest neighbors regression model. A higher R-squared score closer to 1 suggests that the model has a better fit to the data, indicating a higher proportion of the variance in the target variable is explained by the model.\n",
    "\n",
    "Keep in mind that R-squared is just one of several metrics available for evaluating regression models. You can also use other metrics such as mean squared error (MSE), mean absolute error (MAE), and root mean squared error (RMSE) to assess the performance of regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models\n",
    "### Linear models for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression\n",
    "\n",
    "Linear regression is a widely used and fundamental algorithm in the field of machine learning. It is a linear approach to modeling the relationship between a dependent variable (target variable) and one or more independent variables (predictor variables). Linear regression assumes that the relationship between the variables can be approximated by a straight line.\n",
    "\n",
    "The objective of linear regression is to find the best-fit line that minimizes the difference between the predicted values and the actual values. This is achieved by estimating the coefficients (slope and intercept) of the line based on the given data.\n",
    "\n",
    "Linear regression can be used for both simple and multiple regression tasks. In simple linear regression, there is only one independent variable, whereas in multiple linear regression, there are multiple independent variables.\n",
    "\n",
    "Here's an example Python code snippet that demonstrates how to implement linear regression using the scikit-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X, y = generate_wave_dataset()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "linear_regressor = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "linear_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = linear_regressor.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calculate the coefficient of determination (R-squared)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the mean squared error and R-squared score\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared score:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, you first import the necessary modules, including LinearRegression from sklearn.linear_model, mean_squared_error and r2_score from sklearn.metrics, and train_test_split from sklearn.model_selection.\n",
    "\n",
    "You then generate the wave dataset by calling generate_wave_dataset() and assign the input features to X and the corresponding target values to y.\n",
    "\n",
    "Next, you split the dataset into training and testing sets using train_test_split(). The test_size parameter specifies the proportion of the dataset to be used for testing (in this case, 20%), and the random_state parameter ensures reproducibility of the split.\n",
    "\n",
    "After that, you create a linear regression model by instantiating LinearRegression().\n",
    "\n",
    "The model is then fitted to the training data using the fit() method, which trains the model on the provided training features (X_train) and target values (y_train).\n",
    "\n",
    "Subsequently, you make predictions on the test data using the predict() method of the linear regression model, and store the predicted target values in y_pred.\n",
    "\n",
    "To evaluate the performance of the model, you calculate the mean squared error (MSE) using mean_squared_error() by providing the true target values (y_test) and the predicted target values (y_pred). Additionally, you calculate the coefficient of determination (R-squared) using r2_score().\n",
    "\n",
    "Finally, you print the mean squared error and R-squared score to assess the accuracy and goodness of fit of the linear regression model on the generated wave dataset.\n",
    "\n",
    "Executing this code will fit a linear regression model to the wave dataset, make predictions on the test data, calculate the mean squared error, and calculate the R-squared score. These metrics provide insights into the performance and accuracy of the linear regression model in capturing the underlying relationship between the input feature and the target variable in the wave dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the intercept and coefficients\n",
    "intercept = linear_regressor.intercept_\n",
    "coefficients = linear_regressor.coef_\n",
    "\n",
    "# Print the intercept and coefficients\n",
    "print(\"Intercept:\", intercept)\n",
    "print(\"Coefficients:\", coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept_ attribute is always a single float number, while the coef_ attribute is a NumPy array with one entry per input feature. As we only have a single input feature in the wave dataset, lr.coef_ only has a single entry.\n",
    "Let’s look at the training set and test set performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the training and test data\n",
    "y_train_pred = linear_regressor.predict(X_train)\n",
    "y_test_pred = linear_regressor.predict(X_test)\n",
    "\n",
    "# Compute the scores (R-squared)\n",
    "train_score = linear_regressor.score(X_train, y_train)\n",
    "test_score = linear_regressor.score(X_test, y_test)\n",
    "\n",
    "# Print the scores\n",
    "print(\"Training score:\", train_score)\n",
    "print(\"Test score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, after fitting the linear regression model to the training data, we use the predict() method to make predictions on both the training data (X_train) and the test data (X_test).\n",
    "\n",
    "Next, we compute the scores (R-squared) by calling the score() method on the linear regression model and providing the input features and target values for both the training set and the test set. The score() method automatically computes the R-squared value for the predictions made by the model.\n",
    "\n",
    "Finally, we print the training score and the test score to assess the performance of the model on both sets.\n",
    "\n",
    "Analyzing the scores can provide insights into whether the model is overfitting or underfitting:\n",
    "\n",
    "- If the training score is significantly higher than the test score, it suggests overfitting. In this case, the model is performing well on the training data but generalizing poorly to unseen data.\n",
    "- If both the training score and the test score are low, it suggests underfitting. This indicates that the model is not capturing the underlying patterns in the data, resulting in poor performance on both the training and test sets.\n",
    "- If the training score and the test score are relatively close and high, it suggests a good fit. This implies that the model is generalizing well to unseen data, indicating a good balance between capturing the patterns in the training data and generalizing to new observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's repeat the same analysis on a more complex dataset, the California Housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "\n",
    "# Extract the features and target variable\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "linear_regressor = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "linear_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Access the intercept and coefficients\n",
    "intercept = linear_regressor.intercept_\n",
    "coefficients = linear_regressor.coef_\n",
    "\n",
    "# Print the intercept and coefficients\n",
    "print(\"Intercept:\", intercept)\n",
    "print(\"Coefficients:\", coefficients)\n",
    "\n",
    "# Make predictions on the training and test data\n",
    "y_train_pred = linear_regressor.predict(X_train)\n",
    "y_test_pred = linear_regressor.predict(X_test)\n",
    "\n",
    "# Compute the scores (R-squared)\n",
    "train_score = linear_regressor.score(X_train, y_train)\n",
    "test_score = linear_regressor.score(X_test, y_test)\n",
    "\n",
    "# Print the scores\n",
    "print(\"Training score:\", train_score)\n",
    "print(\"Test score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the training score of *0.6126* and the test score of *0.5758*, we can analyze the performance of the linear regression model on the California Housing dataset.\n",
    "\n",
    "Since the training score is slightly higher than the test score, it indicates that the model is performing better on the training data compared to the unseen test data. This discrepancy suggests some degree of overfitting, meaning that the model might be capturing noise or specific patterns present in the training data that do not generalize well to new observations.\n",
    "\n",
    "The relatively lower test score implies that the model's performance on the test set is moderate. It suggests that the model captures some of the underlying patterns in the data but may not generalize optimally to new, unseen data points.\n",
    "\n",
    "To mitigate overfitting, you could consider applying regularization techniques or exploring more complex models that can capture additional nuances in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression\n",
    "\n",
    "Ridge regression is a variant of linear regression that introduces a regularization term to address the issue of overfitting. It is particularly useful when dealing with datasets that have a large number of features or when multicollinearity (high correlation) exists among the features.\n",
    "\n",
    "The key idea behind ridge regression is to add a penalty term to the loss function during model training. This penalty term, controlled by a hyperparameter called alpha (λ), shrinks the magnitude of the coefficients, encouraging them to be smaller. By doing so, ridge regression can reduce the model's sensitivity to the input variables and improve its generalization ability.\n",
    "\n",
    "To implement ridge regression in scikit-learn, you can use the Ridge class from the sklearn.linear_model module. It offers similar functionalities to linear regression but includes the regularization term.\n",
    "\n",
    "Here's an example code snippet that demonstrates how to perform ridge regression on the California Housing dataset, compare the results with linear regression, and analyze the coefficients and intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Load the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "\n",
    "# Extract the features and target variable\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "linear_regressor = LinearRegression()\n",
    "\n",
    "# Fit the linear regression model to the training data\n",
    "linear_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using linear regression\n",
    "y_train_pred_linear = linear_regressor.predict(X_train)\n",
    "y_test_pred_linear = linear_regressor.predict(X_test)\n",
    "\n",
    "# Compute the scores for linear regression\n",
    "train_score_linear = r2_score(y_train, y_train_pred_linear)\n",
    "test_score_linear = r2_score(y_test, y_test_pred_linear)\n",
    "\n",
    "# Create a ridge regression model\n",
    "ridge_regressor = Ridge(alpha=100.0)  # You can adjust the value of alpha as needed\n",
    "\n",
    "# Fit the ridge regression model to the training data\n",
    "ridge_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using ridge regression\n",
    "y_train_pred_ridge = ridge_regressor.predict(X_train)\n",
    "y_test_pred_ridge = ridge_regressor.predict(X_test)\n",
    "\n",
    "# Compute the scores for ridge regression\n",
    "train_score_ridge = ridge_regressor.score(X_train, y_train)\n",
    "test_score_ridge = ridge_regressor.score(X_test, y_test)\n",
    "\n",
    "# Print the scores\n",
    "print(\"Linear Regression - Training score:\", train_score_linear)\n",
    "print(\"Linear Regression - Test score:\", test_score_linear)\n",
    "print(\"Ridge Regression - Training score:\", train_score_ridge)\n",
    "print(\"Ridge Regression - Test score:\", test_score_ridge)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Linear Regression - Coefficients:\", linear_regressor.coef_)\n",
    "print(\"Linear Regression - Intercept:\", linear_regressor.intercept_)\n",
    "\n",
    "# Print the coefficients and intercept for ridge regression\n",
    "print(\"Ridge Regression - Coefficients:\", ridge_regressor.coef_)\n",
    "print(\"Ridge Regression - Intercept:\", ridge_regressor.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we start by loading the California Housing dataset using the fetch_california_housing function. We extract the features (X) and target variable (y) from the dataset.\n",
    "\n",
    "Next, we split the dataset into training and test sets using train_test_split from sklearn.model_selection.\n",
    "\n",
    "We create a linear regression model by instantiating the LinearRegression class and fit it to the training data.\n",
    "\n",
    "Using the trained linear regression model, we make predictions on both the training and test data.\n",
    "\n",
    "We compute the R-squared scores for linear regression by comparing the predicted values with the true values.\n",
    "\n",
    "Then, we create a ridge regression model by instantiating the Ridge class, specifying the value of the alpha parameter (λ) as needed. The alpha value controls the strength of the regularization.\n",
    "\n",
    "We fit the ridge regression model to the training data and make predictions on both the training and test data.\n",
    "\n",
    "Similarly, we compute the R-squared scores for ridge regression.\n",
    "\n",
    "Finally, we print the scores for both linear regression and ridge regression, as well as the coefficients and intercept for each model.\n",
    "\n",
    "By comparing the scores between linear regression and ridge regression, you can assess the impact of regularization in ridge regression. Additionally, by analyzing the coefficients and intercept, you can observe how ridge regression modifies the magnitude and importance of the features compared to linear regression.\n",
    "\n",
    "In ridge regression, the alpha parameter (also known as lambda, denoted as α) controls the strength of regularization applied to the model. It determines the trade-off between the model's simplicity (smaller coefficients) and its ability to fit the training data well.\n",
    "\n",
    "Here's how different values of alpha can affect the result in ridge regression:\n",
    "\n",
    "- Low Alpha (Close to 0):\n",
    "\n",
    "    When alpha is very close to zero, the regularization term has minimal impact. The model will behave similar to linear regression, aiming to fit the training data with little constraint on the coefficients. This can lead to potential overfitting, especially if the dataset has a large number of features or multicollinearity. The model may have larger coefficients and could be more sensitive to noise in the training data.\n",
    "\n",
    "- Moderate Alpha:\n",
    "\n",
    "    A moderate value of alpha strikes a balance between simplicity and fitting the training data well. It helps to reduce the impact of multicollinearity and makes the model less sensitive to noise in the data. The model tends to have smaller coefficients compared to linear regression, but not as small as with high alpha values. It aims to provide a good trade-off between bias and variance, leading to better generalization to unseen data.\n",
    "\n",
    "- High Alpha:\n",
    "\n",
    "    As alpha increases, the regularization term becomes more dominant. The model will heavily constrain the coefficients, pushing them towards zero. High alpha values encourage sparsity, meaning some coefficients may become exactly zero, effectively performing feature selection. This helps to simplify the model and mitigate the impact of irrelevant features, but it may lead to underfitting if the true underlying relationship is complex. Choosing the appropriate value of alpha depends on the specific dataset and problem at hand. Cross-validation techniques can be employed to find the optimal alpha value that balances model complexity and performance on unseen data.\n",
    "\n",
    "By adjusting the alpha value, you can control the amount of regularization in ridge regression and tailor the model's behavior to achieve the desired balance between flexibility and generalization.\n",
    "\n",
    "Comparing these scores, we observe that ridge regression with alpha 100 slightly improves the test score compared to linear regression. This suggests that regularization with a high alpha value helped the model generalize better to unseen data in this particular case.\n",
    "\n",
    "Analyzing the coefficients, we notice that ridge regression with alpha 100 has significantly smaller coefficients compared to linear regression. The regularization effectively shrinks the magnitude of the coefficients toward zero, making the model less sensitive to noise and reducing the impact of irrelevant features. This can help prevent overfitting and improve the model's generalization ability.\n",
    "\n",
    "Additionally, the intercept value for ridge regression with alpha 100 is slightly smaller compared to linear regression. The intercept represents the expected target value when all features are zero.\n",
    "\n",
    "Overall, these results indicate that ridge regression with a higher alpha value of 100 helped improve the model's performance on the test set compared to linear regression. It achieved this by reducing the impact of irrelevant features and noise through regularization, leading to a more generalizable model.\n",
    "\n",
    "**The Ridge model makes a trade-off between the simplicity of the model (near-zero coefficients) and its performance on the training set. How much importance the model places on simplicity versus training set performance can be specified by the user, using the alpha parameter.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a more qualitative insight into how the alpha parameter changes the model, inspect the coef_ attribute of models with different values of alpha. A higher alpha means a more restricted model, so we expect the entries of coef_ to have smaller magnitude for a high value of alpha than for a low value of alpha. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models and alpha values\n",
    "models = [\n",
    "    ('Ridge alpha=10', Ridge(alpha=100)),\n",
    "    ('Ridge alpha=100', Ridge(alpha=1000)),\n",
    "    ('Ridge alpha=0.1', Ridge(alpha=0.1)),\n",
    "    ('Linear Regression', LinearRegression())\n",
    "]\n",
    "\n",
    "# Fit the models to the training data and collect the coefficients\n",
    "coefficients = []\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    coefficients.append(model.coef_)\n",
    "\n",
    "# Plot the coefficient magnitudes for each model\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, (name, _) in enumerate(models):\n",
    "    plt.plot(coefficients[i], marker='o', label=name)\n",
    "    \n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.axhline(0, color='gray', linestyle='dashed')\n",
    "plt.xlim(0, len(coefficients[0])-1)\n",
    "plt.ylim(-1, 2)\n",
    "plt.legend()\n",
    "\n",
    "# Add a title and comment the graph\n",
    "plt.title(\"Comparison of Coefficient Magnitudes\")\n",
    "plt.annotate(\"Higher alpha\\n(smaller coefficients)\",\n",
    "             xy=(3, 0.2), xycoords='data',\n",
    "             xytext=(-40, -50), textcoords='offset points',\n",
    "             arrowprops=dict(arrowstyle=\"->\"),\n",
    "             fontsize=10, color='red')\n",
    "plt.annotate(\"Linear Regression\\n(no regularization)\",\n",
    "             xy=(3, 0.9), xycoords='data',\n",
    "             xytext=(-40, 20), textcoords='offset points',\n",
    "             arrowprops=dict(arrowstyle=\"->\"),\n",
    "             fontsize=10, color='blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graph generated by the code, the x-axis represents the coefficient index, which corresponds to the index position of the coefficient in the model's coefficient array. Each feature in the dataset is associated with a coefficient, and the coefficient index indicates the feature's position in the coefficient array.\n",
    "\n",
    "On the y-axis, we have the coefficient magnitude, which represents the magnitude or value of the coefficient associated with each feature. The coefficient magnitude reflects the weight or importance of each feature in the model's prediction.\n",
    "\n",
    "The plot showcases the coefficient magnitudes for each feature of the dataset, allowing for a comparison between different models. Each model is represented by different markers in the plot.\n",
    "\n",
    "By examining the coefficient magnitudes, we can observe how they vary across the features for different models. Comparing the coefficient magnitudes provides insights into the impact of regularization on the weights assigned to each feature. In ridge regression, higher alpha values tend to shrink the coefficients towards zero, resulting in smaller magnitudes. Thus, the graph enables us to understand how regularization affects the importance of each feature and the level of feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Regression\n",
    "\n",
    "Lasso regression, also known as L1 regularization, is a linear regression technique that incorporates regularization by adding a penalty term to the cost function. It is particularly useful for feature selection and can be effective when dealing with high-dimensional datasets.\n",
    "\n",
    "Regularization is a technique used to prevent overfitting and improve the generalization ability of a model. In Lasso regression, the regularization term is the L1 norm of the coefficient vector multiplied by a regularization parameter (alpha). The L1 norm encourages sparsity in the coefficients by promoting some coefficients to be exactly zero. As a result, Lasso regression can effectively perform feature selection by driving the coefficients of less important features to zero.\n",
    "\n",
    "The effect of regularization in Lasso regression is twofold:\n",
    "\n",
    "- Feature Selection: The L1 penalty encourages sparse solutions, meaning it tends to assign zero coefficients to irrelevant or less important features. This property makes Lasso regression capable of automatically selecting relevant features and discarding the irrelevant ones, simplifying the model and potentially improving interpretability.\n",
    "- Shrinking Coefficients: Lasso regularization also shrinks the magnitudes of non-zero coefficients towards zero. This reduces the impact of less important features, reducing the model's complexity and potential for overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s apply the lasso to the extended Boston Housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "\n",
    "# Extract the features and target variable\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Lasso model\n",
    "lasso = Lasso(alpha=1)  # Set the regularization parameter alpha\n",
    "\n",
    "# Fit the Lasso model to the training data\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Analyze the results on the training set\n",
    "train_score = lasso.score(X_train, y_train)\n",
    "print(\"Training R-squared score:\", train_score)\n",
    "\n",
    "# Analyze the results on the test set\n",
    "test_score = lasso.score(X_test, y_test)\n",
    "print(\"Test R-squared score:\", test_score)\n",
    "\n",
    "# Count the number of features used\n",
    "num_features_used = np.sum(coefficients != 0)\n",
    "print(\"Number of features used:\", num_features_used)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is not performing very well. The training score is very low, indicating that the model is not able to fit the training data well. The test score is also low, suggesting that the model is not able to generalize well to unseen data. Let's try with different values of alpha to see if we can improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the alpha values to test\n",
    "alpha_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Create empty lists to store the results\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "coefficients = []\n",
    "\n",
    "# Fit the Lasso model for each alpha value\n",
    "for alpha in alpha_values:\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    \n",
    "    # Collect the results\n",
    "    train_scores.append(lasso.score(X_train, y_train))\n",
    "    test_scores.append(lasso.score(X_test, y_test))\n",
    "    coefficients.append(lasso.coef_)\n",
    "\n",
    "# Plot the coefficient magnitudes\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, alpha in enumerate(alpha_values):\n",
    "    plt.plot(coefficients[i], marker='o', label=f\"Alpha={alpha}\")\n",
    "    \n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.axhline(0, color='gray', linestyle='dashed')\n",
    "plt.xlim(0, len(coefficients[0])-1)\n",
    "plt.ylim(-0.5, 1)\n",
    "plt.legend()\n",
    "plt.title(\"Lasso Regression - Coefficient Magnitudes\")\n",
    "plt.show()\n",
    "\n",
    "# Print the results\n",
    "for i, alpha in enumerate(alpha_values):\n",
    "    print(f\"Alpha={alpha}:\")\n",
    "    print(\"Training R-squared score:\", train_scores[i])\n",
    "    print(\"Test R-squared score:\", test_scores[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion on Linear Regression\n",
    "\n",
    "**Advantages of Lasso Regression (L1 Regularization):**\n",
    "\n",
    "*Feature Selection*: Lasso regression performs automatic feature selection by driving the coefficients of irrelevant features to zero. It can effectively handle high-dimensional datasets and select the most relevant features, simplifying the model and improving interpretability.\n",
    "*Sparse Solutions*: Lasso encourages sparse solutions, meaning it assigns zero coefficients to less important features. This leads to a more interpretable model with a subset of features that have a strong impact on the target variable.\n",
    "Handles Multicollinearity: Lasso regression can handle multicollinearity among features by selecting one feature from a group of highly correlated features and setting the coefficients of the rest to zero.\n",
    "\n",
    "**Potential Problems of Lasso Regression:**\n",
    "\n",
    "*Arbitrary Feature Selection*: Lasso may arbitrarily select features when they are highly correlated or have similar predictive power. The choice of features may vary with slight changes in the data or random seed, which can make the model less stable.\n",
    "*Inconsistent Results*: Lasso tends to have unstable coefficient estimates, particularly when the number of predictors is large compared to the number of observations. This instability can lead to inconsistent results and difficulties in model interpretation.\n",
    "*Limited Stability for Coefficient Magnitudes*: Lasso may not be able to stabilize the magnitudes of coefficients when features have similar importance. As a result, it may not accurately represent the true underlying relationships between features and the target variable.\n",
    "\n",
    "**Advantages of Ridge Regression (L2 Regularization):**\n",
    "\n",
    "*Improved Generalization*: Ridge regression improves generalization by reducing model complexity and potential overfitting. It shrinks the magnitudes of the coefficients towards zero without completely eliminating any feature, allowing all features to contribute to the predictions.\n",
    "*Robust to Multicollinearity*: Ridge regression is effective in handling multicollinearity among features by reducing the impact of correlated features. It distributes the weights among the correlated features, providing more stable coefficient estimates.\n",
    "*Stable Results*: Ridge regression provides more stable and reliable coefficient estimates, especially when dealing with datasets with a high dimensionality or a small number of observations.\n",
    "\n",
    "**Potential Problems of Ridge Regression:**\n",
    "\n",
    "*Inability for Feature Selection*: Ridge regression does not perform explicit feature selection, as it does not drive coefficients to zero. It retains all features in the model, which can lead to reduced interpretability when dealing with a large number of features.\n",
    "*Shrinking All Coefficients*: Ridge regression shrinks all coefficients towards zero, including the coefficients of irrelevant or unimportant features. It may retain some noise or irrelevant features in the model, which can impact prediction accuracy in certain cases.\n",
    "*Limited Interpretability*: The penalty term in Ridge regression does not result in exact sparsity or elimination of coefficients, making it challenging to identify the most influential features in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn offers the ElasticNet class, which combines the penalties of both Lasso (L1 regularization) and Ridge (L2 regularization) into a single model. ElasticNet provides a balance between feature selection and coefficient magnitude stabilization by incorporating both regularization techniques.\n",
    "\n",
    "The ElasticNet regularization term is a linear combination of the L1 and L2 penalties, controlled by two hyperparameters: alpha and l1_ratio.\n",
    "\n",
    "- Alpha: The alpha parameter controls the overall regularization strength. A higher alpha value increases the regularization and shrinks the coefficients towards zero, reducing overfitting. Conversely, a lower alpha value decreases the regularization and allows the model to fit the training data more closely.\n",
    "- L1 Ratio: The l1_ratio parameter determines the balance between L1 and L2 regularization. A value of 1 corresponds to pure L1 (Lasso) regularization, while a value of 0 corresponds to pure L2 (Ridge) regularization. Intermediate values allow for a combination of both penalties.\n",
    "\n",
    "By adjusting the alpha and l1_ratio parameters, you can control the trade-off between feature selection and coefficient stabilization in the ElasticNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Load the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the ElasticNet model\n",
    "elastic_net = ElasticNet(alpha=0.5, l1_ratio=0.5)\n",
    "elastic_net.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = elastic_net.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared score:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use the ElasticNet class to create an ElasticNet model with an alpha value of 0.5 and an l1_ratio value of 0.5, indicating an equal balance between L1 and L2 regularization. The model is then fitted to the training data, and predictions are made on the test set. Finally, we evaluate the model's performance using the R-squared score.\n",
    "\n",
    "By adjusting the alpha and l1_ratio parameters, you can find the optimal balance between feature selection and coefficient stabilization, leading to an ElasticNet model that suits your specific dataset and modeling requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear models for classification\n",
    "\n",
    "Linear models for classification are a family of algorithms used to solve classification problems. These models aim to find a linear decision boundary that separates different classes in the input space. They assign a class label to new instances based on their position relative to this decision boundary.\n",
    "\n",
    "Some common linear models for classification are:\n",
    "\n",
    "- Logistic Regression: Logistic Regression is a widely used linear model for binary classification. It estimates the probability of an instance belonging to a particular class using the logistic function. The decision boundary is determined by a linear combination of the input features.\n",
    "- Linear Support Vector Machines (SVM): Linear SVMs are linear models used for both binary and multi-class classification. They aim to find a hyperplane that maximally separates different classes in the input space. SVMs use support vectors, which are the instances closest to the decision boundary, to define the separating hyperplane.\n",
    "- Perceptron: The Perceptron algorithm is a linear model used for binary classification. It iteratively updates the weights of the features based on misclassified instances until all instances are classified correctly or a maximum number of iterations is reached.\n",
    "- Stochastic Gradient Descent (SGD) Classifier: SGD Classifier is a linear classifier that uses stochastic gradient descent to optimize the model's coefficients. It can be used for both binary and multi-class classification tasks and is particularly useful for large-scale datasets.\n",
    "\n",
    "**Advantages of Linear Models for Classification:**\n",
    "\n",
    "- Computational Efficiency: Linear models are generally computationally efficient, making them suitable for large datasets and real-time applications.\n",
    "- Interpretability: Linear models provide interpretable results by assigning weights to the input features, allowing for feature importance analysis.\n",
    "- Low Complexity: Linear models have relatively low complexity and fewer hyperparameters compared to more complex algorithms, making them easier to understand and tune.\n",
    "\n",
    "**Limitations of Linear Models for Classification:**\n",
    "\n",
    "- Limited Representation Power: Linear models assume that the decision boundary is a linear function, which may not accurately capture complex, nonlinear relationships in the data.\n",
    "- Sensitivity to Feature Scaling: Linear models can be sensitive to the scale of the input features. It is often necessary to scale or normalize the features to ensure optimal performance.\n",
    "- Potential Overfitting: If the number of features is much larger than the number of training instances, linear models may be prone to overfitting. Regularization techniques, such as L1 or L2 regularization, can mitigate this issue.\n",
    "\n",
    "Overall, linear models for classification offer a good balance between simplicity, interpretability, and computational efficiency. They are suitable for a wide range of classification tasks, particularly when the decision boundary is relatively simple or linear in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "Logistic Regression is a popular linear model used for binary classification tasks. It is particularly effective in scenarios where the goal is to predict the probability of an instance belonging to a particular class.\n",
    "\n",
    "In Logistic Regression, the model estimates the probability of an instance belonging to the positive class using the logistic function (also known as the sigmoid function). The decision boundary is determined by a linear combination of the input features, and the logistic function maps the linear output to a probability value between 0 and 1.\n",
    "\n",
    "The Breast Cancer dataset, commonly used for illustrating classification algorithms, contains clinical measurements of breast cancer tumors. Each tumor is labeled as \"benign\" (non-cancerous) or \"malignant\" (cancerous). The task is to predict whether a tumor is malignant based on the measurements of the tissue.\n",
    "\n",
    "To demonstrate the application of Logistic Regression on the Breast Cancer dataset, we can use scikit-learn library. Here's an example code that performs Logistic Regression on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "logistic_regression = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Fit the model to the training data\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = logistic_regression.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Create empty lists to store the coefficients and accuracy results\n",
    "coefficients = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "# Perform Logistic Regression with different values of C\n",
    "for C in C_values:\n",
    "    # Create a Logistic Regression model with the specified C and max_iter\n",
    "    logistic_regression = LogisticRegression(C=C, max_iter=10000)\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    logistic_regression.fit(X_train, y_train)\n",
    "    \n",
    "    # Collect the coefficients\n",
    "    coefficients.append(logistic_regression.coef_.ravel())\n",
    "    \n",
    "    # Make predictions on the training and test data\n",
    "    y_train_pred = logistic_regression.predict(X_train)\n",
    "    y_test_pred = logistic_regression.predict(X_test)\n",
    "    \n",
    "    # Calculate the accuracy scores\n",
    "    train_accuracy.append(accuracy_score(y_train, y_train_pred))\n",
    "    test_accuracy.append(accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "# Convert the coefficients to a NumPy array\n",
    "coefficients = np.array(coefficients)\n",
    "\n",
    "# Plot the coefficients\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['r', 'g', 'b', 'c', 'm', 'y']\n",
    "for i, C in enumerate(C_values):\n",
    "    plt.plot(range(coefficients.shape[1]), coefficients[i], marker='o', color=colors[i], label=f\"C={C}\")\n",
    "\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient value\")\n",
    "plt.title(\"Logistic Regression Coefficients\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the accuracy results\n",
    "for i, C in enumerate(C_values):\n",
    "    print(f\"C={C}:\")\n",
    "    print(\"Training Accuracy:\", train_accuracy[i])\n",
    "    print(\"Test Accuracy:\", test_accuracy[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varying the regularization parameter C in Logistic Regression can have different effects on the model's performance and the resulting coefficient values. C is the inverse of the regularization strength, so smaller values of C correspond to stronger regularization, while larger values of C correspond to weaker regularization.\n",
    "\n",
    "**Strong Regularization (Small C Values):**\n",
    "\n",
    "Smaller values of C increase the strength of regularization, leading to more pronounced shrinkage of coefficient values towards zero.\n",
    "Strong regularization can help prevent overfitting by reducing the impact of individual features and preventing excessive reliance on specific variables.\n",
    "With strong regularization, the model's coefficients tend to be smaller in magnitude, indicating a more generalized representation of the data.\n",
    "The decision boundary may become simpler and less prone to overfitting, but it may also result in some loss of accuracy on the training set.\n",
    "\n",
    "**Weak Regularization (Large C Values):**\n",
    "\n",
    "Larger values of C correspond to weaker regularization, allowing the model to fit the training data more closely.\n",
    "Weak regularization may lead to larger coefficient values, indicating a higher reliance on individual features in the decision-making process.\n",
    "The model may become more complex and more prone to overfitting, particularly when the number of features is large compared to the number of training instances.\n",
    "With weak regularization, the decision boundary may follow the training data closely, potentially achieving high accuracy on the training set but potentially sacrificing generalization to new, unseen data.\n",
    "By varying the C parameter, you can control the trade-off between model complexity and overfitting. Smaller values of C favor simpler models with more robust generalization, while larger values of C allow for more complex models that closely fit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear models for multiclass classification\n",
    "\n",
    "Linear models for multiclass classification are a class of algorithms that are used to solve classification problems with more than two classes. These algorithms extend the principles of binary classification, where the goal is to separate instances into two classes, to the scenario where there are multiple classes to be predicted.\n",
    "\n",
    "There are different linear models that can be used for multiclass classification, including:\n",
    "\n",
    "1. One-vs-Rest (OvR) or One-vs-All (OvA):\n",
    "    In this approach, a separate binary classifier is trained for each class against the rest of the classes. For example, if there are three classes (A, B, C), three binary classifiers would be trained: A vs. (B, C), B vs. (A, C), and C vs. (A, B). During prediction, the class with the highest confidence score from the binary classifiers is assigned to the instance.\n",
    "2. Multinomial Logistic Regression (Softmax Regression):\n",
    "    Multinomial Logistic Regression generalizes binary logistic regression to multiple classes using the softmax function. Softmax assigns a probability to each class, and the class with the highest probability is predicted. The model's parameters are estimated through the maximization of the likelihood of the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s apply the one-vs.-rest method to a simple three-class classification dataset. We use a two-dimensional dataset, where each class is given by data sampled from a Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic three-class dataset\n",
    "X, y = make_classification(n_samples=300, n_features=2, n_informative=2, n_redundant=0,\n",
    "                           n_clusters_per_class=1, n_classes=3, random_state=42)\n",
    "\n",
    "# Plot the data points with class labels\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
    "\n",
    "colors = scatter.cmap(scatter.norm(np.unique(y)))\n",
    "\n",
    "# Create custom legend\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', label='Class 0', markerfacecolor=colors[0], markersize=10),\n",
    "                     plt.Line2D([0], [0], marker='o', color='w', label='Class 1', markerfacecolor=colors[1], markersize=10),\n",
    "                        plt.Line2D([0], [0], marker='o', color='w', label='Class 2', markerfacecolor=colors[2], markersize=10)]\n",
    "plt.legend(handles=legend_elements)\n",
    "\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.title(\"Data Points with Class Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet demonstrates the implementation of a One-vs-Rest (OvR) classifier for multiclass classification using the scikit-learn library. The OvR strategy is employed to extend a binary classifier, such as Logistic Regression, to handle multiple classes.\n",
    "\n",
    "First, an instance of the OneVsRestClassifier class is created, specifying LogisticRegression() as the base classifier. This sets up the framework for training multiple binary classifiers, where each classifier is responsible for distinguishing one class from the rest.Next, the model is fitted to the provided data, where X represents the input features and y denotes the corresponding class labels. The classifier is trained using the OvR approach, allowing it to learn the relationships between the features and the multiclass targets.\n",
    "\n",
    "To visualize the decision boundaries, a mesh grid is created with a specified step size (h). Predictions are made on the mesh grid using the trained OvR classifier, producing a grid of class labels. These predictions are reshaped to match the shape of the mesh grid.\n",
    "\n",
    "The decision boundaries and data points are then plotted using contourf to display the regions where each class is predicted. The data points are represented as a scatter plot, with each class assigned a different color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Create a Logistic Regression model using the one-vs-rest strategy\n",
    "ovr_classifier = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "# Fit the model to the data\n",
    "ovr_classifier.fit(X, y)\n",
    "\n",
    "# Plot the decision boundaries and data points\n",
    "h = 0.02  # Step size for mesh grid\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Make predictions on the mesh grid\n",
    "Z = ovr_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundaries and data points\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
    "\n",
    "colors = scatter.cmap(scatter.norm(np.unique(y)))\n",
    "\n",
    "# Create custom legend\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', label='Class 0', markerfacecolor=colors[0], markersize=10),\n",
    "                     plt.Line2D([0], [0], marker='o', color='w', label='Class 1', markerfacecolor=colors[1], markersize=10),\n",
    "                        plt.Line2D([0], [0], marker='o', color='w', label='Class 2', markerfacecolor=colors[2], markersize=10)]\n",
    "plt.legend(handles=legend_elements)\n",
    "\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.title(\"One-vs-Rest Classifier - Logistic Regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine more in detail how the OvR classifier works. The following code snippet demonstrates the implementation of a One-vs-Rest (OvR) classifier for multiclass classification using the scikit-learn library.\n",
    "\n",
    "A LinearSVC model is then instantiated to perform the classification. The model is trained on the dataset by calling the fit method and providing the input features X and corresponding class labels y.\n",
    "\n",
    "To visualize the decision boundaries for each class, the code iterates over the three classes. For each iteration, a binary target is created, where the current class is labeled as 1 and the rest are labeled as 0. The LinearSVC model is trained on this binary target to learn the decision boundaries specific to that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Create a LinearSVC model\n",
    "linear_svc = LinearSVC()\n",
    "\n",
    "# Fit the model to the data\n",
    "linear_svc.fit(X, y)\n",
    "\n",
    "# Plot the decision boundaries for each class individually\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for i in range(3):\n",
    "    # Create a binary target where class i is 1 and the rest are 0\n",
    "    binary_target = np.where(y == i, 1, 0)\n",
    "\n",
    "    # Fit the LinearSVC model to the binary target\n",
    "    linear_svc.fit(X, binary_target)\n",
    "\n",
    "    # Generate a mesh grid to visualize the decision boundaries\n",
    "    h = 0.02  # Step size for mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Make predictions on the mesh grid\n",
    "    Z = linear_svc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot the decision boundaries and data points for the current class\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
    "    plt.xlabel(\"Feature 0\")\n",
    "    plt.ylabel(\"Feature 1\")\n",
    "    plt.title(f\"Class {i}\")\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code superimposes the decision boundaries for each class on the scatter plot of the data points. We can see that a triangle is generated in the middle of the plot, which corresponds to the region where all three classes are predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LinearSVC model\n",
    "linear_svc = LinearSVC()\n",
    "\n",
    "# Fit the model to the data\n",
    "linear_svc.fit(X, y)\n",
    "\n",
    "# Generate a mesh grid to visualize the decision boundaries\n",
    "h = 0.02  # Step size for mesh grid\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Make predictions on the mesh grid\n",
    "Z = linear_svc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundaries and data points\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
    "\n",
    "colors = scatter.cmap(scatter.norm(np.unique(y)))\n",
    "\n",
    "# Create custom legend\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', label='Class 0', markerfacecolor=colors[0], markersize=10),\n",
    "                     plt.Line2D([0], [0], marker='o', color='w', label='Class 1', markerfacecolor=colors[1], markersize=10),\n",
    "                        plt.Line2D([0], [0], marker='o', color='w', label='Class 2', markerfacecolor=colors[2], markersize=10)]\n",
    "plt.legend(handles=legend_elements)\n",
    "\n",
    "# Plot decision boundaries for each class individually\n",
    "for i in range(3):\n",
    "    # Create a binary target where class i is 1 and the rest are -1\n",
    "    binary_target = np.where(y == i, 1, -1)\n",
    "\n",
    "    # Fit the LinearSVC model to the binary target\n",
    "    linear_svc.fit(X, binary_target)\n",
    "\n",
    "    # Compute decision function values on the mesh grid\n",
    "    decision_values = linear_svc.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    decision_values = decision_values.reshape(xx.shape)\n",
    "\n",
    "    # Plot the decision boundary for the current class\n",
    "    plt.contour(xx, yy, decision_values, levels=[0], colors='k', linestyles='dashed')\n",
    "\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.title(\"Decision Boundaries for Three Classes - LinearSVC\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So what about the triangle in the middle of the plot? All three binary classifiers classify points there as “rest.” Which class would a point there be assigned to? The answer is the one with the highest value for the classification formula: the class of the closest line.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees\n",
    "\n",
    "Decision trees are a popular and intuitive machine learning algorithm used for both classification and regression tasks. They are versatile, interpretable, and can handle both numerical and categorical data. Decision trees make predictions by recursively partitioning the input space based on the features, creating a tree-like structure of decisions and outcomes.\n",
    "\n",
    "At each internal node of the tree, a decision is made based on a specific feature and a threshold value. This decision splits the data into two or more branches, leading to subsequent nodes. The splitting process aims to maximize the purity of the data within each resulting subset. In classification, purity refers to the homogeneity of the class labels, while in regression, it pertains to minimizing the variance of the target variable.\n",
    "\n",
    "Leaf nodes, also known as terminal nodes, represent the final outcomes or predictions of the decision tree. Each leaf node corresponds to a specific class label in classification or a predicted value in regression. The path from the root node to a leaf node represents a sequence of feature-based decisions that determine the final prediction.\n",
    "\n",
    "Decision trees have several advantages. Firstly, they are easy to interpret and visualize, making them useful for explaining the decision-making process to stakeholders. They can handle both numerical and categorical features without requiring extensive data preprocessing. Decision trees can capture non-linear relationships and interactions between features. Additionally, decision trees can handle missing values by utilizing surrogate splits.\n",
    "\n",
    "However, decision trees are prone to overfitting, particularly when the tree becomes too deep and complex. Overfitting occurs when the tree captures noise and idiosyncrasies in the training data, leading to poor generalization on unseen data. To mitigate overfitting, pruning techniques and regularization parameters can be employed. Another limitation of decision trees is their sensitivity to small variations in the data, which can result in different trees and predictions.\n",
    "\n",
    "Despite these limitations, decision trees and their variations, such as random forests and gradient boosting, have become widely used in various domains due to their interpretability and effectiveness in a wide range of problem domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Generate a synthetic binary classification dataset\n",
    "X, y = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0,\n",
    "                           n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = tree_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of the decision tree classifier, we calculate the accuracy score by comparing the predicted labels (y_pred) with the true labels (y_test). The accuracy score measures the percentage of correctly predicted instances.\n",
    "\n",
    "Finally, we plot the confusion matrix using the plot_confusion_matrix function from scikit-learn to visualize the performance of the classifier in terms of true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "The confusion matrix is a performance measurement tool used in classification tasks to evaluate the accuracy of a machine learning model. It provides a comprehensive overview of how well the model is performing in terms of correctly and incorrectly predicted instances for each class.\n",
    "\n",
    "A confusion matrix is typically a square matrix with the number of rows and columns equal to the number of classes in the classification problem. For a binary classification task, the matrix will be 2x2, and for a multi-class classification task, it will be NxN, where N is the number of classes.\n",
    "\n",
    "The main components of the confusion matrix are:\n",
    "\n",
    "- True Positives (TP): The number of instances correctly predicted as the positive class.\n",
    "- True Negatives (TN): The number of instances correctly predicted as the negative class.\n",
    "- False Positives (FP): The number of instances incorrectly predicted as the positive class.\n",
    "- False Negatives (FN): The number of instances incorrectly predicted as the negative class.\n",
    "\n",
    "By analyzing the values in the confusion matrix, we can derive various evaluation metrics such as accuracy, precision, recall, and F1-score. These metrics provide insights into the model's performance for specific classes and overall performance.\n",
    "\n",
    "The confusion matrix allows us to identify common classification errors, such as false positives and false negatives, which can have different implications depending on the application. It helps in understanding the strengths and weaknesses of the model and can guide further improvements or adjustments to optimize the classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Controlling complexity of decision trees\n",
    "\n",
    "To better understand the impact of pre-pruning on decision trees, let's delve into the details using the Breast Cancer dataset. As a first step, we import the dataset and split it into a training set and a test set, ensuring we have separate data to train and evaluate our model.\n",
    "\n",
    "Next, we construct a decision tree model using the default settings, which entails growing the tree until all leaves are pure (no impurity remains). To maintain consistency in the decision-making process, we set the random_state parameter for tiebreaking during tree construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "6e5d7a76-9bba-42f7-b26e-907775d289b2"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, stratify=data.target, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier with default settings\n",
    "tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree_classifier.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree_classifier.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon executing the provided code, a decision tree model with fully grown trees is obtained, resulting in every leaf node being pure. This implies that the decision tree has captured all the intricacies and details of the training data, potentially leading to overfitting.\n",
    "\n",
    "As anticipated, the accuracy on the training set is 100%. Since the tree has grown deep enough to perfectly memorize all the labels in the training data, it achieves a flawless accuracy. However, the accuracy on the test set is slightly lower compared to the previously examined linear models, which typically achieved around 95% accuracy.\n",
    "\n",
    "To control the complexity of the decision tree and mitigate overfitting, pre-pruning techniques can be applied. Pre-pruning involves imposing constraints on the tree-building process to restrict its growth, thereby preventing the model from capturing noise or irrelevant features in the data.\n",
    "\n",
    "One commonly used pre-pruning parameter is max_depth, which restricts the maximum depth of the decision tree. By defining a maximum depth, the number of levels in the tree is limited, effectively controlling its complexity.\n",
    "\n",
    "In the forthcoming sections, we will explore diverse pre-pruning techniques and assess their impact on the decision tree's performance using the Breast Cancer dataset.\n",
    "\n",
    "Without imposing any restrictions on the depth of a decision tree, it can potentially grow to an arbitrary depth, resulting in a complex structure. Unpruned trees are susceptible to overfitting and may not generalize well to unseen data. To address this, pre-pruning techniques are employed to halt the tree's growth before it becomes an exact fit to the training data. One approach involves stopping the tree-building process after reaching a specified depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a list of max_depth values to experiment with\n",
    "max_depth_values = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Create empty lists to store the training and test accuracies\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Iterate over different max_depth values\n",
    "for max_depth in max_depth_values:\n",
    "    # Create a decision tree classifier with the specified max_depth\n",
    "    tree_classifier = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    tree_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the training data\n",
    "    y_train_pred = tree_classifier.predict(X_train)\n",
    "    \n",
    "    # Calculate the training accuracy and add it to the train_accuracies list\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_test_pred = tree_classifier.predict(X_test)\n",
    "    \n",
    "    # Calculate the test accuracy and add it to the test_accuracies list\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "# Print the accuracies for different max_depth values\n",
    "for max_depth, train_accuracy, test_accuracy in zip(max_depth_values, train_accuracies, test_accuracies):\n",
    "    print(f\"Max Depth: {max_depth}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing Decision Trees\n",
    "\n",
    "Decision trees are powerful models that provide transparency and interpretability, allowing us to gain insights into the decision-making process. Analyzing decision trees can help us understand how the model is making predictions and extract valuable information from the learned structure.\n",
    "\n",
    "There are several aspects to consider when analyzing decision trees:\n",
    "\n",
    "1. Tree Structure: Examining the structure of the decision tree provides insights into the hierarchy of the features and their importance in the decision-making process. Visualizing the tree structure can help us understand the splits and the path from the root to the leaf nodes.\n",
    "2. Feature Importance: Decision trees inherently provide a measure of feature importance. By assessing the feature importance scores, we can identify the features that contribute the most to the predictive power of the model. This information can guide feature selection, dimensionality reduction, or further analysis.\n",
    "3. Decision Rules: Decision trees consist of a series of decision rules that lead to specific outcomes. Analyzing these decision rules helps us understand the conditions under which different predictions are made. By inspecting the decision rules, we can gain insights into the patterns and relationships captured by the model.\n",
    "4. Model Performance: Evaluating the performance of the decision tree model is crucial to assess its effectiveness. Metrics such as accuracy, precision, recall, and F1-score can provide a comprehensive understanding of the model's predictive performance. Additionally, visualizing metrics like the confusion matrix can reveal the model's strengths and weaknesses in classifying different instances.\n",
    "5. Overfitting and Underfitting: Decision trees are prone to overfitting when they become overly complex and capture noise or irrelevant patterns in the training data. Analyzing the model's performance on both the training and test sets can help identify signs of overfitting or underfitting. Monitoring the accuracy and other metrics as the tree's complexity is adjusted can guide the selection of optimal hyperparameters and prevent overfitting.\n",
    "\n",
    "\n",
    "The following code snippet shows how to visualize the decision tree using the export_graphviz function from scikit-learn. The plot_tree function requires the trained decision tree model and the feature names as input. The feature names are used to label the nodes in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "# import graphviz\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Create a decision tree classifier\n",
    "tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model to the data\n",
    "tree_classifier.fit(X, y)\n",
    "\n",
    "# Export the decision tree to a dot file\n",
    "dot_data = export_graphviz(tree_classifier, out_file=None, feature_names=data.feature_names,\n",
    "                           class_names=data.target_names, filled=True, rounded=True, special_characters=True)\n",
    "\n",
    "# Create a Graphviz object from the dot data\n",
    "# graph = graphviz.Source(dot_data)\n",
    "\n",
    "# Save the decision tree visualization as a PDF file with adjusted size, uncomment to generate the PDF\n",
    "# graph.format = 'pdf'\n",
    "# graph.render(\"decision_tree\", format='pdf', view=True, cleanup=True)\n",
    "\n",
    "# Display the decision tree\n",
    "# graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"gini\" refers to the Gini impurity or Gini index, which is a measure of the impurity or disorder of a node in a decision tree. It quantifies the probability of misclassifying a randomly chosen element if it were randomly labeled according to the distribution of classes in that node.\n",
    "\n",
    "The Gini impurity is calculated for each node during the construction of the decision tree. It ranges between 0 and 1, where 0 indicates a pure node (all samples belong to the same class) and 1 indicates a completely impure node (an equal distribution of samples across all classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance in trees\n",
    "\n",
    "Feature importance in decision trees refers to the **quantification of the relative significance or contribution of each feature in the decision-making process**. It provides insights into the relevance and predictive power of different features in the tree-based model.\n",
    "\n",
    "When a decision tree is constructed, it recursively splits the data based on features to create decision nodes that partition the samples. The feature used for each split is chosen based on criteria such as Gini impurity or information gain, which measure the effectiveness of a split in improving the homogeneity of the target variable within each resulting subset.\n",
    "\n",
    "Feature importance in decision trees is typically derived from how much each feature contributes to reducing the impurity or achieving optimal splits. The importance is calculated by aggregating the contribution of a feature across all nodes and splits in the tree.\n",
    "\n",
    "The feature importance values are often normalized so that they sum up to 1 or are scaled to a specific range. Higher feature importance values indicate that the corresponding feature has a stronger influence on the model's predictions.\n",
    "\n",
    "Feature importance can be beneficial in several ways:\n",
    "\n",
    "Variable Selection: Feature importance scores help identify the most informative and relevant features for the task at hand. By focusing on the most important features, we can simplify the model, reduce dimensionality, and improve interpretability.\n",
    "\n",
    "Insights and Interpretability: Analyzing feature importance provides insights into the underlying patterns and relationships in the data. It helps us understand which features have the most significant impact on the target variable, facilitating interpretation and understanding of the model's behavior.\n",
    "\n",
    "Feature Engineering: Feature importance can guide feature engineering efforts by highlighting the most influential features. It helps in selecting or creating new features that have a higher predictive power, improving the model's performance.\n",
    "\n",
    "Model Comparison: Feature importance can be used to compare the relevance of features across different models or variations of the same model. It allows us to assess the consistency of feature importance rankings and make informed decisions when selecting the best model for a particular task.\n",
    "\n",
    "It's important to note that feature importance in decision trees is derived from the internal structure of the tree and may not capture complex interactions or nonlinear relationships between features. Additionally, feature importance is specific to the model trained on a particular dataset and may not generalize well to other datasets or models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "dc2f68ee-0df0-47ed-b500-7ec99d5a0a5d"
   },
   "outputs": [],
   "source": [
    "print(\"Feature importances:\")\n",
    "print(tree_classifier.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Create a decision tree classifier\n",
    "tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model to the data\n",
    "tree_classifier.fit(X, y)\n",
    "\n",
    "# Get the feature importances from the trained decision tree\n",
    "feature_importances = tree_classifier.feature_importances_\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "sorted_importances = feature_importances[sorted_indices]\n",
    "\n",
    "# Get the names of the features\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Plot the feature importances in a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(feature_importances)), sorted_importances, tick_label=feature_names[sorted_indices])\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.title(\"Feature Importance in Breast Cancer Dataset\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows the feature importances for each feature in the Breast Cancer dataset. The feature importance represents the relative importance of each feature in predicting the target variable.\n",
    "\n",
    "The feature importance values are normalized so that they sum up to 1. The higher the feature importance, the more influential the feature is in making accurate predictions.\n",
    "\n",
    "1. \"worst radius\" has the highest feature importance of 0.696, indicating that it is considered the most influential feature in predicting the target variable.\n",
    "2. Several features, including \"mean area,\" \"mean compactness,\" \"mean symmetry,\" and others, have feature importances of 0.000, indicating that these features are considered less important in predicting the target variable.\n",
    "\n",
    "It is important to note that feature importance is specific to the model trained on a particular dataset and may not generalize well to other datasets or models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "# Generate some synthetic data\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])  # Add some noise\n",
    "\n",
    "# Train a decision tree regressor\n",
    "regr = DecisionTreeRegressor(max_depth=3)\n",
    "regr.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
    "plt.plot(X_test, y_pred, color=\"cornflowerblue\", label=\"prediction\")\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(regr, filled=True, feature_names=['X'], precision=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we want to demonstrate one limitation of Decision Tree Regressors. We will use the ice cream sales dataset to train a decision tree regressor to predict the number of ice creams sold based on the temperature. We will then use the trained model to make predictions for temperatures outside the training range.\n",
    "\n",
    "We first generate synthetic data for temperature and the corresponding number of ice creams sold. We then train a decision tree regressor using the DecisionTreeRegressor class from scikit-learn. The regressor learns the relationship between temperature and ice creams sold from the training data.\n",
    "\n",
    "Next, we generate test temperatures that are outside the training range (15 degrees Celsius and 50 degrees Celsius). We use the trained decision tree regressor to make predictions for these test temperatures.\n",
    "\n",
    "Finally, we print the predictions and visualize the training data, decision boundary (learned by the decision tree regressor), and the predicted points on a scatter plot.\n",
    "\n",
    "When you run this code, you'll see that the decision tree model predicts the number of ice creams sold based on the observed patterns in the training data. However, when we provide temperatures outside the training range, the predictions do not accurately reflect the actual behavior of ice cream sales at those temperatures, demonstrating the limitation of decision trees in extrapolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate synthetic data for temperature and ice creams sold\n",
    "temperature = np.linspace(20, 50, 30).reshape(-1, 1)  # Temperature in Celsius\n",
    "ice_creams = np.linspace(100, 400, 30).reshape(-1, 1)  # Number of ice creams sold\n",
    "\n",
    "# Add random noise to the ice creams data\n",
    "np.random.seed(42)\n",
    "noise = np.random.normal(-10, 10, ice_creams.shape)  # Generate random noise with mean 0 and standard deviation 50\n",
    "ice_creams += noise\n",
    "\n",
    "# Train a decision tree regressor\n",
    "tree_regressor = DecisionTreeRegressor()\n",
    "tree_regressor.fit(temperature, ice_creams)\n",
    "\n",
    "y_pred = tree_regressor.predict(temperature)\n",
    "\n",
    "\n",
    "# Generate test temperatures outside the training range\n",
    "test_temperatures = np.linspace(5, 70, 15).reshape(-1, 1)\n",
    "\n",
    "# Make predictions for the test temperatures\n",
    "predictions = tree_regressor.predict(test_temperatures).reshape(-1, 1)\n",
    "\n",
    "# Plot the training data and decision boundary\n",
    "plt.scatter(temperature, ice_creams, color='b', label='Training Data')\n",
    "plt.scatter(test_temperatures, predictions, color='g', label='Predictions')\n",
    "plt.plot(temperature, y_pred, color='r', label='Decision Boundary')\n",
    "plt.xlabel('Temperature (degrees Celsius)')\n",
    "plt.ylabel('Ice Creams Sold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Generate synthetic data for temperature and ice creams sold\n",
    "temperature = np.linspace(20, 50, 30).reshape(-1, 1)  # Temperature in Celsius\n",
    "ice_creams = np.linspace(100, 400, 30).reshape(-1, 1)  # Number of ice creams sold\n",
    "\n",
    "# Add random noise to the ice creams data\n",
    "np.random.seed(42)\n",
    "noise = np.random.normal(-10, 10, ice_creams.shape)  # Generate random noise with mean -10 and standard deviation 10\n",
    "ice_creams += noise\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_size = int(0.8 * len(temperature))\n",
    "X_train, y_train = temperature[:train_size], ice_creams[:train_size]\n",
    "X_test, y_test = temperature[train_size:], ice_creams[train_size:]\n",
    "\n",
    "# Train a decision tree regressor\n",
    "tree_regressor = DecisionTreeRegressor()\n",
    "tree_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Train a linear regressor\n",
    "linear_regressor = LinearRegression()\n",
    "linear_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set using the decision tree regressor\n",
    "tree_predictions = tree_regressor.predict(X_test)\n",
    "\n",
    "# Make predictions on the test set using the linear regressor\n",
    "linear_predictions = linear_regressor.predict(X_test)\n",
    "\n",
    "# Compute the R-squared score on the test set for the decision tree regressor\n",
    "tree_r2_score = r2_score(y_test, tree_predictions)\n",
    "\n",
    "# Compute the R-squared score on the test set for the linear regressor\n",
    "linear_r2_score = r2_score(y_test, linear_predictions)\n",
    "\n",
    "# Print the R-squared scores on the test set\n",
    "print(\"Decision Tree R-squared Score (Test Set):\", tree_r2_score)\n",
    "print(\"Linear Regression R-squared Score (Test Set):\", linear_r2_score)\n",
    "\n",
    "# Plot the training data and predictions on the test set\n",
    "plt.scatter(X_train, y_train, color='b', label='Training Data')\n",
    "plt.scatter(X_test, y_test, color='g', label='Test Data')\n",
    "plt.plot(X_test, tree_predictions, color='r', label='Decision Tree Predictions')\n",
    "plt.plot(X_test, linear_predictions, color='orange', label='Linear Regression Predictions')\n",
    "plt.xlabel('Temperature (degrees Celsius)')\n",
    "plt.ylabel('Ice Creams Sold')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles of Decision Trees\n",
    "\n",
    "Ensembles of decision trees are powerful machine learning techniques that combine multiple individual decision trees to make predictions. Instead of relying on a single decision tree, ensembles aim to leverage the collective wisdom of multiple trees to improve prediction accuracy and reduce overfitting.\n",
    "\n",
    "### Random Forests\n",
    "Random forests are a popular ensemble learning method that is based on the concept of bagging (bootstrap aggregating) and random feature selection. In random forests, a set of decision trees is trained on different subsets of the training data, where each subset is randomly sampled with replacement. Additionally, at each split in each tree, only a random subset of features is considered for splitting, adding an element of randomness to the model.\n",
    "\n",
    "Random forests are known for their ability to handle high-dimensional datasets and provide robust predictions. They have several advantages, including:\n",
    "\n",
    "- Reduction of overfitting: By aggregating the predictions of multiple trees, random forests reduce the risk of overfitting, which occurs when a model performs well on the training data but fails to generalize to unseen data.\n",
    "- Robustness to noise and outliers: Random forests are less sensitive to noise and outliers in the data compared to individual decision trees.\n",
    "- Feature importance estimation: Random forests can provide a measure of feature importance, which indicates the relative importance of each feature in making predictions.\n",
    "- Parallelizable: The training of individual decision trees in random forests can be easily parallelized, allowing for efficient computations.\n",
    "\n",
    "#### Analyzing Random Forests\n",
    "\n",
    "Analyzing random forests can provide insights into the behavior and performance of the ensemble model. Key aspects to consider when analyzing random forests include:\n",
    "\n",
    "- Feature Importance: Random forests can estimate the importance of each feature in making predictions. This measure is based on the decrease in impurity achieved by each feature when it is used for splitting in the decision trees. Higher feature importance values suggest stronger predictive power.\n",
    "- Visualization of Decision Trees: Although random forests consist of multiple decision trees, it is possible to visualize individual decision trees within the forest. Visualizing decision trees can provide insights into the structure and splitting criteria used by the trees, aiding interpretability.\n",
    "- Prediction Accuracy: Evaluating the prediction accuracy of a random forest is crucial to assess its performance. Metrics such as accuracy, precision, recall, F1-score (for classification), or mean squared error (for regression) can be used to evaluate the model's performance on both the training and test sets.\n",
    "- Out-of-Bag (OOB) Error: Random forests utilize a technique called out-of-bag (OOB) error estimation, which estimates the model's performance using data points that were not included in the bootstrap sample of each tree. The OOB error serves as an additional evaluation metric for assessing the generalization ability of the random forest.\n",
    "\n",
    "Analyzing these aspects of random forests helps gain insights into their performance, interpretability, and generalization ability. It aids in understanding the strengths and weaknesses of the model, guides further model improvement, and supports feature selection. Comparing random forests with other ensemble methods or individual decision trees can help choose the most suitable approach for a specific task or dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application\n",
    "\n",
    "Let’s generate the make_moons dataset with 1000 samples and 0.3 noise. Then, we split the dataset into training and test sets. We create a Random Forest classifier with five trees using the RandomForestClassifier class from scikit-learn. We fit the model to the training data and make predictions on the test data. The accuracy of the model is calculated using the accuracy_score function. Finally, we plot the decision boundary of the Random Forest classifier using the mesh grid and visualize the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate the two_moons dataset\n",
    "X, y = make_moons(n_samples=1000, noise=0.3, random_state=42)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier with five trees\n",
    "random_forest = RandomForestClassifier(n_estimators=5, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the decision boundary\n",
    "h = 0.02  # Step size for mesh grid\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = random_forest.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.8, cmap='coolwarm')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap='coolwarm')\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Random Forest Classifier (n_estimators=5)\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting the Random Forest classifier to the data, we iterate over each tree in the ensemble using random_forest.estimators_. For each tree, we generate a mesh grid and make predictions on it to obtain the decision boundaries for that specific tree. We then plot the decision boundaries along with the data points. Finally, we plot the aggregate prediction of the Random Forest by making predictions on the entire mesh grid. The resulting plot shows the decision boundaries of each tree and the aggregated decision boundaries of the Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Generate the two_moons dataset\n",
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier with five trees\n",
    "random_forest = RandomForestClassifier(n_estimators=5, random_state=42)\n",
    "\n",
    "# Fit the model to the data\n",
    "random_forest.fit(X, y)\n",
    "\n",
    "# Plot the decision boundaries for each tree and their aggregate prediction\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the decision boundaries of each tree\n",
    "for tree_idx, tree in enumerate(random_forest.estimators_):\n",
    "    plt.subplot(2, 3, tree_idx + 1)\n",
    "    \n",
    "    # Generate a mesh grid to visualize the decision boundaries\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Make predictions on the mesh grid for the current tree\n",
    "    Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the decision boundaries\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8, cmap='coolwarm')\n",
    "    \n",
    "    # Plot the data points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap='coolwarm')\n",
    "    plt.title(f\"Tree {tree_idx + 1}\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "\n",
    "# Plot the aggregate prediction of the Random Forest\n",
    "plt.subplot(2, 3, 6)\n",
    "\n",
    "# Generate a mesh grid to visualize the decision boundaries\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Make predictions on the mesh grid for the entire Random Forest\n",
    "Z = random_forest.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plt.contourf(xx, yy, Z, alpha=0.8, cmap='coolwarm')\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap='coolwarm')\n",
    "plt.title(\"Random Forest\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When analyzing the decision boundaries learned by the five trees in the Random Forest, we can observe that each tree captures different aspects of the data. This is due to the randomness introduced during the training process, where each tree is trained on a bootstrap sample of the training data. As a result, the decision boundaries of individual trees may not be accurate for all points.\n",
    "\n",
    "In the plot, we can see that some training points are misclassified by certain trees, as they were not included in the training sets of those trees. This demonstrates the diversity among the trees in the ensemble. However, when we consider the aggregate prediction of the Random Forest, which combines the predictions of all trees, we can observe a more reliable and intuitive decision boundary.\n",
    "\n",
    "It is important to note that the Random Forest reduces overfitting compared to any individual tree. The ensemble approach helps in generalizing the patterns in the data and provides a smoother decision boundary. In practice, Random Forests typically consist of hundreds or even thousands of trees, leading to even smoother decision boundaries and improved performance.\n",
    "\n",
    "By using a larger number of trees, the Random Forest model is more robust to noise and can better capture complex patterns in the data. This highlights the advantage of ensembles in providing more accurate and reliable predictions compared to individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another example, let’s apply a random forest consisting of 100 trees on the Breast Cancer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier with 100 trees\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training and test data\n",
    "y_train_pred = rf_classifier.predict(X_train)\n",
    "y_test_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model on the training and test sets\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest algorithm yields an impressive accuracy of 96.5% on the Breast Cancer dataset, surpassing the performance of both linear models and a single decision tree. It achieves this high accuracy without requiring manual parameter tuning. Although it is possible to adjust parameters like `max_features` or apply pre-pruning techniques as we did with the single decision tree, the default settings of the random forest often produce satisfactory results.\n",
    "\n",
    "One notable advantage of the random forest is its ability to provide reliable feature importances. These importances are computed by aggregating the individual feature importances across all the trees in the forest. Compared to a single decision tree, the feature importances obtained from the random forest are typically more robust and trustworthy. This feature analysis enables us to identify the most influential features in the dataset, aiding in understanding the underlying factors driving the predictive performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Create a random forest classifier with 100 trees\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X, y)\n",
    "\n",
    "# Get the feature importances\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "sorted_importances = feature_importances[sorted_indices]\n",
    "sorted_features = data.feature_names[sorted_indices]\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_features)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest algorithm provides a more comprehensive understanding of the data by considering multiple decision trees with different random subsets of features and training samples. This randomness allows the random forest to capture a broader picture and provide more informative feature importances compared to a single decision tree.\n",
    "\n",
    "In the case of the Breast Cancer dataset, the random forest assigns non-zero importance to a larger number of features compared to a single tree. It still highlights the importance of the \"worst radius\" feature, but interestingly, it considers \"worst area\" to be the most informative feature overall. This variation in feature importance reflects the different perspectives and interpretations provided by the individual trees in the random forest.\n",
    "\n",
    "By incorporating randomness and aggregating the results from multiple trees, the random forest is able to provide a more robust and reliable assessment of feature importance, giving us a better understanding of the dataset and its underlying patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "Gradient Boosting is another popular ensemble technique that combines multiple weak predictive models, typically decision trees, in a sequential manner. Unlike random forests, where trees are trained independently, gradient boosting builds the trees in a stage-wise fashion. Each tree is trained to correct the mistakes made by the previous trees, focusing on the instances that were misclassified or had high residuals. The final prediction is obtained by aggregating the predictions of all the trees. Gradient Boosting algorithms, such as XGBoost and LightGBM, are known for their high predictive power and the ability to handle complex tasks.\n",
    "\n",
    "#### Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Generate the two_moons dataset\n",
    "X, y = make_moons(n_samples=1000, noise=0.3, random_state=42)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an XGBoost classifier with five trees\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=5, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the decision boundary\n",
    "h = 0.02  # Step size for mesh grid\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = xgb_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.8, cmap='coolwarm')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap='coolwarm')\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"XGBoost Classifier (n_estimators=5)\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an XGBoost classifier with five estimators\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=5, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training and test data\n",
    "y_train_pred = xgb_classifier.predict(X_train)\n",
    "y_test_pred = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model on the training and test sets\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Create an XGBoost classifier with 5 estimators\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=5, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "xgb_classifier.fit(X, y)\n",
    "\n",
    "# Get the feature importances\n",
    "feature_importances = xgb_classifier.feature_importances_\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "sorted_importances = feature_importances[sorted_indices]\n",
    "sorted_features = data.feature_names[sorted_indices]\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_features)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### AdaBoost\n",
    "AdaBoost, short for Adaptive Boosting, is an ensemble method that combines multiple weak classifiers in a weighted manner to create a strong classifier. In AdaBoost, each weak classifier is trained on a subset of the data, with more emphasis on the misclassified instances from previous classifiers. The final prediction is determined by combining the predictions of all weak classifiers, where the weight of each classifier's prediction is determined by its accuracy. AdaBoost is particularly effective when used with simple classifiers, such as decision stumps (small decision trees with only one split).\n",
    "\n",
    "#### Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Generate the two_moons dataset\n",
    "X, y = make_moons(n_samples=1000, noise=0.3, random_state=42)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lgb_classifier = lgb.LGBMClassifier(random_state=42)\n",
    "lgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = lgb_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the decision boundary\n",
    "h = 0.02  # Step size for mesh grid\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = lgb_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.8, cmap='coolwarm')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap='coolwarm')\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"LightGBM Classifier\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a LightGBM classifier with 5 estimators\n",
    "lgb_classifier = lgb.LGBMClassifier(n_estimators=5, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "lgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training and test sets\n",
    "y_train_pred = lgb_classifier.predict(X_train)\n",
    "y_test_pred = lgb_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model on the training and test sets\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Create a LightGBM classifier with 5 estimators\n",
    "lgb_classifier = lgb.LGBMClassifier(n_estimators=5, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "lgb_classifier.fit(X, y)\n",
    "\n",
    "# Get the feature importances\n",
    "feature_importances = lgb_classifier.feature_importances_\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "sorted_importances = feature_importances[sorted_indices]\n",
    "sorted_features = data.feature_names[sorted_indices]\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(X.shape[1]), sorted_importances, tick_label=sorted_features)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('LightGBM Feature Importance')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-and-image-analytics-ut4sJMav-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "latex_metadata": {
   "author": "Andreas C. M\\\"ller",
   "title": "Machine Learning with Python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
