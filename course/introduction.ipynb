{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Python tools for data science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy\n",
    "NumPy is the fundamental package for scientific computing with Python. It contains among other things:\n",
    "- a powerful N-dimensional array object\n",
    "- sophisticated (broadcasting) functions\n",
    "- tools for integrating C/C++ and Fortran code\n",
    "- useful linear algebra, Fourier transform, and random number capabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "e2b8e959-75f0-4fa9-a878-5ab024f89223"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create a simple array\n",
    "z = np.array([1, 2, 3, 4, 5])\n",
    "print(f\"z:{z}\")\n",
    "\n",
    "# create a simple 2D array\n",
    "y = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(f\"y:\\n{y}\")\n",
    "\n",
    "# perform some operations on the arrays\n",
    "print(f\"z + 1: {z + 1}\")\n",
    "print(f\"y * 2:\\n{y * 2}\")\n",
    "\n",
    "# some complex operations\n",
    "y_1d = y.flatten() # reshape y to a 1D array\n",
    "print(f\"y_1d:\\n{y_1d}\")\n",
    "\n",
    "# use only the first 5 elements of y_1d and multiply them with z (element-wise)\n",
    "print(f\"y_1d * z:\\n{y_1d[:5] * z}\")\n",
    "\n",
    "# use only the first 5 elements of y_1d and multiply them with z (cross-product)\n",
    "print(f\"y_1d x z:\\n{np.dot(y_1d[:5], z)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matplotlib\n",
    "Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "30faf136-0ef7-4762-bd82-3795eea323d0"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a sequence of numbers from -10 to 10 with 100 steps in between\n",
    "x = np.linspace(-10, 10, 100)\n",
    "# Create a second array using sine\n",
    "y = np.sin(x)\n",
    "# The plot function makes a line chart of one array against another\n",
    "plt.plot(x, y, marker=\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create also more advanced graphs with matplotlib, adding some interactivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "x = np.linspace(0, 2*np.pi, 100)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Create the figure and axis objects\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the data\n",
    "line, = ax.plot(x, y, color='blue', lw=2)\n",
    "\n",
    "# Create a function to update the plot based on user input\n",
    "def update_plot(amplitude, frequency):\n",
    "    line.set_ydata(amplitude * np.sin(frequency * x))\n",
    "    fig.canvas.draw()\n",
    "\n",
    "# Create interactive widgets\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "\n",
    "amplitude_slider = widgets.FloatSlider(value=1.0, min=0.1, max=2.0, step=0.1, description='Amplitude:')\n",
    "frequency_slider = widgets.FloatSlider(value=1.0, min=0.1, max=5.0, step=0.1, description='Frequency:')\n",
    "\n",
    "# Define the interactive function\n",
    "interactive_plot = interactive(update_plot, amplitude=amplitude_slider, frequency=frequency_slider)\n",
    "\n",
    "# Display the interactive plot\n",
    "display(interactive_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas\n",
    "Pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. It can easily handle missing data and has a lot of useful functions for data analysis. For example:\n",
    "- easy handling of missing data\n",
    "- size mutability: columns can be inserted and deleted from DataFrame and higher dimensional objects\n",
    "- automatic and explicit data alignment: objects can be explicitly aligned to a set of labels, or the user can simply ignore the labels and let Series, DataFrame, etc. automatically align the data for you in computations\n",
    "- powerful, flexible group by functionality to perform split-apply-combine operations on data sets, for both aggregating and transforming data\n",
    "- make it easy to convert ragged, differently-indexed data in other Python and NumPy data structures into DataFrame objects\n",
    "- intelligent label-based slicing, fancy indexing, and subsetting of large data sets\n",
    "- intuitive merging and joining data sets\n",
    "- flexible reshaping and pivoting of data sets\n",
    "- hierarchical labeling of axes\n",
    "- robust IO tools for loading data from flat files (CSV and delimited), Excel files, databases, and saving/loading data from the ultrafast HDF5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "ad1b06f7-e03a-4938-9d59-5bb40e848553"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a simple dataset of people\n",
    "data = {'Name': [\"John\", \"Anna\", \"Peter\", \"Linda\"],\n",
    "        'Location' : [\"New York\", \"Paris\", \"Berlin\", \"London\"],\n",
    "        'Age' : [24, 13, 53, 33]\n",
    "       }\n",
    "\n",
    "data_pandas = pd.DataFrame(data)\n",
    "# IPython.display allows \"pretty printing\" of dataframes in the Jupyter notebook\n",
    "display(data_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all rows that have an age column greater than 30\n",
    "display(data_pandas[data_pandas.Age > 30])\n",
    "\n",
    "# perform some (a bit more) complex operation on the dataframe\n",
    "data_pandas['Age'] = data_pandas['Age'].apply(lambda x: x + 1)\n",
    "print(\"After applying the lambda function:\")\n",
    "display(data_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Basic data inspection\n",
    "print(\"First 5 rows of the dataframe:\")\n",
    "display(data_pandas.head())\n",
    "\n",
    "print(\"Last 5 rows of the dataframe:\")\n",
    "display(data_pandas.tail())\n",
    "\n",
    "print(\"Information about the dataframe:\")\n",
    "data_pandas.info()\n",
    "\n",
    "print(\"Statistical summary of the dataframe:\")\n",
    "display(data_pandas.describe())\n",
    "\n",
    "# 3. Filtering data based on multiple conditions\n",
    "print(\"Rows where Age is greater than 25 and Location is not 'Paris':\")\n",
    "filtered_data = data_pandas[(data_pandas.Age > 25) & (data_pandas.Location != 'Paris')]\n",
    "display(filtered_data)\n",
    "\n",
    "# 4. Grouping data and calculating summary statistics\n",
    "print(\"Group by Location and calculate the mean age:\")\n",
    "grouped_data = data_pandas.groupby('Location').Age.mean()\n",
    "display(grouped_data)\n",
    "\n",
    "# 5. Merging two DataFrames\n",
    "# Create another DataFrame to demonstrate merging\n",
    "data2 = {\n",
    "    'Name': ['John', 'Anna', 'Peter', 'Linda'],\n",
    "    'Salary': [50000, 60000, 55000, 65000]\n",
    "}\n",
    "data_pandas2 = pd.DataFrame(data2)\n",
    "\n",
    "print(\"First dataframe:\")\n",
    "display(data_pandas)\n",
    "\n",
    "print(\"Second dataframe:\")\n",
    "display(data_pandas2)\n",
    "\n",
    "merged_data = pd.merge(data_pandas, data_pandas2, on='Name')\n",
    "print(\"Merged dataframe:\")\n",
    "display(merged_data)\n",
    "\n",
    "# 6. Handling missing data\n",
    "# Introduce missing data\n",
    "data_pandas.loc[1, 'Age'] = None\n",
    "print(\"Dataframe with missing data:\")\n",
    "display(data_pandas)\n",
    "\n",
    "print(\"Drop rows with missing values:\")\n",
    "data_pandas_dropped = data_pandas.dropna()\n",
    "display(data_pandas_dropped)\n",
    "\n",
    "print(\"Fill missing values with the mean of the column:\")\n",
    "data_pandas_filled = data_pandas.fillna(data_pandas['Age'].mean())\n",
    "display(data_pandas_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Sorting data\n",
    "print(\"Data sorted by Age:\")\n",
    "sorted_data = data_pandas.sort_values(by='Age')\n",
    "display(sorted_data)\n",
    "\n",
    "# 8. Value Counts\n",
    "print(\"Counts of unique values in the 'Location' column:\")\n",
    "location_counts = data_pandas['Location'].value_counts()\n",
    "display(location_counts)\n",
    "\n",
    "# 9. Pivot Tables\n",
    "print(\"Pivot table of mean Age by Location:\")\n",
    "pivot_table = data_pandas.pivot_table(values='Age', index='Location', aggfunc='mean')\n",
    "display(pivot_table)\n",
    "\n",
    "# 10. Datetime Manipulation\n",
    "print(\"Adding a Date of Birth column and extracting year:\")\n",
    "data_pandas['Date of Birth'] = pd.to_datetime(['1998-05-01', '2009-07-15', '1968-12-22', '1987-03-11'])\n",
    "data_pandas['Birth Year'] = data_pandas['Date of Birth'].dt.year\n",
    "display(data_pandas)\n",
    "\n",
    "# 11. Crosstab\n",
    "print(\"Crosstab of Age and Location:\")\n",
    "crosstab = pd.crosstab(data_pandas['Age'], data_pandas['Location'])\n",
    "display(crosstab)\n",
    "\n",
    "# 12. String Methods\n",
    "print(\"Converting Location to uppercase:\")\n",
    "data_pandas['Location'] = data_pandas['Location'].str.upper()\n",
    "display(data_pandas)\n",
    "\n",
    "# 13. Apply with Functions\n",
    "print(\"Applying a custom function to the Age column:\")\n",
    "def custom_function(x):\n",
    "    return x * 2\n",
    "\n",
    "data_pandas['Age Doubled'] = data_pandas['Age'].apply(custom_function)\n",
    "display(data_pandas)\n",
    "\n",
    "# 14. Conditional Column Creation\n",
    "print(\"Creating a new column based on a condition:\")\n",
    "data_pandas['Is Adult'] = data_pandas['Age'] >= 18\n",
    "display(data_pandas)\n",
    "\n",
    "# 15. Sampling Data\n",
    "print(\"Random sample of 2 rows:\")\n",
    "sampled_data = data_pandas.sample(2)\n",
    "display(sampled_data)\n",
    "\n",
    "# 16. Visualisation with Pandas\n",
    "print(\"Plotting Age distribution:\")\n",
    "data_pandas['Age'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more complex dataset\n",
    "data = {\n",
    "    'Name': ['John', 'Anna', 'Peter', 'Linda', 'John', 'Anna', 'Peter', 'Linda'],\n",
    "    'Year': [2020, 2020, 2020, 2020, 2021, 2021, 2021, 2021],\n",
    "    'Maths': [85, 90, 78, 92, 88, 91, 82, 94],\n",
    "    'Science': [88, 94, 80, 95, 90, 96, 83, 97],\n",
    "    'English': [90, 85, 85, 88, 91, 86, 87, 89]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original dataframe\n",
    "print(\"Original DataFrame:\")\n",
    "display(df)\n",
    "\n",
    "# Melting the DataFrame\n",
    "melted_df = pd.melt(df, id_vars=['Name', 'Year'], value_vars=['Maths', 'Science', 'English'],\n",
    "                    var_name='Subject', value_name='Score')\n",
    "\n",
    "print(\"Melted DataFrame:\")\n",
    "display(melted_df)\n",
    "\n",
    "# Pivoting the DataFrame back to wide format\n",
    "pivoted_df = melted_df.pivot_table(index=['Name', 'Year'], columns='Subject', values='Score').reset_index()\n",
    "\n",
    "print(\"Pivoted DataFrame:\")\n",
    "display(pivoted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Excercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Exercises Notebook\n",
    "\n",
    "# Exercise 1: Creating and Inspecting DataFrames\n",
    "\n",
    "# Task: Create a DataFrame from the given dictionary and inspect its first few rows, last few rows, and basic information.\n",
    "\n",
    "data = {\n",
    "    'Name': ['John', 'Anna', 'Peter', 'Linda'],\n",
    "    'Location': ['New York', 'Paris', 'Berlin', 'London'],\n",
    "    'Age': [24, 13, 53, 33]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "# Your code here\n",
    "\n",
    "# Display the first few rows\n",
    "# Your code here\n",
    "\n",
    "# Display the last few rows\n",
    "# Your code here\n",
    "\n",
    "# Display basic information\n",
    "# Your code here\n",
    "\n",
    "# Exercise 2: Filtering Data\n",
    "\n",
    "# Task: Filter the DataFrame to include only rows where Age is greater than 30.\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# Exercise 3: Sorting Data\n",
    "\n",
    "# Task: Sort the DataFrame by Age in descending order.\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# Exercise 4: Grouping Data and Calculating Summary Statistics\n",
    "\n",
    "# Task: Group the DataFrame by Location and calculate the mean Age for each group.\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# Exercise 5: Handling Missing Data\n",
    "\n",
    "# Task: Introduce missing data into the DataFrame and demonstrate dropping and filling missing values.\n",
    "\n",
    "# Introduce missing value\n",
    "# Your code here\n",
    "\n",
    "# Display the DataFrame with missing data\n",
    "# Your code here\n",
    "\n",
    "# Drop rows with missing values\n",
    "# Your code here\n",
    "\n",
    "# Fill missing values with the mean of the column\n",
    "# Your code here\n",
    "\n",
    "# Exercise 6: Melting Data\n",
    "\n",
    "# Task: Melt the DataFrame to transform it from wide format to long format.\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# Exercise 7: Pivoting Data\n",
    "\n",
    "# Task: Pivot the melted DataFrame back to wide format.\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# Exercise 8: Value Counts\n",
    "\n",
    "# Task: Calculate the value counts for the Location column.\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# Exercise 9: Apply with Functions\n",
    "\n",
    "# Task: Apply a custom function to the Age column to create a new column 'Age Doubled'.\n",
    "\n",
    "# Define a custom function\n",
    "# Your code here\n",
    "\n",
    "# Apply the function to the Age column\n",
    "# Your code here\n",
    "\n",
    "# Exercise 10: Conditional Column Creation\n",
    "\n",
    "# Task: Create a new column 'Is Adult' based on the condition that Age is greater than or equal to 18.\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# Advanced Pandas Exercises\n",
    "\n",
    "# Exercise 11: Merging DataFrames\n",
    "\n",
    "# Task: Create another DataFrame and merge it with the original DataFrame on the 'Name' column.\n",
    "\n",
    "data2 = {\n",
    "    'Name': ['John', 'Anna', 'Peter', 'Linda'],\n",
    "    'Salary': [50000, 60000, 55000, 65000]\n",
    "}\n",
    "\n",
    "# Create a second DataFrame\n",
    "# Your code here\n",
    "\n",
    "# Merge the two DataFrames on the 'Name' column\n",
    "# Your code here\n",
    "\n",
    "# Exercise 12: Datetime Manipulation\n",
    "\n",
    "# Task: Add a 'Date of Birth' column and extract the year, month, and day into separate columns.\n",
    "\n",
    "dob_data = ['1998-05-01', '2009-07-15', '1968-12-22', '1987-03-11']\n",
    "\n",
    "# Add 'Date of Birth' column\n",
    "# Your code here\n",
    "\n",
    "# Extract year, month, and day into separate columns\n",
    "# Your code here\n",
    "\n",
    "# Exercise 13: Pivot Tables\n",
    "\n",
    "# Task: Create a pivot table to calculate the mean Age for each Location and Name combination.\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# Exercise 14: Handling Duplicate Data\n",
    "\n",
    "# Task: Introduce duplicate rows into the DataFrame and demonstrate how to identify and remove duplicates.\n",
    "\n",
    "# Introduce duplicate rows\n",
    "# Your code here\n",
    "\n",
    "# Identify duplicate rows\n",
    "# Your code here\n",
    "\n",
    "# Remove duplicate rows\n",
    "# Your code here\n",
    "\n",
    "# Exercise 15: String Methods\n",
    "\n",
    "# Task: Convert all Location names to uppercase and create a new column 'Initial' with the first letter of each Name.\n",
    "\n",
    "# Convert Location names to uppercase\n",
    "# Your code here\n",
    "\n",
    "# Create 'Initial' column\n",
    "# Your code here\n",
    "\n",
    "# Exercise 16: Crosstab\n",
    "\n",
    "# Task: Create a crosstab to show the frequency of Age groups by Location.\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# Exercise 17: Sampling Data\n",
    "\n",
    "# Task: Take a random sample of 2 rows from the DataFrame.\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# Exercise 18: Visualisation with Pandas\n",
    "\n",
    "# Task: Plot the distribution of Ages using a histogram.\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# Exercise 19: Multi-index DataFrames\n",
    "\n",
    "# Task: Create a multi-index DataFrame and demonstrate basic indexing and slicing.\n",
    "\n",
    "multi_index_data = {\n",
    "    'State': ['California', 'California', 'New York', 'New York'],\n",
    "    'City': ['Los Angeles', 'San Francisco', 'New York City', 'Buffalo'],\n",
    "    'Population': [4000000, 870000, 8400000, 260000]\n",
    "}\n",
    "\n",
    "# Step 1: Create a DataFrame from the given dictionary with 'State' and 'City' as the indexes.\n",
    "# Your code here\n",
    "\n",
    "# Step 2: Set 'State' and 'City' as the multi-index of the DataFrame.\n",
    "# Your code here\n",
    "\n",
    "# Task: Demonstrate basic indexing and slicing\n",
    "# Step 1: Select all rows corresponding to 'California'.\n",
    "# Your code here\n",
    "\n",
    "# Step 2: Select a specific row corresponding to ('California', 'Los Angeles').\n",
    "# Your code here\n",
    "\n",
    "# Step 3: Select a cross-section of all rows corresponding to 'New York City' across all states.\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A First Application: Classifying Iris Species\n",
    "We are going to take a look at a simple machine learning application that will cover the tools we have discussed. We will use data about iris flowers (https://en.wikipedia.org/wiki/Iris_flower_data_set) to classify them into three species: Iris setosa, Iris virginica and Iris versicolor.\n",
    "\n",
    "Found below an image of the characteristics we will use to classify the flowers:\n",
    "\n",
    "![sepal_petal](images/iris_petal_sepal.png)\n",
    "\n",
    "\n",
    "### Meet the Data\n",
    "The data we will use for this example is the Iris dataset, a classical dataset in machine learning and statistics. It is included in scikit-learn in the datasets module. We can load it by calling the load_iris function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()\n",
    "\n",
    "# print the type of iris_dataset\n",
    "print(f\"Type of iris_dataset: {type(iris_dataset)}\")\n",
    "\n",
    "# python dir function return a list of the valid attributes of the object\n",
    "print(f\"Attributes of iris_dataset: {dir(iris_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris object that is returned by load_iris is a Bunch object, which is very similar to a dictionary. It contains keys and values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Keys of iris_dataset:\\n\", iris_dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the key DESCR is a short description of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(iris_dataset['DESCR'][:193] + \"\\n...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the key target_names is an array of strings, containing the species of flower that we want to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target names:\", iris_dataset['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of feature_names is a list of strings, giving the description of each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature names:\\n\", iris_dataset['feature_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data itself is contained in the target and data fields. data contains the numeric measurements of sepal length, sepal width, petal length, and petal width in a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type of data:\", type(iris_dataset['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of data:\", iris_dataset['data'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the array contains measurements for *150 different flowers*. Remember that the individual items are called samples in machine learning, and their properties are called features. The shape of the data array is the number of samples multiplied by the number of features. This is a convention in scikit-learn, and your data will\n",
    "always be assumed to be in this shape. \n",
    "\n",
    "Here are the feature values for the first five samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First five rows of data:\\n\", iris_dataset['data'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this data, we can see that all of the first five flowers have a petal width of 0.2 cm and that the first flower has the longest sepal, at 5.1 cm. \n",
    "\n",
    "The target array contains the species of each of the flowers that were measured, also as a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type of target:\", type(iris_dataset['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of target:\", iris_dataset['target'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target:\\n\", iris_dataset['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring Success: Training and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to construct a machine learning model using the given dataset that can accurately predict the species of iris based on new measurements. However, before we can trust the predictions of our model on new data, it is crucial to evaluate its performance.\n",
    "\n",
    "Unfortunately, we cannot evaluate the model using the same data that was used for training. If we were to do so, the model could simply memorize the entire training set and always provide correct predictions for those points. However, this ability to remember the training set does not indicate how well the model will generalize to new, unseen data.\n",
    "\n",
    "To assess the model's performance, we need to present it with new data for which we already know the correct labels. This is typically accomplished by splitting the labeled data we have collected (in this case, the 150 flower measurements) into two parts. One part, called the training data or training set, is used to build the machine learning model. The remaining part, known as the test data, test set, or hold-out set, is used to evaluate the model's performance.\n",
    "\n",
    "To facilitate this process, scikit-learn provides the `train_test_split` function. This function shuffles the dataset and automatically divides it into a training set (75% of the data) and a test set (25% of the data), along with their corresponding labels. Although the choice of how much data to allocate for training and testing is somewhat arbitrary, a common guideline is to use a test set containing 25% of the data.\n",
    "\n",
    "In scikit-learn, the convention is to represent the data as a two-dimensional array (matrix) denoted by a capital X, while the labels are represented as a one-dimensional array (vector) denoted by a lowercase y. This notation is inspired by the mathematical formulation f(x) = y, where x represents the input to a function and y represents the corresponding output.\n",
    "\n",
    "Let's apply the `train_test_split` function to our data and assign the outputs accordingly, following this naming convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris_dataset['data'], iris_dataset['target'], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before splitting the dataset, the `train_test_split` function applies shuffling to ensure randomness in the resulting subsets. Without shuffling, if we simply took the last 25% of the data as the test set, all the data points would have the label 2. This is because the data points are sorted by label, as shown in the output for iris['target'] previously. *It is important to have a test set that includes data from all classes to properly evaluate the model's generalization performance.*\n",
    "\n",
    "To ensure reproducibility of the results, we can set a fixed seed for the pseudorandom number generator using the random_state parameter. This means that every time we run the function with the same seed value, we will obtain the same output. Fixing the random_state is essential when using randomized procedures to ensure consistency and comparability of the results.\n",
    "\n",
    "The output of the train_test_split function consists of four NumPy arrays: X_train, X_test, y_train, and y_test. The X_train array contains 75% of the rows from the original dataset, while the X_test array contains the remaining 25%. These arrays correspond to the features (input data) for the training and test sets, respectively.\n",
    "\n",
    "To complete the evaluation process, we also have y_train and y_test, which are the corresponding labels for the training and test sets, respectively. These arrays contain the target values (in this case, the iris species) associated with each data point in X_train and X_test.\n",
    "\n",
    "By splitting the data in this manner, we can train our machine learning model on the training set (X_train and y_train) and assess its performance on the test set (X_test and y_test). This allows us to estimate how well the model will generalize to unseen data and make predictions for new measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before constructing a machine learning model, it is beneficial to examine the data to determine if the task can be easily solved without machine learning or if the desired information is not present in the data.\n",
    "\n",
    "Furthermore, data inspection helps identify abnormalities and peculiarities. It is not uncommon to encounter inconsistencies and unexpected measurements in real-world datasets.\n",
    "\n",
    "Visualization is one of the most effective methods for data inspection. One approach is to use scatter plots, where one feature is plotted along the x-axis and another feature along the y-axis, with each data point represented by a dot. However, since computer screens are limited to two dimensions, it becomes challenging to visualize datasets with more than two or three features simultaneously.\n",
    "\n",
    "To address this limitation, a pair plot can be employed, which examines all possible pairs of features. For datasets with a small number of features, such as the four features we have here, this approach is reasonable. However, it's important to note that a pair plot does not capture the interaction among all the features simultaneously, and certain interesting aspects of the data may not be revealed through this visualization.\n",
    "\n",
    "The figure below displays a pair plot of the features in the training set, where each data point is color-coded based on the corresponding iris species. To create this plot, we convert the NumPy array into a pandas DataFrame. The pandas library provides a function called scatter_matrix specifically designed for creating pair plots. The diagonal of the matrix in the pair plot is filled with histograms of each individual feature, providing additional insights into their distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe from data in X_train\n",
    "# label the columns using the strings in iris_dataset.feature_names\n",
    "iris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)\n",
    "# create a scatter matrix from the dataframe, color by y_train\n",
    "pd.plotting.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15),\n",
    "               marker='o', hist_kwds={'bins': 20}, s=60,\n",
    "               alpha=.8, cmap='viridis')\n",
    "\n",
    "# Display the scatter matrix plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot visualizes the relationship between different pairs of features in the iris dataset using a matrix of subplots. Each subplot represents a scatter plot of two features against each other. The diagonal subplots display histograms of individual features.\n",
    "\n",
    "In this specific scatter plot, we are using the training data (X_train) to create the scatter matrix. The data is organized in a DataFrame called iris_dataframe, where each column corresponds to a specific feature from the iris dataset.\n",
    "\n",
    "The scatter_matrix function from pandas plotting is used to generate the scatter plot matrix. The c parameter is set to y_train, which assigns colors to the data points based on their corresponding target labels.\n",
    "\n",
    "By coloring the data points based on the target labels (y_train), we can visually distinguish different iris species within the scatter plot matrix. The cmap parameter is set to 'viridis', which determines the color map used to represent the different species. The 'viridis' colormap ensures that the colors assigned to the species form a visually distinct and continuous gradient.\n",
    "\n",
    "Each subplot within the scatter plot matrix represents a combination of two features. For example, the top-left subplot might show sepal length on the x-axis and sepal width on the y-axis, while the bottom-right subplot might show petal width on the x-axis and petal length on the y-axis.\n",
    "\n",
    "The size of the scatter points is set to s=60, which determines the marker size, while the transparency is set to alpha=.8, providing a balance between visibility and transparency for overlapping data points.\n",
    "\n",
    "Overall, the scatter plot matrix offers a comprehensive visualization of the relationships between the iris dataset's features. It allows us to identify patterns, correlations, and separations between different feature pairs, providing valuable insights into the data's structure and potential discriminatory power for iris species classification.\n",
    "\n",
    "From the plots, we can see that the three classes seem to be relatively well separated using the sepal and petal measurements. This means that a machine learning model will likely be able to learn to separate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Your First Machine Learning Model: k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can proceed with building our machine learning model. Scikit-learn offers various classification algorithms that we can employ for our task. In this case, we will utilize the k-nearest neighbors (KNN) classifier due to its simplicity and ease of understanding.\n",
    "\n",
    "Constructing the KNN model primarily involves storing the training set. To make predictions for new data points, the algorithm locates the point in the training set that is closest to the new point and assigns the label of this nearest training point to the new data point.\n",
    "\n",
    "The \"k\" in k-nearest neighbors signifies that, instead of considering only the closest neighbor, we can take into account a fixed number \"k\" of neighbors from the training set. For instance, we can consider the three or five nearest neighbors. By evaluating the majority class among these neighbors, we can make predictions.\n",
    "\n",
    "In this instance, we will focus on using just a single neighbor. Further details about this approach will be explored in subsequent sessions.\n",
    "\n",
    "In scikit-learn, each machine learning model is implemented within its dedicated class, known as an Estimator class. The K-nearest neighbors classification algorithm is implemented in the KNeighborsClassifier class, which is located in the neighbors module. Before utilizing the model, we need to instantiate this class to create an object. During this instantiation, we can specify and configure any desired parameters. For the KNeighborsClassifier, the most crucial parameter is the number of neighbors, which we will set to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the class\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# instantiate the model (with the default parameters and k=1)\n",
    "knn = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The knn object encapsulates the algorithm that will be used to build the model from the training data, as well the algorithm to make predictions on new data points. It will also hold the information that the algorithm has extracted from the training data. In the case of KNeighborsClassifier, it will just store the training set.\n",
    "\n",
    "To build the model on the training set, we call the `fit` method of the knn object, which takes as arguments the NumPy array X_train containing the training data and the NumPy array y_train of the corresponding training labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit method returns the knn object itself, modifying it in place. As a result, we obtain a string representation of our classifier. This representation provides valuable insight into the parameters used during the model creation process. While most of the parameters adopt their default values, we can specifically observe the presence of n_neighbors=1, which corresponds to the parameter we passed when instantiating the K-nearest neighbors classifier.\n",
    "\n",
    "It is important to note that scikit-learn models often encompass a wide range of parameters. However, the majority of these parameters are either designed to optimize computational speed or cater to very specific use cases. As you progress through our lessons, we will delve into the details of the essential parameters, ensuring a comprehensive understanding of their significance.\n",
    "\n",
    "Printing a scikit-learn model representation may generate lengthy strings, but there is no need to feel overwhelmed by their complexity. Rest assured, we will cover all the crucial parameters and thoroughly explore their implications in upcoming lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can utilize our trained model to make predictions on new data where the correct labels are unknown to us.\n",
    "\n",
    "Let's consider an example scenario: We have come across an iris flower in the wild with the following measurements:\n",
    "\n",
    "Sepal length: 5 cm\n",
    "Sepal width: 2.9 cm\n",
    "Petal length: 1 cm\n",
    "Petal width: 0.2 cm\n",
    "To determine the species of this iris flower, we can organize this data into a NumPy array. The shape of the array will be calculated by multiplying the number of samples (1) by the number of features (4). Here's an example code snippet:\n",
    "\n",
    "Executing the following code will yield the shape of the new data array, which is (1, 4). This shape indicates that we have one sample (iris flower) with four features (sepal length, sepal width, petal length, and petal width).\n",
    "\n",
    "Now, we can proceed to use this new data array to make predictions and identify the species of the iris flower using our trained K-nearest neighbors classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a NumPy array with the new data point\n",
    "X_new = np.array([[5.0, 2.9, 1.0, 0.2]])\n",
    "\n",
    "# Determine the shape of the array\n",
    "shape = X_new.shape\n",
    "\n",
    "print(\"Shape of the new data array:\", shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we made the measurements of this single flower into a row in a twodimensional NumPy array, as scikit-learn always expects two-dimensional arrays for the data.\n",
    "\n",
    "To make a prediction, we call the predict method of the knn object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = knn.predict(X_new)\n",
    "print(\"Prediction:\", prediction)\n",
    "print(\"Predicted target name:\",\n",
    "       iris_dataset['target_names'][prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model predicts that this new iris belongs to the class 0, meaning its species is setosa. But how do we know whether we can trust our model? We don’t know the correct species of this sample, which is the whole point of building the model!\n",
    "\n",
    "Indeed, this brings up an important point. In this case, we don't have the correct species label for the new iris sample, which raises the question of how we can assess the reliability of our model's prediction. The goal of building the model is precisely to predict the species of unseen data points accurately.\n",
    "\n",
    "To evaluate the trustworthiness of our model, we typically employ techniques such as model evaluation, validation, and testing. Here are a few approaches we can consider:\n",
    "\n",
    "- Test-Set Validation: We can reserve a portion of the labeled data for testing purposes, separate from the data used for training the model. By comparing the model's predictions on the test set with the known true labels, we can gauge its performance.\n",
    "- Cross-Validation: Cross-validation involves dividing the data into multiple subsets or folds. The model is trained on a combination of these folds and then tested on the remaining fold. This process is repeated several times, rotating the fold used for testing each time. By aggregating the performance across multiple iterations, we can obtain a more robust evaluation.\n",
    "- Metrics and Performance Measures: Various metrics can quantify the performance of our model, such as accuracy, precision, recall, or F1 score. These metrics provide insights into the model's predictive capability, allowing us to assess its effectiveness.\n",
    "- Comparisons with Baseline Models: We can compare our model's performance against simple baseline models or other established models to determine its relative effectiveness and identify any areas for improvement.\n",
    "\n",
    "By employing these evaluation techniques and comparing our model's predictions against known labels or benchmark models, we can gain confidence in its reliability and understand its limitations. It's important to remember that no model is perfect, and ongoing evaluation and refinement are necessary for robust and trustworthy predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the reliability of our model, we will employ a straightforward technique called test set evaluation.\n",
    "\n",
    "The test set we created earlier plays a crucial role here. This set of data was specifically reserved and not used during the model building process. However, we do possess the correct species labels for each iris in the test set.\n",
    "\n",
    "Using the trained model, we can now make predictions for each iris in the test data and compare those predictions to their known labels. By measuring the accuracy, we can evaluate how well our model performs. Accuracy is calculated as the fraction of flowers for which the correct species was predicted.\n",
    "\n",
    "To compute the accuracy, we can follow these steps:\n",
    "\n",
    "1. Utilize the trained K-nearest neighbors classifier to make predictions for each iris in the test set.\n",
    "2. Compare these predicted species labels against the true species labels of the irises in the test set.\n",
    "3. Calculate the fraction of irises for which the predicted species matches the true species.\n",
    "4. This fraction represents the accuracy of our model, indicating the proportion of correctly predicted species in the test set.\n",
    "\n",
    "By obtaining the accuracy score, we can gauge the performance and reliability of our model. A higher accuracy suggests that the model is making more correct predictions, while a lower accuracy may indicate areas for improvement. This evaluation process allows us to make informed assessments of our model's performance on unseen data and ascertain its effectiveness in predicting the species of irises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "print(\"Test set predictions:\\n\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set score: {:.2f}\".format(np.mean(y_pred == y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the score method of the knn object, which will compute the test set accuracy for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After evaluating our model on the test set, we have obtained a test set accuracy of approximately 0.97. This implies that our model made the correct predictions for 97% of the irises in the test set.\n",
    "\n",
    "Under certain *mathematical assumptions*, this high accuracy suggests that we can expect our model to be correct about 97% of the time when making predictions for new, unseen irises. These assumptions include the assumption that the test set is representative of the population of irises we want to predict on, and that the statistical properties of the test set are similar to those of the unseen data.\n",
    "\n",
    "However, it's important to note that accuracy alone may not provide a complete picture of the model's performance. Other metrics and considerations specific to the application and dataset should be taken into account. For example, precision, recall, or the use of confidence intervals can provide additional insights into the reliability and variability of the accuracy estimate.\n",
    "\n",
    "In our hobby botanist application, this high level of accuracy indicates that our model may be trustworthy enough to utilize. As we delve further into subsequent chapters, we will explore techniques for improving model performance and discuss important caveats and considerations when fine-tuning a model.\n",
    "\n",
    "By addressing these topics, we aim to enhance our model's effectiveness and provide a more comprehensive understanding of its performance characteristics.\n",
    "\n",
    "#### Mathematical Assumptions: \n",
    "\n",
    "We are referring to the statistical framework in which the accuracy metric is calculated. Here's a detailed explanation:\n",
    "\n",
    "- *Assumption of Independent and Identically Distributed Data*: The accuracy of our model is evaluated based on the test set, which is assumed to be a representative sample of the population we want to make predictions on. It is assumed that the test set and the unseen data (new irises) share similar statistical properties. This assumption allows us to generalize the performance of our model from the test set to unseen data.\n",
    "- *Assumption of Generalizability*: Our model's high accuracy on the test set suggests that it has learned the underlying patterns and relationships in the iris dataset effectively. Therefore, assuming that the test set is representative of future unseen data, we can expect our model to generalize well and make correct predictions for new, unseen irises.\n",
    "- *Confidence Interval*: The accuracy score of 97% represents a point estimate of the model's performance. However, it's important to consider the associated uncertainty. By calculating a confidence interval, we can express the range within which we are confident the true accuracy of our model lies. This interval provides a measure of the reliability and variability of the accuracy estimate.\n",
    "- *Limitations and Model Assumptions*: It's essential to acknowledge that accuracy alone may not provide a comprehensive evaluation of model performance. Depending on the specific application and dataset, other metrics, such as precision, recall, or F1 score, might be more appropriate. Additionally, the accuracy of the model assumes that the features used for training and testing are sufficient and relevant for accurately predicting the iris species. If the features are not representative of the underlying patterns in the data, the model may not perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Outlook\n",
    "In this notebook we have covered the following topics:\n",
    "- The machine learning workflow\n",
    "- The Python data science ecosystem\n",
    "- Loading the iris dataset\n",
    "- Exploratory data analysis\n",
    "- Building a machine learning model\n",
    "- Evaluating the model\n",
    "\n",
    "We have also discussed the following concepts:\n",
    "- The importance of data representation and feature engineering\n",
    "- The importance of model evaluation and validation\n",
    "\n",
    "The following code includes all the steps need to build the model and evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris_dataset['data'], iris_dataset['target'], random_state=0)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
