{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction and Feature Extraction\n",
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, a dataset is generated comprising two features. Feature one is a set of values drawn from a standard normal distribution. For feature two, random noise is added to feature one. This noise is also normally distributed but with a smaller standard deviation, resulting in feature two having less variation and therefore being closely correlated to feature one.\n",
    "\n",
    "Subsequent to creating the dataset, standardisation is applied. Standardisation is the process of scaling the features such that they have a mean of zero and a standard deviation of one. This is a crucial step in many machine learning algorithms, including PCA, as it ensures that the outcome is not biased by the original scale of the features.\n",
    "\n",
    "Following this, the original, standardised data is visualised in a scatter plot. Each dot represents an observation, with the colour denoting the sequence of data points.\n",
    "\n",
    "Next, PCA is performed. The aim of PCA is to find and rank the axes in the data space along which the data varies the most. These new axes, termed principal components, are orthogonal (at right angles to each other), and they are linear combinations of the original features. The first principal component is the axis along which the data varies the most, while the second principal component is the axis orthogonal to the first that captures the maximum remaining variance, and so on. \n",
    "\n",
    "The data is then transformed by aligning it with these principal components, resulting in a rotation of the original data space. The transformed data is visualised in a second scatter plot.\n",
    "\n",
    "Dimensionality reduction is performed next. This process involves discarding some of the principal components, typically the ones corresponding to lower variance. In the code, this step is demonstrated by keeping only the first principal component, effectively reducing the data's dimensionality from two dimensions to one.\n",
    "\n",
    "The reduced data is visualised in a third plot. Now, the data points are aligned along the first principal component in a one-dimensional space.\n",
    "\n",
    "Finally, the PCA transformation is reversed, bringing the data back into the original feature space. This is achieved by multiplying the dimensionality-reduced data with the principal components. The result is a dataset with the same number of features as the original data but reduced variance. In this case, all points lie along the direction of the first principal component. The result of this transformation is visualised in a fourth scatter plot.\n",
    "\n",
    "In conclusion, this process demonstrates how PCA can be used to find the directions in which data varies the most, reduce dimensionality by discarding less important directions, and reconstruct the data in the original feature space but with reduced variance. It's a powerful tool for data exploration, noise reduction, and feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a random dataset of two features\n",
    "np.random.seed(0)\n",
    "feature1 = np.random.normal(0, 1, 100)\n",
    "feature2 = feature1 + np.random.normal(0, 0.5, 100)  # less variation in feature 2\n",
    "data = pd.DataFrame({'Feature1': feature1, 'Feature2': feature2})\n",
    "\n",
    "# Scaling the features to have zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Plot the original data\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(data_scaled[:, 0], data_scaled[:, 1], c=range(len(data_scaled)), cmap='viridis', s=150)\n",
    "plt.title('Original Data')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Plot the data after PCA transformation\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(data_pca[:, 0], data_pca[:, 1], c=range(len(data_pca)), cmap='viridis', s=150)\n",
    "plt.title('Data after PCA')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "\n",
    "# Perform PCA with only the most significant component\n",
    "pca = PCA(n_components=1)\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Plot the data after dimensionality reduction\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(data_pca, [0]*len(data_pca), c=range(len(data_pca)), cmap='viridis', s=150)\n",
    "plt.title('Data after Dimensionality Reduction')\n",
    "plt.xlabel('Principal Component 1')\n",
    "\n",
    "# Reverse PCA transformation\n",
    "data_inverse = pca.inverse_transform(data_pca)\n",
    "\n",
    "# Plot the data after reversing PCA transformation\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(data_inverse[:, 0], data_inverse[:, 1], c=range(len(data_inverse)), cmap='viridis', s=150)\n",
    "plt.title('Data after Inverse PCA Transformation')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying PCA for improved dataset visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the widely used applications of Principal Component Analysis (PCA) is to visually represent high-dimensional datasets. When dealing with data that has more than two features, it becomes challenging to create scatter plots for comprehensive visualization. However, there is a simpler alternative available: computing histograms for each feature. Let's explore an example using the Breast Cancer Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load Breast Cancer Dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "transformed_data = pca.fit_transform(X)\n",
    "\n",
    "# Separate samples by class\n",
    "malignant_samples = X[y == 0]\n",
    "benign_samples = X[y == 1]\n",
    "\n",
    "# Plotting histograms for each feature\n",
    "n_features = X.shape[1]\n",
    "n_bins = 30  # Number of bins for histogram\n",
    "\n",
    "fig, axes = plt.subplots(nrows=5, ncols=6, figsize=(18, 12))\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.hist(malignant_samples[:, i], bins=n_bins, alpha=0.5, color='red', label='Malignant', edgecolor='black')\n",
    "    ax.hist(benign_samples[:, i], bins=n_bins, alpha=0.5, color='blue', label='Benign', edgecolor='black')\n",
    "    ax.set_xlabel(data.feature_names[i])\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Histograms for Each Feature (Malignant vs Benign)\", fontsize=16, y=1.05)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these plots, we create a histogram for each of the features, counting how often a data point appears with a feature in a certain range (called a bin). Each plot overlays two histograms, one for all of the points in the benign class (blue) and one for all the points in the malignant class (red). This gives us some idea of how each feature is distributed across the two classes, and allows us to venture a guess as to which features are better at distinguishing malignant and benign samples. For example, the feature \"smoothness error\" seems quite uninformative, because the two histograms mostly overlap, while the feature \"worst concave points\" seems quite informative, because the histograms are quite disjoint.\n",
    "\n",
    "However, this plot doesnâ€™t show us anything about the interactions between variables and how these relate to the classes. Using PCA, we can capture the main interactions and get a slightly more complete picture. We can find the first two principal components, and visualise the data in this new two-dimensional space with a single scatter plot.\n",
    "\n",
    "By plotting the data points in this new PCA space, we can gain a more holistic view of the dataset. This visualisation allows us to observe the overall patterns, relationships, and separability between the classes. It provides a more comprehensive representation of the data, taking into account the interactions between features and their impact on classification.\n",
    "\n",
    "By combining the insights gained from both the histogram analysis and the PCA scatter plot, we can develop a deeper understanding of the dataset's structure, identify informative features, and make more informed decisions when it comes to classification tasks such as distinguishing between malignant and benign samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Before applying the PCA, it is better to scale the data so that each feature has unit variance using StandardScaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Breast Cancer Dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "\n",
    "# Scale the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform PCA on scaled data\n",
    "pca = PCA(n_components=2)\n",
    "transformed_data = pca.fit_transform(X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the PCA transformation\n",
    "\n",
    "Learning and applying the PCA transformation is straightforward and can be treated as a preprocessing step. We start by creating an instance of the PCA object. Then, we find the principal components by invoking the fit method on our data. Once the principal components are determined, we can apply the rotation and dimensionality reduction by calling the transform method.\n",
    "\n",
    "By default, PCA performs the rotation and shifting of the data, preserving all the principal components. However, if we wish to reduce the dimensionality of our data, we need to specify the desired number of components when creating the PCA object. This allows us to indicate how many principal components we want to retain.\n",
    "\n",
    "For example, if we set the number of components to 2, PCA will identify the two most informative principal components. These components capture the maximum variance in the data, summarizing the essential information contained within it. The remaining components, if any, will be discarded, resulting in a lower-dimensional representation of the data.\n",
    "\n",
    "It is important to consider the trade-off between the amount of information retained and the dimensionality reduction achieved. Retaining fewer components may result in some loss of information, but it can simplify the data representation and potentially enhance interpretability and computational efficiency in subsequent tasks.\n",
    "\n",
    "Therefore, when using PCA, we need to decide on the appropriate number of components based on our specific requirements, balancing the need for dimensionality reduction with the goal of preserving as much relevant information as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "n_samples = 100\n",
    "n_features = 5\n",
    "data = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Instantiate PCA object and specify the desired number of components\n",
    "n_components = 2\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Fit the PCA model to the data\n",
    "pca.fit(data)\n",
    "\n",
    "# Apply the PCA transformation\n",
    "transformed_data = pca.transform(data)\n",
    "\n",
    "# Print the original data shape and the transformed data shape\n",
    "print(\"Original Data Shape:\", data.shape)\n",
    "print(\"Transformed Data Shape:\", transformed_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the first two principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Breast Cancer Dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Scale the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Instantiate PCA object and specify the desired number of components\n",
    "n_components = 2\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Fit the PCA model to the scaled data\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Apply the PCA transformation to the scaled data\n",
    "transformed_data = pca.transform(X_scaled)\n",
    "\n",
    "# Create separate arrays for malignant and benign samples\n",
    "malignant_samples = transformed_data[y == 0]\n",
    "benign_samples = transformed_data[y == 1]\n",
    "\n",
    "# Plot the first two principal components with different colors for malignant and benign\n",
    "plt.scatter(malignant_samples[:, 0], malignant_samples[:, 1], color='red', label='Malignant')\n",
    "plt.scatter(benign_samples[:, 0], benign_samples[:, 1], color='blue', label='Benign')\n",
    "\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"PCA: First Two Principal Components\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that PCA is an unsupervised method, meaning it does not consider class information when determining the rotation of the data. Instead, it focuses on identifying correlations within the data. In the scatter plot presented here, we plotted the first principal component against the second principal component and then utilised the class information to assign colours to the data points.\n",
    "\n",
    "By examining the scatter plot, we can observe a clear separation between the two classes in this two-dimensional space. This suggests that even a linear classifier, which would learn a line in this space, could effectively distinguish between the two classes. Additionally, it is evident that the malignant samples (depicted in red) exhibit a greater spread compared to the benign samples (depicted in blue).\n",
    "\n",
    "However, one limitation of PCA is that the axes in the plot are often challenging to interpret directly. The principal components represent directions in the original data and are combinations of the original features. Nevertheless, these combinations tend to be intricate and intricate, as we will explore shortly. The principal components themselves are stored in the `components_` attribute of the PCA object during the fitting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PCA component shape: {}\".format(pca.components_.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `components_` attribute of PCA provides valuable insights into the principal components obtained from the analysis. Each row in `components_` corresponds to a specific principal component, sorted in order of importance. For instance, the first row represents the first principal component, the second row corresponds to the second principal component, and so on. \n",
    "\n",
    "On the other hand, the columns in `components_` correspond to the original features present in the dataset. For instance, if we consider the \"Breast Cancer\" dataset, the columns might represent attributes such as \"mean radius,\" \"mean texture,\" and so forth.\n",
    "\n",
    "To gain a better understanding of the principal components, we can examine the content of `components_`. Each element in `components_` reflects the weight or coefficient assigned to the corresponding original feature in the creation of the respective principal component. These coefficients highlight the contribution of each original feature to the principal component.\n",
    "\n",
    "To facilitate comprehension, we can visualize these coefficients using a heat map. A heat map represents the coefficients as a colour-coded grid, making it easier to identify patterns and relationships. Darker colours usually indicate higher coefficients, while lighter colours indicate lower coefficients. By examining the heat map, we can discern the relative importance and influence of the original features in each principal component.\n",
    "\n",
    "The heat map provides a visual summary of the relationships between the original features and the principal components. It offers a valuable tool for understanding the underlying structure and significance of the data attributes in the dimensionality reduction process achieved by PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Breast Cancer Dataset\n",
    "breast_cancer_dataset = load_breast_cancer()\n",
    "X = breast_cancer_dataset.data\n",
    "\n",
    "# Scale the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Instantiate PCA object and fit to the scaled data\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Create a data frame with the PCA components\n",
    "components_df = pd.DataFrame(data=pca.components_, columns=breast_cancer_dataset.feature_names, index=['PC1', 'PC2'])\n",
    "display(components_df)\n",
    "\n",
    "# Create a heat map of the coefficients\n",
    "plt.matshow(pca.components_, cmap='viridis')\n",
    "plt.yticks([0, 1], [\"First component\", \"Second component\"])\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(breast_cancer_dataset.feature_names)), breast_cancer_dataset.feature_names, rotation=60, ha='left')\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Principal components\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap provides insights into the relationship between the original features and the principal components. Each cell in the heatmap represents a coefficient value, indicating the strength and direction of the relationship between a specific feature and a principal component.\n",
    "\n",
    "When examining the heatmap, you can observe certain patterns and interpret them to gain a better understanding of the data. For example, if in the first component, all features have the same sign (positive or negative), it suggests a general correlation between all features. This means that when one feature has a high value, the other features are likely to have high values as well. Similarly, if all features have the same negative sign, it implies an inverse relationship between the features.\n",
    "\n",
    "On the other hand, if the second component exhibits mixed signs (positive and negative), it indicates a more complex relationship. Some features may have a positive correlation with the second component, while others may have a negative correlation. This mixed sign pattern suggests that different subsets of features contribute to the second component, possibly capturing different aspects of the data.\n",
    "\n",
    "Furthermore, it's important to note that both the first and second components involve all of the 30 features. This means that both principal components take into account information from every feature in the dataset. This highlights the comprehensive nature of the principal components, encompassing the collective information present in all the features.\n",
    "\n",
    "By analysing the heatmap and understanding the patterns in the coefficients, you can gain insights into the relationships between the original features and the principal components. This understanding aids in comprehending the underlying structure and significance of the data attributes in the dimensionality reduction achieved by PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another valuable application of PCA is feature extraction. The concept behind feature extraction is to discover a data representation that is more suitable for analysis than the original raw representation. An excellent example of where feature extraction proves beneficial is in working with images. Images consist of pixels, typically stored as red, green, and blue (RGB) intensities, and working with them in their raw form can be challenging.\n",
    "\n",
    "Feature extraction allows us to transform the image data into a more informative and compact representation. Instead of considering each individual pixel as a separate feature, we can extract meaningful features that capture essential characteristics of the image. These features could represent patterns, shapes, or other relevant visual attributes.\n",
    "\n",
    "By applying PCA for feature extraction on images, we can obtain a lower-dimensional representation that captures the most significant variations in the dataset. The principal components derived from PCA can be seen as a compressed version of the image data, where each component represents a combination of pixels that contributes most to the overall variance.\n",
    "\n",
    "This dimensionality reduction achieved through PCA enables more efficient analysis and processing of images. It helps in tasks such as image recognition, object detection, and image compression. By extracting relevant features, we can focus on the most important aspects of the images while discarding redundant or less informative pixel-level details.\n",
    "\n",
    "Through feature extraction with PCA, we transform complex image data into a more manageable and meaningful representation, facilitating subsequent analysis and interpretation. This process plays a crucial role in extracting valuable insights from image datasets and paves the way for various applications in computer vision and image processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use the fetch_lfw_people function from scikit-learn to load the LFW dataset. We specify min_faces_per_person=50 to ensure a sufficient number of samples for each individual and resize=0.4 to resize the images for faster computation.\n",
    "\n",
    "\n",
    "Step 1: Fetching and Displaying the LFW Dataset\n",
    "- We start by fetching the LFW dataset using `fetch_lfw_people` and storing the data in `X` and the corresponding labels in `y`. We also obtain the target names (`target_names`) for labelling purposes.\n",
    "- We display example images for each person in a 5x5 grid using subplots. This helps us visualize the dataset and gain a better understanding of its structure and content.\n",
    "\n",
    "Step 2: Counting the Frequency of Each Target\n",
    "- To understand the distribution of the dataset, we count the frequency of each target (person) in the original dataset using `np.unique` with `return_counts=True`. This provides us with the unique targets and their corresponding counts.\n",
    "- We then iterate over the unique targets and print the target name along with its count. This gives us insights into the distribution of the dataset and the number of photos available for each person.\n",
    "\n",
    "Step 3: Reducing Skewing by Limiting the Number of Photos per Person\n",
    "- We reduce the skewing in the dataset by limiting the number of photos per person. In this case, we set `max_photos_per_person` to 50, ensuring a balanced representation.\n",
    "- We calculate the counts of each person's photos using `np.unique` and `return_counts=True` to obtain `unique_people` and `person_counts`. These arrays help us iterate over each person and select a maximum of `max_photos_per_person` indices for each person.\n",
    "- We collect these filtered indices in `filtered_indices` to create filtered versions of `X` and `y` as `filtered_X` and `filtered_y`, respectively. This ensures that the resulting dataset has an equal representation of different individuals.\n",
    "\n",
    "Step 4: Shuffling the Filtered Dataset Representation\n",
    "- To avoid any bias or order-related patterns in the dataset, we shuffle the representation of the filtered dataset. We first create an array of shuffled indices (`shuffled_indices`) using `np.arange` and then shuffle them using `np.random.shuffle`.\n",
    "- We use the shuffled indices to reorder `filtered_X` and `filtered_y`, resulting in a shuffled representation of the filtered dataset.\n",
    "\n",
    "Step 5: Displaying Example Images after Reducing Skewing\n",
    "- We display example images from the filtered dataset after reducing the skewing in a similar manner to the previous subplot grid. These images now represent a more balanced representation of different individuals.\n",
    "- The shuffled representation ensures that the images are displayed in a random order, avoiding any potential biases in the visual representation.\n",
    "\n",
    "Step 6: Counting the Frequency of Each Target after Reducing the Skewing\n",
    "- We count the frequency of each target in the filtered dataset after reducing the skewing using `np.unique` with `return_counts=True`. This provides us with the unique filtered targets and their respective counts.\n",
    "- We iterate over the unique filtered targets and print the target name along with its count. This allows us to observe the distribution of the filtered dataset and verify that the skewing has been effectively reduced.\n",
    "\n",
    "By running this code, students will be able to fetch and explore the LFW dataset, understand the skewing effect, reduce the skewing by limiting the number of photos per person, shuffle the representation, and observe the impact of these steps on the dataset's representation and target frequency.\n",
    "\n",
    "Feel free to discuss and analyze the results further or modify the code to suit your specific needs and learning objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "# Fetch LFW dataset\n",
    "lfw_dataset = fetch_lfw_people(min_faces_per_person=50, resize=0.4)\n",
    "X = lfw_dataset.data\n",
    "y = lfw_dataset.target\n",
    "target_names = lfw_dataset.target_names\n",
    "\n",
    "# Scale grayscale values to be between 0 and 1\n",
    "X = X / 255.0\n",
    "\n",
    "# Display example images for each person\n",
    "fig, axes = plt.subplots(5, 5, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X[i].reshape(lfw_dataset.images.shape[1:]), cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(target_names[y[i]], fontsize=8)\n",
    "\n",
    "plt.suptitle(\"LFW Dataset - Example Images\", fontsize=14)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# Count the frequency of each target\n",
    "unique_targets, target_counts = np.unique(y, return_counts=True)\n",
    "\n",
    "# Print the counts\n",
    "for target, count in zip(unique_targets, target_counts):\n",
    "    print(f\"Target: {target_names[target]} | Count: {count}\")\n",
    "\n",
    "# Reduce skewing by limiting the number of photos per person\n",
    "max_photos_per_person = 50\n",
    "unique_people, person_counts = np.unique(y, return_counts=True)\n",
    "filtered_indices = []\n",
    "for person in unique_people:\n",
    "    indices = np.where(y == person)[0]\n",
    "    filtered_indices.extend(indices[:max_photos_per_person])\n",
    "filtered_X = X[filtered_indices]\n",
    "filtered_y = y[filtered_indices]\n",
    "\n",
    "# Shuffle the filtered dataset representation\n",
    "shuffled_indices = np.arange(filtered_X.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "filtered_X = filtered_X[shuffled_indices]\n",
    "filtered_y = filtered_y[shuffled_indices]\n",
    "\n",
    "# Display example images after reducing skewing\n",
    "fig, axes = plt.subplots(5, 5, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(filtered_X[i].reshape(lfw_dataset.images.shape[1:]), cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(target_names[filtered_y[i]], fontsize=8)\n",
    "\n",
    "plt.suptitle(\"LFW Dataset - Example Images (After Reducing Skewing)\", fontsize=14)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# Count the frequency of each target after reducing the skewing\n",
    "unique_filtered_targets, filtered_target_counts = np.unique(filtered_y, return_counts=True)\n",
    "\n",
    "# Print the counts after reducing the skewing\n",
    "print(\"\\nTarget frequencies after reducing the skewing:\")\n",
    "for target, count in zip(unique_filtered_targets, filtered_target_counts):\n",
    "    print(f\"Target: {target_names[target]} | Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the field of face recognition, a common task is to determine whether a newly encountered face belongs to a known individual from a database. This task finds its applications in various domains such as photo collection management, social media platforms, and security systems. One approach to tackling this problem would involve building a classifier where each person is treated as a separate class. However, face databases typically contain numerous different individuals with very few images available per person. This scarcity of training examples per class poses challenges for most classifiers. Furthermore, it is desirable to have a system that can easily accommodate new individuals without requiring extensive retraining of a large model.\n",
    "\n",
    "A straightforward solution is to employ a one-nearest-neighbor classifier, which searches for the most similar face image to the face being classified. This classifier has the advantage that it can potentially work with just a single training example per class. Now, let's delve into how well the KNeighborsClassifier performs in this context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is to be run after the previous code cell\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(filtered_X, filtered_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the K-nearest neighbors classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier on the test set\n",
    "accuracy = knn_classifier.score(X_test, y_test)\n",
    "print(f\"Accuracy of K-nearest neighbors classifier: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into training and test sets using train_test_split. This allows us to train the classifier on a portion of the data and evaluate its performance on unseen test data.\n",
    "\n",
    "We create an instance of the K-nearest neighbors classifier (KNeighborsClassifier) with n_neighbors=1, indicating that we want to consider only the closest neighbor for classification.\n",
    "\n",
    "The classifier is then fitted to the training data using the fit method, enabling it to learn the relationships between the face images and their corresponding labels.\n",
    "\n",
    "Finally, we evaluate the performance of the classifier on the test set using the score method, which calculates the accuracy of the classifier by comparing the predicted labels to the true labels.\n",
    "\n",
    "By running this code, you will obtain the accuracy of the K-nearest neighbors classifier in identifying previously unseen faces. This accuracy serves as a measure of how well the classifier performs in the face recognition task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the PCA\n",
    "\n",
    "PCA (Principal Component Analysis) comes to our aid in addressing the limitations of using pixel distances for measuring similarity between faces. Comparing images based solely on their pixel values is not an effective way to capture the nuances of facial features. When comparing two images using a pixel representation, we simply compare the grayscale value of each pixel to the corresponding pixel in the other image. However, this raw pixel representation differs significantly from how humans perceive and interpret facial images.\n",
    "\n",
    "Consider this: if we measure similarity based on pixel distances, even a slight shift of a face by just one pixel to the right would result in a drastically different representation. Such a representation fails to capture the true essence of facial features. To overcome this challenge, we can leverage PCA and distances along principal components to potentially improve the accuracy of our face recognition system.\n",
    "\n",
    "In this code snippet, we enable the \"whitening\" option of PCA. Whitening involves rescaling the principal components to have the same scale. This rescaling is analogous to using StandardScaler after the PCA transformation. By incorporating whitening into our PCA analysis, we not only rotate the data but also ensure that the center panel of the transformed data resembles a circular shape rather than an ellipse.\n",
    "\n",
    "This transformation allows us to capture the most important features of the face by reducing the impact of irrelevant variations such as lighting conditions or facial position. With PCA's ability to extract essential facial characteristics, we move closer to a representation that aligns with human interpretation, leading to improved accuracy in face recognition tasks.\n",
    "\n",
    "Feel free to experiment with different PCA configurations and explore other options to further enhance the accuracy and effectiveness of your face recognition system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA with whitening\n",
    "pca = PCA(n_components=50, whiten=True)\n",
    "X_pca = pca.fit_transform(filtered_X)\n",
    "\n",
    "# Visualize the effect of whitening on an example image\n",
    "example_index = 0  # Choose an index for the example image\n",
    "example_image = filtered_X[example_index].reshape(lfw_dataset.images.shape[1:])\n",
    "whitened_example_image = pca.inverse_transform(X_pca[example_index]).reshape(lfw_dataset.images.shape[1:])\n",
    "\n",
    "# Display the original and whitened example images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].imshow(example_image, cmap='gray')\n",
    "axes[0].set_title(\"Original Example Image\")\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(whitened_example_image, cmap='gray')\n",
    "axes[1].set_title(\"Whitened Example Image\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.suptitle(\"Effect of Whitening on an Example Image\", fontsize=14)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the transformed data and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, filtered_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the K-nearest neighbors classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier on the test set\n",
    "accuracy = knn_classifier.score(X_test, y_test)\n",
    "print(f\"Accuracy of K-nearest neighbors classifier with PCA and whitening: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with image data, we have the advantage of being able to visualize the principal components that are discovered through PCA. It's important to recall that these components represent directions within the input space. In the case of our image data, the input space consists of grayscale images with dimensions of 50 pixels by 37 pixels. Therefore, the principal components themselves can also be visualized as grayscale images with the same dimensions.\n",
    "\n",
    "This visual representation allows us to gain insights into the nature of the principal components. We can observe the patterns and structures that these components capture within the image data. Each principal component image represents a distinct direction in the input space, indicating the variations or features that are significant in the dataset.\n",
    "\n",
    "By visualizing the principal components, we can develop an intuitive understanding of the most important image characteristics that contribute to the overall variance in the dataset. This knowledge can guide further analysis and interpretation of the data, facilitating the extraction of meaningful information and aiding in tasks such as image reconstruction or feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of pca.components_\n",
    "print(\"Shape of pca.components_:\", pca.components_.shape)\n",
    "\n",
    "# Visualize the principal components\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(pca.components_[i].reshape((50, 37)), cmap='viridis')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Component {i+1}\")\n",
    "\n",
    "plt.suptitle(\"Principal Components - Visual Representation\", fontsize=14)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we may not fully comprehend all aspects of these components, we can make educated guesses about the facial features and attributes they may capture. For instance, the first component seems to primarily encode the contrast between the face and the background, emphasizing the distinction between the two. The second component, on the other hand, captures differences in lighting between the right and left sides of the face. As we delve into higher-order components, they may capture even more subtle and intricate details of the facial images.\n",
    "\n",
    "Although this representation derived from PCA is slightly more meaningful than the raw pixel values, it still falls short of how humans perceive and interpret faces. The PCA model, being based on pixels, heavily relies on the alignment of facial features (such as eyes, chin, and nose) and the impact of lighting conditions. Consequently, the similarity between two images is predominantly influenced by these factors in the pixel representation. However, it is crucial to acknowledge that these factors are not the primary aspects humans consider when perceiving faces.\n",
    "\n",
    "When we ask individuals to assess the similarity of faces, they are more inclined to focus on attributes like age, gender, facial expression, and hairstyle. These attributes are complex and challenging to infer solely from pixel intensities. It is important to note that algorithms often interpret data, especially visual data such as images, quite differently from how humans do. Humans possess rich prior knowledge and contextual understanding that aids in their interpretation, whereas algorithms primarily rely on statistical patterns and pixel-level information.\n",
    "\n",
    "As we explore the realm of computer vision and image analysis, it is essential to recognize the limitations and differences in how algorithms process and perceive visual data compared to human perception. Striking a balance between algorithmic interpretation and human interpretation is a fascinating and ongoing pursuit in the field, aiming to bridge the gap between artificial intelligence and human understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Elbow Method\n",
    "distortions = []\n",
    "K = range(1,10)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k, n_init=10)\n",
    "    kmeanModel.fit(X)\n",
    "    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()\n",
    "\n",
    "# Silhouette Analysis\n",
    "sil = []\n",
    "K = range(2,10)  # Silhouette Coefficient is only defined if number of labels is 2 <= n_labels <= n_samples - 1\n",
    "for k in K:\n",
    "  kmeans = KMeans(n_clusters = k, n_init=10).fit(X)\n",
    "  labels = kmeans.labels_\n",
    "  sil.append(silhouette_score(X, labels, metric = 'euclidean'))\n",
    "\n",
    "# Plot Silhouette Analysis\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(K, sil, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis showing the optimal k')\n",
    "plt.show()\n",
    "\n",
    "# Gap Statistic Method\n",
    "# There's no built-in implementation of the Gap Statistic in sklearn, \n",
    "# so it's a bit more complex to implement and may require other packages like gapstat_rs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have plotted the distortion and silhouette score against the number of clusters (`k`), you're looking for specific patterns or points in these plots to help identify the optimal number of clusters.\n",
    "\n",
    "**Elbow Method**\n",
    "\n",
    "The plot for the Elbow method should ideally have a clear 'elbow'-like bend, where the rate of decrease significantly slows beyond a certain point. This point represents the optimal number of clusters according to the Elbow method. The x-coordinate of this point (i.e., the number of clusters) is the value that you're looking for. \n",
    "\n",
    "However, real-world data may not always present a distinct 'elbow'. In such cases, the choice isn't as clear-cut and may require further analysis or domain knowledge to make a good decision.\n",
    "\n",
    "**Silhouette Analysis**\n",
    "\n",
    "The Silhouette graph is a useful way to assess the quality of your clustering, providing a visual and quantitative measure of how well each object lies within its cluster.\n",
    "\n",
    "The silhouette coefficient is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette coefficient ranges from -1 to +1. When the coefficient is high (close to +1), it means the object is well-matched to its own cluster and poorly matched to neighbouring clusters. If most objects have a high value, then the clustering configuration is considered appropriate.\n",
    "\n",
    "On the contrary, a low silhouette coefficient (around 0) indicates that the object is near the decision boundary for two neighbouring clusters. A negative value indicates that those samples might have been assigned to the wrong cluster, as a different cluster is more similar.\n",
    "\n",
    "The silhouette plot displays a measure of how close each point in one cluster is to the points in the neighbouring clusters. To interpret the graph, you're looking for:\n",
    "\n",
    "Size of clusters: The vertical lines in the graph partition it into sections for each cluster. The size of these sections represents the size of the clusters.\n",
    "\n",
    "Score: The width of the sections represents the silhouette score of all samples within a cluster. The width of each section is proportional to the number of samples in the cluster.\n",
    "\n",
    "Overall average score: The red-dashed line represents the average silhouette score of all samples, regardless of the cluster.\n",
    "\n",
    "You typically want the score of each cluster to be above this average line, indicating that on average, samples in this cluster have a silhouette score higher than the average of all samples.\n",
    "\n",
    "Fluctuation in size: Large fluctuations in the size of the silhouette plots can be a clear indication of a problematic clustering.\n",
    "In an ideal scenario, the silhouette plot of each cluster would be above the overall average score (red-dashed line), and the width of the plot would be consistent, suggesting that the cluster configuration is appropriate.\n",
    "\n",
    "The optimal number of clusters would therefore be the one that maximises the average silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_wine, load_digits, load_breast_cancer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "datasets = [load_wine(), load_digits(), load_breast_cancer()]\n",
    "\n",
    "for dataset in datasets:\n",
    "    X = dataset.data\n",
    "    \n",
    "    # Elbow Method\n",
    "    distortions = []\n",
    "    K = range(1,10)\n",
    "    for k in K:\n",
    "        kmeanModel = KMeans(n_clusters=k, n_init=10)\n",
    "        kmeanModel.fit(X)\n",
    "        distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "    \n",
    "    # Plot the elbow\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(K, distortions, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title('The Elbow Method showing the optimal k for {}'.format(dataset.DESCR.split('\\n')[0]))\n",
    "    plt.show()\n",
    "    \n",
    "    # Silhouette Analysis\n",
    "    sil = []\n",
    "    K = range(2,10)  # Silhouette Coefficient is only defined if number of labels is 2 <= n_labels <= n_samples - 1\n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters = k, n_init=10).fit(X)\n",
    "        labels = kmeans.labels_\n",
    "        sil.append(silhouette_score(X, labels, metric = 'euclidean'))\n",
    "\n",
    "    # Plot Silhouette Analysis\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(K, sil, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Analysis showing the optimal k for {}'.format(dataset.DESCR.split('\\n')[0]))\n",
    "\n",
    "    # Add the average silhouette score as a red dashed line\n",
    "    avg_sil = sum(sil) / len(sil)\n",
    "    plt.axhline(y=avg_sil, color='red', linestyle='--')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Visualisation - k-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "\n",
    "# create synthetic dataset\n",
    "np.random.seed(0)\n",
    "X, y = make_blobs(n_samples=300, centers=3, random_state=0, cluster_std=0.60)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "colors = ['blue', 'green', 'purple']\n",
    "\n",
    "# visualize dataset\n",
    "axs[0, 0].scatter(X[:, 0], X[:, 1], s=50, color='white', edgecolors='black')\n",
    "axs[0, 0].set_title(\"Initial Dataset\")\n",
    "\n",
    "# function to plot kmeans steps\n",
    "def plot_kmeans(kmeans, X, n_iter, ax, final=False):\n",
    "    kmeans.fit(X)\n",
    "    labels = kmeans.predict(X)\n",
    "    # plot the input data\n",
    "    ax.scatter(X[:, 0], X[:, 1], c='white', s=50, edgecolors='black')\n",
    "\n",
    "    # Plot points assigned to each cluster\n",
    "    for cluster in np.unique(labels):\n",
    "        ax.scatter(X[labels == cluster, 0], X[labels == cluster, 1], color=colors[cluster], label=f'Cluster {cluster+1}', s=50)\n",
    "\n",
    "    # plot the representation of the k-means model\n",
    "    centers = kmeans.cluster_centers_\n",
    "    ax.scatter(centers[:, 0], centers[:, 1], c=colors, s=200, alpha=0.5, label='Centroids');\n",
    "    \n",
    "    ax.set_title(f\"Iteration: {n_iter+1}\")\n",
    "\n",
    "    if final:\n",
    "        vor = Voronoi(centers)\n",
    "        voronoi_plot_2d(vor, ax=ax, show_vertices=False, line_colors='orange', line_width=2, line_alpha=0.6, point_size=2)\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "# plot three iterations of k-means\n",
    "for i in range(3):\n",
    "    row = (i+1) // 2\n",
    "    col = (i+1) % 2\n",
    "    kmeans = KMeans(n_clusters=3, init='random', n_init=1, max_iter=(i+1), random_state=6)\n",
    "    plot_kmeans(kmeans, X, i, axs[row, col], final=(i == 2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying k-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "# make_blobs() generates synthetic data in groups (or 'blobs'), which is perfect for our example.\n",
    "X, y = make_blobs(n_samples=500, centers=4, random_state=1, cluster_std=0.5)\n",
    "\n",
    "# Visualize the synthetic dataset\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.title(\"Initial Dataset\")\n",
    "plt.show()\n",
    "\n",
    "# Instantiate KMeans\n",
    "# We specify the number of clusters we're expecting (4 in this case).\n",
    "# random_state ensures reproducibility.\n",
    "kmeans = KMeans(n_clusters=4, random_state=1, n_init=10)\n",
    "\n",
    "# Fit the model to the data\n",
    "# This is where KMeans does its job and finds the clusters in the data.\n",
    "kmeans.fit(X)\n",
    "\n",
    "# During the algorithm, each training data point in X is assigned a cluster label. You can find these labels in the kmeans.labels_ attribute:\n",
    "print(\"Cluster memberships:\\n{}\".format(kmeans.labels_))\n",
    "\n",
    "# Visualize the fitted data\n",
    "# Each cluster is represented by a different color.\n",
    "# The red texts represent the cluster centers (or 'centroids').\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_)\n",
    "plt.title(\"Training dataset with Cluster Labels\")\n",
    "for i, center in enumerate(kmeans.cluster_centers_):\n",
    "    plt.text(center[0], center[1], str(i), fontsize=12, color='red')\n",
    "plt.show()\n",
    "\n",
    "# You can also assign cluster labels to new points, using the predict method. Each new point is assigned to the closest cluster center when predicting, but the existing model is not changed. Running predict on the training set returns the same result as labels_:\n",
    "print(\"Predicted cluster memberships:\\n{}\".format(kmeans.predict(X)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Failure cases of k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Clustering: Labels and Ground Truth**\n",
    "\n",
    "Clustering, similar to classification, involves assigning labels to data points. However, unlike classification, clustering is an unsupervised learning task, meaning there is no ground truth or predefined labels. As a consequence, the assigned labels have no inherent meaning before examining the data.\n",
    "\n",
    "Let's revisit the example of clustering face images that we discussed earlier. When clustering faces, the algorithm provides clusters without any preconceived notion of what each cluster represents. For instance, if the algorithm labels a group of faces as \"Cluster 3,\" it does not imply any specific characteristic or identity associated with that label. We can only understand the meaning of each cluster by visually inspecting the images within them.\n",
    "\n",
    "**Arbitrariness of Cluster Numbers**\n",
    "\n",
    "The number of clusters is determined beforehand, often denoted as \"k\" in k-means clustering. However, the choice of \"k\" can be somewhat arbitrary, and different values of \"k\" may lead to different interpretations of the data. For example, if we apply k-means clustering to a two-dimensional toy dataset and obtain two clusters labeled 0 and 1, these labels themselves do not carry inherent significance.\n",
    "\n",
    "The randomness involved in the initialization step of the k-means algorithm can also influence the cluster numbering. Running the algorithm multiple times may result in different cluster assignments due to this random initialization.\n",
    "\n",
    "**Iterative Nature of Clustering**\n",
    "\n",
    "To summarize, clustering is a data exploration technique that helps identify patterns and groups within data. It does not provide predefined meanings for the clusters, and the number of clusters (k) should be chosen based on the nature of the problem or the desired level of granularity. The iterative nature of clustering algorithms ensures that the clusters are refined at each step, leading to a more accurate representation of underlying patterns within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While K-Means can be an effective clustering algorithm, its performance can sometimes diverge from expectations. For instance, consider a scenario where a dataset has three distinct, dense regions: one each at the lower left, upper right, and the center. Given that K-Means tends to stretch its clusters to reach towards the center, the clusters assigned to the lower left and upper right may also include points from the center, which intuitively belong to a separate cluster.\n",
    "\n",
    "Moreover, K-Means presumes equal importance for all directions within each cluster. This assumption becomes a limitation when dealing with data that has distinct clusters elongated along a specific direction, as K-Means only takes into account the distance to the nearest cluster center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_blobs(random_state=170, n_samples=600)\n",
    "rng = np.random.RandomState(74)\n",
    "\n",
    "# Transform the data to be stretched\n",
    "transformation = rng.normal(size=(2, 2))\n",
    "X = np.dot(X, transformation)\n",
    "\n",
    "# Cluster the data into three clusters\n",
    "kmeans = KMeans(n_clusters=3, n_init=10)\n",
    "kmeans.fit(X)\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "# Define colors for clusters and centroids\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "# Plot the data and the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=[colors[label] for label in y_pred])\n",
    "\n",
    "# Plot the centroids with corresponding cluster colors and black border\n",
    "for i, center in enumerate(kmeans.cluster_centers_):\n",
    "    plt.scatter(center[0], center[1], marker='o', alpha=0.5, s=200, edgecolor='black', linewidths=1, color=colors[i])\n",
    "\n",
    "plt.title(\"Clusters formed by KMeans\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might have expected the dense region in the lower left to be the first cluster, the dense region in the upper right to be the second, and the less dense region in the center to be the third. Instead, both cluster 0 and cluster 1 have some points that are far away from all the other points in these clusters that â€œreachâ€ toward the center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-Means algorithm may perform poorly with complex shapes, such as the \"two moons\" dataset. This is due to the fact that K-Means uses a distance measure that creates hyper-spherical clusters, which can be inappropriate for data with complex geometries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate two_moons synthetic data\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "# Cluster the data into two clusters\n",
    "kmeans = KMeans(n_clusters=2, n_init=10)\n",
    "kmeans.fit(X)\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "# Define colors for clusters and centroids\n",
    "colors = ['blue', 'red']\n",
    "\n",
    "# Plot the data and the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=[colors[label] for label in y_pred])\n",
    "\n",
    "# Plot the centroids with corresponding cluster colors and black border\n",
    "for i, center in enumerate(kmeans.cluster_centers_):\n",
    "    plt.scatter(center[0], center[1], marker='o', alpha=0.5, s=200, edgecolor='black', linewidths=2, color=colors[i])\n",
    "\n",
    "plt.title(\"Clusters formed by KMeans on Two Moons dataset\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering algorithm can't discover the two half-moon shapes. This kind of shapes are not possible to be discovered using the k-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Connection between k-Means and PCA**\n",
    "\n",
    "While k-means is primarily known as a clustering algorithm, it exhibits intriguing similarities to decomposition methods like PCA (Principal Component Analysis). PCA aims to discover the directions of maximum variance in the data and represents data points as a sum over some components. On the other hand, k-means seeks to represent each data point using a cluster center. This perspective of k-means as a decomposition method, where each point is represented by a single component, is termed \"vector quantization.\"\n",
    "\n",
    "**Vector Quantization and k-Means**\n",
    "\n",
    "An interesting aspect of vector quantization using k-means is its capability to employ more clusters than input dimensions to encode our data. Let's consider the example of the two_moons dataset. With PCA, there is limited flexibility as the data resides in only two dimensions. Reducing it to one dimension with PCA would result in the loss of important structural information. However, k-means allows us to achieve a more expressive representation by utilizing additional cluster centers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate two_moons synthetic data\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "# Perform PCA with 1 component\n",
    "pca = PCA(n_components=1)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Perform KMeans with 10 clusters (more than the number of dimensions)\n",
    "kmeans = KMeans(n_clusters=10, n_init=10)\n",
    "kmeans.fit(X)\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "# Plot original data, PCA transformed data and KMeans clustering result\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].scatter(X[:, 0], X[:, 1])\n",
    "axes[0].set_title(\"Original Data\")\n",
    "\n",
    "axes[1].scatter(X_pca, [0]*len(X_pca))\n",
    "axes[1].set_title(\"PCA Transformed Data\")\n",
    "\n",
    "colors = plt.cm.get_cmap('viridis', 10)\n",
    "axes[2].scatter(X[:, 0], X[:, 1], c=y_pred, cmap=colors)\n",
    "\n",
    "# Plot the centroids with corresponding cluster colors, black border, size 200 and semi-transparency\n",
    "for i, center in enumerate(kmeans.cluster_centers_):\n",
    "    axes[2].scatter(center[0], center[1], marker='o', s=200, edgecolor='black', linewidths=2, color=colors(i), alpha=0.5)\n",
    "    \n",
    "axes[2].set_title(\"KMeans Clustering Result\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Enhanced Data Separation with k-Means**\n",
    "\n",
    "With the enhanced representation obtained through k-means vector quantization, we have expanded our possibilities for data separation and analysis. Previously, the two half-moon shapes in the dataset were not easily separable using a linear model with only the original two features. \n",
    "\n",
    "**Distance-Based Features**\n",
    "\n",
    "In addition to achieving better separability, we can further enrich the data representation by leveraging the distances to each of the cluster centers as features. This exciting possibility can be realized using the `transform` method of k-means:\n",
    "\n",
    "By incorporating the distances to the cluster centers as new features, we create a more expressive and higher-dimensional representation of the data. These additional features can capture intricate patterns and relationships that were previously hidden in the original dataset. As a result, this enriched representation equips us with powerful insights and facilitates the application of advanced machine learning techniques for more accurate predictions and analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_features = kmeans.transform(X)\n",
    "print(\"Distance feature shape: {}\".format(distance_features.shape))\n",
    "print(\"Distance features:\\n{}\".format(distance_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic dataset with 3 clusters\n",
    "X, y = make_blobs(random_state=1, n_samples=300, centers=3)\n",
    "\n",
    "# Define Agglomerative Clustering model\n",
    "agg = AgglomerativeClustering(n_clusters=3)\n",
    "\n",
    "# Fit the model and predict the cluster memberships simultaneously\n",
    "# This is because agglomerative clustering doesn't have a separate predict method\n",
    "labels = agg.fit_predict(X)\n",
    "\n",
    "# Now, let's visualise the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "plt.title(\"Agglomerative Clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm retrieves the clusters perfectly. Now let's test it with the two moons dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate two_moons synthetic data\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "# Cluster the data into two clusters\n",
    "agg = AgglomerativeClustering(n_clusters=2)\n",
    "\n",
    "# Fit the model and predict the cluster memberships simultaneously\n",
    "labels = agg.fit_predict(X)\n",
    "\n",
    "# Now we create a scatter plot of the clustering\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "\n",
    "# Label the axes and show the plot\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Agglomerative Clustering of Two Moons Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agglomerative clustering algorithm demonstrates a subpar performance on the 'two moons' dataset, primarily due to its inherent structure. The dataset consists of two interlocked half-moon shapes, a configuration that poses a challenge for this algorithm. The essence of agglomerative clustering lies in its ability to build hierarchical clusters by successively merging or splitting existing groups. \n",
    "\n",
    "However, it struggles with 'non-globular' shapes and complex spatial structures such as the two half-moon shapes in this dataset. The reason is that it operates based on local decisionsâ€”either merging or splitting clustersâ€”without considering the broader dataset structure. Consequently, this leads to its inability to correctly identify and separate the two distinctive half-moon shapes in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hierarchical Clustering and Dendrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Generate a dataset with 3 clusters, each having 4 features\n",
    "X, y = make_blobs(n_samples=20, n_features=2, centers=3, random_state=0)\n",
    "\n",
    "# Compute the linkage array - this computes the hierarchical clustering\n",
    "# 'ward' is one method of clustering - it minimizes the variance of the clusters being merged\n",
    "# You can experiment with 'single', 'complete', 'average', and 'centroid' too.\n",
    "Z = linkage(X, 'ward')\n",
    "\n",
    "# Now we are ready to plot the dendrogram\n",
    "fig = plt.figure(figsize=(10, 5))  # set the size of the figure\n",
    "\n",
    "# Create a dendrogram\n",
    "dn = dendrogram(Z)\n",
    "\n",
    "plt.title('Dendrogram')  # title of the plot\n",
    "plt.ylabel('Euclidean distances')  # label the y-axis\n",
    "\n",
    "# Add horizontal lines to indicate the cut-off for two clusters\n",
    "# We are choosing 15 and 10 arbitrarily. Ideally, you would base this on an understanding of your dataset\n",
    "plt.axhline(y=11, color='r', linestyle='--')\n",
    "\n",
    "# Add horizontal lines to indicate the cut-off for three clusters\n",
    "plt.axhline(y=7, color='b', linestyle='--')\n",
    "\n",
    "# Add annotation text\n",
    "plt.text(60, 11.5, 'Two clusters', color='r', va='center', fontweight='bold')\n",
    "plt.text(60, 7.5, 'Three clusters', color='b', va='center', fontweight='bold')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dendrogram is a tree-like diagram that visualises the process of hierarchical clustering. It can sometimes appear complex, but once you understand its structure, it's a powerful tool for interpreting your data. Here's a guide on how to read it:\n",
    "\n",
    "1. **Starting from the Bottom**: The leaves at the bottom of the tree represent individual data points. Each vertical line is a cluster (at different levels), initially treating each data point as a single-cluster.\n",
    "\n",
    "2. **Height of the Lines**: The height of each vertical line represents the distance (dissimilarity) between two clusters that are merged. A longer line signifies a greater distance, implying that less similar clusters are being combined. \n",
    "\n",
    "3. **Moving Upwards**: As you move up the tree, vertical lines are linked by horizontal lines, which represent the merging of two clusters. The height of these horizontal lines indicates the distance at which these clusters have been combined. This distance is a measure of the dissimilarity between the two clusters being merged. Thus, lower merges represent more similar clusters, while higher merges suggest more diverse clusters.\n",
    "\n",
    "4. **Reading the Clusters**: At each height level, you can slice the dendrogram horizontally to define your clusters. For instance, a cut at a lower height will result in more clusters, each containing fewer elements (closer to individual data points). Conversely, a cut at a higher height will result in fewer clusters, each containing more elements (possibly very different from each other).\n",
    "\n",
    "5. **Colouring and Truncation**: Some dendrograms may use colour to indicate cluster groups or may truncate the tree for readability, showing only the last p merged clusters. The colour threshold in a dendrogram is the distance at which clusters are painted with separate colours. Truncation level indicates the maximum depth of the linkage tree to show.\n",
    "\n",
    "6. **Annotations**: Annotations like horizontal lines and texts are usually added to help with the visual understanding of the number of clusters formed at certain distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dendrogram function and the ward clustering function from SciPy\n",
    "from scipy.cluster.hierarchy import dendrogram, ward\n",
    "\n",
    "X, y = make_blobs(random_state=0, n_samples=12)\n",
    "# Apply the ward clustering to the data array X\n",
    "# The SciPy ward function returns an array that specifies the distances\n",
    "# bridged when performing agglomerative clustering\n",
    "linkage_array = ward(X)\n",
    "# Now we plot the dendrogram for the linkage_array containing the distances\n",
    "# between clusters\n",
    "dendrogram(linkage_array)\n",
    "\n",
    "# mark the cuts in the tree that signify two or three clusters\n",
    "ax = plt.gca()\n",
    "bounds = ax.get_xbound()\n",
    "ax.plot(bounds, [7.25, 7.25], '--', c='k')\n",
    "ax.plot(bounds, [4, 4], '--', c='k')\n",
    "\n",
    "ax.text(bounds[1], 7.25, ' two clusters', va='center', fontdict={'size': 15})\n",
    "ax.text(bounds[1], 4, ' three clusters', va='center', fontdict={'size': 15})\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Cluster distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Generate synthetic data (you can replace this with your own dataset)\n",
    "n_samples = 200\n",
    "data, _ = make_moons(n_samples=n_samples, noise=0.1, random_state=42)\n",
    "\n",
    "# Define the range of parameter values to explore (3 values each)\n",
    "eps_values = [0.05, 0.1, 0.2]\n",
    "min_samples_values = range(3, 6)  # Assuming range(2, 5) gives [2, 3, 4]\n",
    "\n",
    "# Create a grid of subplots for the plots (3x3 grid)\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15), sharex=True, sharey=True)\n",
    "\n",
    "# Loop through the first 9 combinations of parameter values and plot the results\n",
    "for i, eps in enumerate(eps_values):\n",
    "    for j, min_samples in enumerate(min_samples_values):\n",
    "        if i*3 + j >= 9:\n",
    "            break\n",
    "\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(data)\n",
    "\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = list(labels).count(-1)\n",
    "\n",
    "        # Plot the clusters and noise points\n",
    "        unique_labels = set(labels)\n",
    "        colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "        for k, col in zip(unique_labels, colors):\n",
    "            if k == -1:\n",
    "                col = [0, 0, 0, 1]  # Black for noise points\n",
    "            class_member_mask = (labels == k)\n",
    "            xy = data[class_member_mask]\n",
    "            axes[i, j].plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markersize=6)\n",
    "\n",
    "        axes[i, j].set_title(\"Eps={}, Min_samples={}\\nClusters: {}, Noise: {}/{}\".format(eps, min_samples, n_clusters, n_noise, n_samples))\n",
    "\n",
    "# Add axis labels and a title to the entire plot\n",
    "fig.text(0.5, 0.04, 'X1', ha='center')\n",
    "fig.text(0.04, 0.5, 'X2', va='center', rotation='vertical')\n",
    "fig.suptitle(\"DBSCAN Clustering with Varying Parameters\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Generate synthetic data (you can replace this with your own dataset)\n",
    "n_samples = 200\n",
    "data, _ = make_blobs(n_samples=n_samples, random_state=42)\n",
    "\n",
    "# Define the range of parameter values to explore (3 values each)\n",
    "eps_values = [0.1, 0.5, 1]\n",
    "min_samples_values = range(3, 6)  # Assuming range(2, 5) gives [2, 3, 4]\n",
    "\n",
    "# Create a grid of subplots for the plots (3x3 grid)\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15), sharex=True, sharey=True)\n",
    "\n",
    "# Loop through the first 9 combinations of parameter values and plot the results\n",
    "for i, eps in enumerate(eps_values):\n",
    "    for j, min_samples in enumerate(min_samples_values):\n",
    "        if i*3 + j >= 9:\n",
    "            break\n",
    "\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(data)\n",
    "\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = list(labels).count(-1)\n",
    "\n",
    "        # Plot the clusters and noise points\n",
    "        unique_labels = set(labels)\n",
    "        colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "        for k, col in zip(unique_labels, colors):\n",
    "            if k == -1:\n",
    "                col = [0, 0, 0, 1]  # Black for noise points\n",
    "            class_member_mask = (labels == k)\n",
    "            xy = data[class_member_mask]\n",
    "            axes[i, j].plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markersize=6)\n",
    "\n",
    "        axes[i, j].set_title(\"Eps={}, Min_samples={}\\nClusters: {}, Noise: {}/{}\".format(eps, min_samples, n_clusters, n_noise, n_samples))\n",
    "\n",
    "# Add axis labels and a title to the entire plot\n",
    "fig.text(0.5, 0.04, 'X1', ha='center')\n",
    "fig.text(0.04, 0.5, 'X2', va='center', rotation='vertical')\n",
    "fig.suptitle(\"DBSCAN Clustering with Varying Parameters\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The code explores the behavior of the DBSCAN algorithm by varying two important parameters: `eps` (neighborhood distance) and `min_samples` (minimum number of points in a neighborhood).\n",
    "\n",
    "2. The code generates synthetic moon-shaped data with added noise using scikit-learn's `make_moons` function. The data contains 200 samples.\n",
    "\n",
    "3. It defines a range of parameter values to explore: `eps_values` and `min_samples_values`. For `eps`, it generates three equally spaced values between 0.1 and 0.5. For `min_samples`, it takes values [2, 3, 4].\n",
    "\n",
    "4. The code creates a 3x3 grid of subplots using matplotlib, where each subplot will show the clusters and noise points obtained for a specific combination of `eps` and `min_samples`.\n",
    "\n",
    "5. It loops through the parameter combinations and fits the DBSCAN model to the synthetic data for each combination of `eps` and `min_samples`.\n",
    "\n",
    "6. For each combination, the code computes the cluster labels and the number of clusters and noise points.\n",
    "\n",
    "7. The clusters and noise points are then plotted on the corresponding subplot in the grid. Clusters are represented by colored markers, while noise points are shown as black markers. The size of the markers indicates whether the points are core samples or boundary points.\n",
    "\n",
    "8. The code adds titles to each subplot, displaying the values of `eps`, `min_samples`, the number of clusters, and the number of noise points for the specific combination.\n",
    "\n",
    "9. The resulting plot showcases the behavior of the DBSCAN algorithm for different combinations of `eps` and `min_samples`. It illustrates how the choice of these parameters affects the clustering outcome, cluster sizes, and noise points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing and evaluating clustering algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics.cluster import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Define the algorithms\n",
    "algorithms = [\n",
    "    ('KMeans', KMeans(n_clusters=3, n_init=10)),\n",
    "    ('AgglomerativeClustering', AgglomerativeClustering(n_clusters=3)),\n",
    "    ('DBSCAN', DBSCAN(eps=0.5))\n",
    "]\n",
    "\n",
    "# Initialize list to store scores\n",
    "ari_scores = []\n",
    "nmi_scores = []\n",
    "\n",
    "# Random state for reproducibility\n",
    "rng = check_random_state(0)\n",
    "\n",
    "# Compute random assignment labels\n",
    "random_labels = rng.permutation(y)\n",
    "\n",
    "# Compute the ARI and NMI for random assignments\n",
    "ari_random = adjusted_rand_score(y, random_labels)\n",
    "nmi_random = normalized_mutual_info_score(y, random_labels)\n",
    "\n",
    "# Iterate over algorithms\n",
    "for name, algorithm in algorithms:\n",
    "    # Predict the clusters\n",
    "    y_pred = algorithm.fit_predict(X)\n",
    "\n",
    "    # Compute the ARI\n",
    "    ari = adjusted_rand_score(y, y_pred)\n",
    "    ari_scores.append(ari)\n",
    "\n",
    "    # Compute the NMI\n",
    "    nmi = normalized_mutual_info_score(y, y_pred)\n",
    "    nmi_scores.append(nmi)\n",
    "\n",
    "# Plotting\n",
    "labels = [name for name, _ in algorithms]\n",
    "labels.append('Random')\n",
    "x = np.arange(len(labels))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - 0.2, ari_scores+[ari_random], 0.4, label='ARI')\n",
    "rects2 = ax.bar(x + 0.2, nmi_scores+[nmi_random], 0.4, label='NMI')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Scores by algorithms and index')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing algorithms on the faces dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTIONAL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's engage the K-means, DBSCAN, and agglomerative clustering algorithms with the 'Labeled Faces in the Wild' dataset to explore any intriguing structures they might uncover.\n",
    "\n",
    "We'll utilise the eigenface representation of the data, generated by Principal Component Analysis (PCA) with the whiten=True parameter and 100 components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "\n",
    "# Load the Labeled Faces in the Wild dataset\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=50, resize=0.4)\n",
    "\n",
    "# Apply PCA with 100 components\n",
    "pca = PCA(n_components=150, whiten=True)\n",
    "X_pca = pca.fit_transform(lfw_people.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we discovered that this representation encapsulates a more semantic understanding of the face images in comparison to the raw pixels. Moreover, it accelerates the computational process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing the faces dataset with DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "\n",
    "# Loading the faces dataset\n",
    "people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\n",
    "X_people = people.data\n",
    "y_people = people.target\n",
    "image_shape = people.images[0].shape\n",
    "\n",
    "# Normalizing the data\n",
    "X_people = X_people / 255.\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(n_components=100, whiten=True, random_state=42)\n",
    "X_pca = pca.fit_transform(X_people)\n",
    "\n",
    "# apply DBSCAN with default parameters\n",
    "dbscan = DBSCAN()\n",
    "labels = dbscan.fit_predict(X_pca)\n",
    "print(\"Unique labels: {}\".format(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all the returned labels are â€“1, so all of the data was labeled as â€œnoiseâ€ by DBSCAN. There are two things we can change to help this: we can make eps higher, to expand the neighborhood of each point, and set min_samples lower, to consider smaller groups of points as clusters. Letâ€™s try changing min_samples first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(min_samples=3)\n",
    "labels = dbscan.fit_predict(X_pca)\n",
    "print(\"Unique labels: {}\".format(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even when considering groups of three points, everything is labeled as noise. So, we need to increase eps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(min_samples=3, eps=15)\n",
    "labels = dbscan.fit_predict(X_pca)\n",
    "print(\"Unique labels: {}\".format(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a much larger eps of 15, we get only a single cluster and noise points. We can use this result to find out what the â€œnoiseâ€ looks like compared to the rest of the data.\n",
    "\n",
    "To understand better whatâ€™s happening, letâ€™s look at how many points are noise, and how many points are inside the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of points in all clusters and noise.\n",
    "# bincount doesn't allow negative numbers, so we need to add 1.\n",
    "# the first number in the result corresponds to noise points\n",
    "print(\"Number of points per cluster: {}\".format(np.bincount(labels + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = X_people[labels==-1]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, subplot_kw={'xticks': (), 'yticks': ()},\n",
    "                         figsize=(12, 4))\n",
    "for image, ax in zip(noise, axes.ravel()):\n",
    "    ax.imshow(image.reshape(image_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these images to the random sample of face images gathered in [76], it is possible to guess why they were labeled as noise: all of them have something strange (poses, objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of analysisâ€”trying to find â€œthe odd one outâ€â€”is called outlier detection. If this was a real application, we might try to do a better job of cropping images, to get more homogeneous data. There is little we can do about people in photos sometimes wearing hats, drinking, or holding something in front of their faces, but itâ€™s good to know that these are issues in the data that any algorithm we might apply needs to handle.\n",
    "If we want to find more interesting clusters than just one large one, we need to set eps smaller, somewhere between 15 and 0.5 (the default). Letâ€™s have a look at what different values of eps result in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in [1, 3, 5, 7, 9, 11]:\n",
    "    print(\"\\neps={}\".format(eps))\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=3)\n",
    "    labels = dbscan.fit_predict(X_pca)\n",
    "    print(\"Number of clusters: {}\".format(len(np.unique(labels))))\n",
    "    print(\"Cluster sizes: {}\".format(np.bincount(labels + 1)))\n",
    "    print(\"Clusters present: {}\".format(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For low settings of eps, all points are labeled as noise. Starting from eps 5 we get some noise points and many smaller clusters. For eps=11 we still get many noise points, but we get one big cluster and some smaller clusters. \n",
    "\n",
    "What is interesting to note is that there is never more than one large cluster. At most, there is one large cluster containing most of the points, and some smaller clusters. This indicates that there are not two or three different kinds of face images in the data that are very distinct, but rather that all images are more or less equally similar to (or dissimilar from) the rest.\n",
    "\n",
    "To visualise a the cluster for a determined eps and min_values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(min_samples=3, eps=7)\n",
    "labels = dbscan.fit_predict(X_pca)\n",
    "\n",
    "for cluster in range(max(labels) + 1):\n",
    "    mask = labels == cluster\n",
    "    n_images =  np.sum(mask)\n",
    "    fig, axes = plt.subplots(1, n_images, figsize=(n_images * 1.5, 4),\n",
    "                             subplot_kw={'xticks': (), 'yticks': ()})\n",
    "    for image, label, ax in zip(X_people[mask], y_people[mask], axes):\n",
    "\n",
    "        ax.imshow(image.reshape(image_shape))\n",
    "        ax.set_title(people.target_names[label].split()[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the clusters correspond to similar orientation and expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our analysis of the DBSCAN algorithm applied to the faces dataset. As\n",
    "you can see, we are doing a manual analysis here, different from the much more automatic\n",
    "search approach we could use for supervised learning based on R2 score or\n",
    "accuracy.\n",
    "Letâ€™s move on to applying k-means and agglomerative clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyzing the faces dataset with k-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that it was not possible to create\n",
    "more than one big cluster using DBSCAN. Agglomerative clustering and k-means are\n",
    "much more likely to create clusters of even size, but we do need to set a target number\n",
    "of clusters. We could set the number of clusters to the known number of people in\n",
    "the dataset, though it is very unlikely that an unsupervised clustering algorithm will\n",
    "recover them. Instead, we can start with a low number of clusters, like 10, which\n",
    "might allow us to analyze each of the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract clusters with k-Means\n",
    "km = KMeans(n_clusters=10, n_init=10, random_state=0)\n",
    "labels_km = km.fit_predict(X_pca)\n",
    "print(\"Cluster sizes k-means: {}\".format(np.bincount(labels_km)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite different from the result of DBSCAN. We can further analyze the outcome of k-means by visualizing the cluster centers. As we clustered in the representation produced by PCA, we need to rotate the cluster centers back into the original space to visualize them, using pca.inverse_transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, subplot_kw={'xticks': (), 'yticks': ()},\n",
    "                         figsize=(12, 4))\n",
    "for center, ax in zip(km.cluster_centers_, axes.ravel()):\n",
    "    ax.imshow(pca.inverse_transform(center).reshape(image_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster centers found by k-means are very smooth versions of faces. This is not very surprising, given that each center is an average of many face images. Working with a reduced PCA representation adds to the smoothness of the images. The clustering seems to pick up on different orientations of the face, different expressions (the third\n",
    "cluster center seems to show a smiling face), and the presence of shirt collars (see the second-to-last cluster center). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyzing the faces dataset with agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract clusters with ward agglomerative clustering\n",
    "agglomerative = AgglomerativeClustering(n_clusters=10)\n",
    "labels_agg = agglomerative.fit_predict(X_pca)\n",
    "print(\"cluster sizes agglomerative clustering: {}\".format(\n",
    "       np.bincount(labels_agg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are more uneven than those produced by k-means, but much more even than the ones produced by DBSCAN. We can compute the ARI to measure whether the two partitions of the data given by agglomerative clustering and k-means are similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ARI: {:.2f}\".format(adjusted_rand_score(labels_agg, labels_km)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This low ARI means that the two clusterings labels_agg and labels_km have little in common. This is not very surprising, given the fact that points further away from the cluster centers seem to have little in common for k-means. Next, we might want to plot the dendrogram. Weâ€™ll limit the depth of the tree in the plot, as branching down to the individual 2,063 data points would result in an unreadably dense plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_array = ward(X_pca)\n",
    "# now we plot the dendrogram for the linkage_array\n",
    "# containing the distances between clusters\n",
    "plt.figure(figsize=(20, 5))\n",
    "dendrogram(linkage_array, p=7, truncate_mode='level', no_labels=True)\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Cluster distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the faces data, there doesnâ€™t seem to be a very natural cutoff point. There are some branches that represent more distinct groups, but there doesnâ€™t appear to be a particular number of clusters that is a good fit. This is not surprising, given the results\n",
    "of DBSCAN, which tried to cluster all points together.\n",
    "\n",
    "Letâ€™s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that there is no notion of cluster center in agglomerative clustering (though we could compute the mean), and we simply show the first couple of points in each cluster. We show the number of points in each cluster to the left of the first image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 10\n",
    "for cluster in range(n_clusters):\n",
    "    mask = labels_agg == cluster\n",
    "    fig, axes = plt.subplots(1, 10, subplot_kw={'xticks': (), 'yticks': ()},\n",
    "                             figsize=(15, 8))\n",
    "    axes[0].set_ylabel(np.sum(mask))\n",
    "    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\n",
    "                                      labels_agg[mask], axes):\n",
    "        ax.imshow(image.reshape(image_shape))\n",
    "        ax.set_title(people.target_names[label].split()[-1],\n",
    "                     fontdict={'fontsize': 9})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While some of the clusters seem to have a semantic theme, many of them are too\n",
    "large to be actually homogeneous. To get more homogeneous clusters, we can run the\n",
    "algorithm again, this time with 40 clusters, and pick out some of the clusters that are\n",
    "particularly interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract clusters with ward agglomerative clustering\n",
    "agglomerative = AgglomerativeClustering(n_clusters=40)\n",
    "labels_agg = agglomerative.fit_predict(X_pca)\n",
    "print(\"cluster sizes agglomerative clustering: {}\".format(np.bincount(labels_agg)))\n",
    "\n",
    "n_clusters = 40\n",
    "for cluster in [10, 13, 19, 22, 36]: # hand-picked \"interesting\" clusters\n",
    "    mask = labels_agg == cluster\n",
    "    fig, axes = plt.subplots(1, 15, subplot_kw={'xticks': (), 'yticks': ()},\n",
    "                             figsize=(15, 8))\n",
    "    cluster_size = np.sum(mask)\n",
    "    axes[0].set_ylabel(\"#{}: {}\".format(cluster, cluster_size))\n",
    "    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\n",
    "                                      labels_agg[mask], axes):\n",
    "        ax.imshow(image.reshape(image_shape))\n",
    "        ax.set_title(people.target_names[label].split()[-1],\n",
    "                     fontdict={'fontsize': 9})\n",
    "    for i in range(cluster_size, 15):\n",
    "        axes[i].set_visible(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
